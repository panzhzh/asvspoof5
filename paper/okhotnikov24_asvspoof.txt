The Automatic Speaker Verification Spoofing Countermeasures Workshop (ASVspoof 2024)
31 August 2024, Kos, Greece

43

10.21437/ASVspoof.2024-7

IDVoiceteamSystemDescriptionforASVSpoof5ChallengeAlexandrAlenin,AndreiBalykin,EstebanG´omez,RostislavMakarovPavelMalov,AntonOkhotnikov,NikitaTorgashov,IvanYakovlevIDR&DInc.NewYork,USA{alenin,andrew.balykin,esteban.gomez,makarov,pavel.malov,ohotnikov,torgashov,yakovlev}@idrnd.netAbstractASVSpoofisaseriesofcommunity-ledchallengesaimedatad-vancingthedevelopmentofrobustautomaticspeakerverifica-tion(ASV)systemsandanti-spoofingcountermeasures(CM).Thefiftheditionofthechallengefocusesonspeechdeepfakesandfeaturestwotracks:Track1:RobustSpeechDeepfakeDe-tection(DF)andTrack2:Spoofing-RobustAutomaticSpeakerVerification(SASV).Inthisreport,wedescribeindetailthesystemsubmittedbytheIDVoiceteamtotheopenconditionoftheSASVtrack(Track2).Oursolutionisascore-levelfusionofindependentlytrainedCMandASVsystems.TheCMsys-temiscomposedofsixneuralnetworksoffourdistinctarchi-tectures,whiletheASVsystemisaResNet-basedmodel.Ourfinalsubmissionachievesa0.1156mina-DCFonthechallengeevaluationset.1.IntroductionVoiceCloning(VC)andText-To-Speech(TTS)enginesmadesignificantprogressinthelastseveralyears,makingitverychallengingforthehumaneartodetectsyntheticspeech.TheASVspoof5[1]isaninitiativedrivenbytheresearchcommu-nitythataimstoevaluateandenhancetheperformanceofmod-ernCMandASVsystemsagainstspeechdeepfakes.ThisreportpresentstheIDVoiceteamsystemdescription.We’vebeenfocusedonthetaskofspoofing-robustautomaticspeakerverification(Track2SASV)withless-constraineddatarequirements(opencondition).Inourwork,weutilizedmodernapproaches:subnetworks[2]trainedontopofSSLpre-trainsandontopofdiscriminativelypre-trainedconventionalCNNbackbonesandnewarchitectures,suchasReDimNet[3].TocombinethescoresofindependentlytrainedCMsystems,weusedalinearensemblewiththefollowingscorecalibration.ToobtainaSASVjointscore,wecombinedtheCMsystemfusedscorewithastandardizedoutputofacosinebackendoftheASVsystem.Thepaperisorganizedasfollows:Section2describesthearchitecturesofpre-trainsweusedforCMandASVmodels.InSection3wepresentinformationontrainingdatasets.Section4representsourexperimentsetupandimplementationdetails.Andfinally,Section5andSection6highlightourresultsandfindings.2.SystemDescriptionAsmentionedbefore,oursubmissionisbasedonacombina-tionofindependentlyoptimizedASVandCMsystems,whichweretrainedonvariouslargetrainingdatasetswithextensivedataaugmentationstrategiesapplied.Below,wediscussthekeycomponentsofthearchitecturesweutilized.2.1.ArchitecturesToenhancetherobustnessandaccuracyofoursystem,weusedseveraldeepneuralnetworkarchitectures,successfullyadoptedfortheASVandCMproblems.2.1.1.ASVSystemsOurASVsystemisbuiltuponConvolutionalNeuralNetwork(CNN)ResNet100architecture,whichwastrainedonalargeYouTube-baseddataset.Fortheimplementationdetailspleasereferto[4].2.1.2.CMSystemsAllofourCMsystemsaresmallneuralnetworksbuiltontopofabackbonewithasubnetworkapproach[2],wheretheback-boneiseitherpre-trainedinasupervisedmanner(e.g.forspeakerrecognitiontask)orinaself-supervised(SSL)fashion.SSLTransformers:WeutilizedmultipleSSLmodelsthatwerepre-trainedontheLibriSpeech960hdataset[5]fromtheHuggingFace[6]platform:•wav2vec2-conformer-rel-pos-large[7]•wav2vec2-large-960h[8]•data2vec-audio-large-960h[9]CNNs:Wealsoemployed2DCNNmodelspre-trainedfortheASVtask,becauseaccordingtoourexperimentresultstheypro-videorthogonalitytoSSLmodelsbasedonrawaudioinputandshowgreatperformancewhenfusedtogether:•ResNet100[4]•ReDimNet-B2[3]BothASVandCMsystemsweretrainedindependentlyandthenfusedtocombinethestrengthsofeachapproach,ulti-matelyenhancingtheoverallsystemperformanceindetectingspoofedaudiosamples.3.DatasetsWeutilizedvariousdatasetsfortraininganddevelopmentpur-poses.Thedatasetsarecategorizedasfollows:3.1.CMsystemTotrainourCMsystems,weusedthefollowingdata:
44

3.1.1.TrainingDatasets•ASVspoof5-Train:TrainingsubsetoftheASVSpoof5challenge.•ASVSpoof21-Eval[10]:ExtensivedatasetofvariousTTSandVCengines,evaluationpartfromthechallengesofthelastyears.Includesmorethan100differenten-ginesreleasedbefore2021,and14differenttypesofcodecsandcompressions.•ASVSpoof15[11]/ASVSpoof19[12]:Trainpartfromthechallengesofthepreviousyears.IncludesoldTTSenginesfrom2015and2019.•Blizzard[13]:TheBlizzarddatasetcontainssyntheticspeechmaterialssubmittedbyparticipantsinvariousBlizzardChallengesheldannuallysince2008.Weutilizedtheeditionsofachallengewithnon-LibriVoxsourcedatasuchas2014,2019-2021.•DECRO[14]:TheDECROdatasetconsistsofaudiosamplesdesignedtoevaluatetheimpactoflanguagedif-ferencesondeepfakedetection,featuringbothrealandfakespeechinEnglishandChinese.Thespoofedsam-plesaregeneratedusingamixofcommercialandopen-sourcealgorithms,includingTTSandVCtechniques.•Internaltraindata:InternallycollectedTTS/VCdatabasedonsourceaudiofromVoxTube[15],VCTK[16],andLibriSpeechdata.Thedatasetiscreatedusingacombinationofcommercialandopen-sourcealgorithms,incorporatingbothTTSandVCtechniques,aswellasvariousVocoderandneuralcodecsystemsincludingEn-Codec*[17].*WeappliedEnCodectobonafideaudiosonlyandconsideredthemasspoofs.3.1.2.DevelopmentDatasetsAsadevelopmentset,wedecidedtoexpandthedevelopmentsetissuedbytheorganizersusingsuchdatasetsasInTheWild[18],ChineseFakeAudioDetection[19]dataset,andinternallycollecteddata.Afterward,wemergedallthedevsetsintooneandusedtheresultingdatasettoassessthequalityofthemodelsandalsotooptimizethehyperparametersofthemodelsduringtraining.3.2.ASVsystemWeusedtheResNet100modeltrainedonVoxTube-Largedataset[4]withtheonlydifferenceinaugmentationstrategy,whereweremovedMUSANMusicaugmentationandreplacedMUSANSpeechcorpusasasourceofbabblenoiseaugmenta-tionwithspeakerstakenfromVoxTube-Largedataset.4.Experiments4.1.DatasetupAllCMmodelsweretrainedusingthesamesetofdatasetsde-scribedin3.1.1.Tomakethetrainingprocessandvalidationmetricsstable,weuniformlysampledallVC/TTS/Vocodersen-ginesfromeachdataset,resultingindistinct750ktrainingfileswithinatrainingprotocol,whichincludes23%ofbonafideand77%ofspoofs.Allaudiosweresampledwithauniformproba-bilitytoformtrainbatches.4.2.FeaturesAsinputintoourneuralnetworks,weusedeitherspectralfea-turesfor2DmodelsorrawsignalsforSSL-basedmodels.For2DCNNmodels,weusedmean-normalizedMelfilterbanklog-energieswitha25msframelength,andanFFTsizeof512over20-7600Hzfrequencylimits.ForResNet100weutilized96fre-quencybinsand10msstep,whileforReDimNetweused72frequencybinsand12.5msstep.4.3.AugmentationssetupExtensivedataaugmentationtechniqueswereemployedtoen-hancetherobustnessofourASVandCMmodels:•RoomImpulseResponse(RIR)[20]:Artificiallyrever-beratedasignalviaconvolutionwithrealRIRs.•NoiseAddition:IncorporatingnoisefromMusan[21](musicandnoisesubsets),DEMAND[22],andDCASE[23]corpuswith0-15dbSNR.•Babble:Werandomlypicked3-7bonafidespeakersfromthetrainingdataset,summedthemtogether,andthenaddedthemtotheoriginalsignalwith13-20dbSNRtomodelthebabblenoise.•Mixedsampledataaugmentation:CutMix[24]&MixUp[25]•RawBoost:WeusedRawBoostalgorithm#5[26]•Low-passFiltering:Weappliedreal-timelow-passfil-tering(upto4kHzinfrequencyrange)basedontheFFT,withaprobabilityof30%.•SpecAug[27]:Wemaskedfrom0to5framesinthetemporalaxisandfrom0to10framesinthefrequencyaxisusingtheSpecAug.•Codecs:Weapplieddiversecodectransformationstobothbonafideandspoofsamples.ThetransformationsusedincludeG722,A-law,Mu-law,Opus,MP3,andOGG.Codecswereappliedusingffmpeg1library.4.4.HyperparametersWehadtwodistincthyperparametersetsfortraining2DCNN-basedmodelsandSSL-basedmodels,thesehyperparameterswerederivedfromthehyper-parameteroptimizationalgorithm:HPO.Also,wehadsomehyperparametersfixedacrossallmod-elstraining:•Optimizer:weusedAdamW[28]optimizerwithdefaultbetas&epsilonparameters•Lossfunction:AMSoftmax[29]withscalesetto20.0•Trainingsegmentduration:3.0seconds•Numepochs:1.Modelseeseachdatasampleduringtrainingonlyonce.WeappliedHEBO[30]hyper-parametertuningalgorithmfromray.tune[31]librarytosearchinfollowinghyperpa-rameterspaces:•Learningrate:loguniformin[10−4,10−3]•Weigthdecay:loguniformin[10−5,10−3]•ProbofRawBoost-5algorithm:uniformin[0.0,1.0]•ProbofNoiseaugmentation:uniformin[0.0,1.0]1https://github.com/FFmpeg/FFmpeg
45

•ProbofReverbaugmentation:uniformin[0.0,1.0]•ProbofCutMixappliedtobatch:uniformin[0.0,1.0]•MarginofAMSoftmaxloss:uniformin[0.0,0.2]AfterapplyingHPO,wegotthefollowingvaluesforhyper-parametersfor2modelgroupspresentedinTable1.2D-CNNsubnetsSSLsubnetsbatchsize*80256learningrate0.00060.001RawBoostprob0.00.5weightdecay0.00010.0001noiseaugprob0.90.45reverbaugprob0.10.0CutMixprob0.50.017margin0.120.07Table1:ComparisonofHyperparametersforCNNandSSLmodelstraining*BatchsizewasdefinedbyGPUmemorycapacity:wepickedthemaximumbatchsize,thatfitsasingleGPUforeachmodel.4.5.FusiondescriptionFirstly,alinearcombinationof6CMmodelswasusedtoobtainasingleCMsystemoutput.Todeterminetheoptimalweightofeachmodelinfusionact-DCFoptimizationonanextendeddevelopmentsetwasused.Then,jointASVandCMsystemsoutputwasobtainedasaminimumvalueofCMfusionsystemoutputandastandardizedoutputofacosinesimilaritybackendofASVsystem:min(ScoreCM,ScoreASV).Foroptimiza-tion,weusedaCOBYLAoptimizer[32]toolkit.5.Results5.1.CodecsimpactonCMsystemForclarityreasons,weprovideourresultsanalysisinaformwherewesplittheimpactoftraditionalcodecsandneuralcodecs.Fromtheperspectiveofperformanceonconventionalcodecs,ourCMsystemexhibitedthehighesterroronthe8kand16kSpeexcodecs,asthesecodecswerenotincludedinthetrainingaugmentationsstrategy.Similarly,theOpus8kcodecalsodemonstratedhighererror,asonlythe16kversionofthiscodecwasusedduringtraining.CompleteresultsofourCMsystemonthe16kand8kcodecsareshowninTable2.Table2:MinDCFandEERvaluesfortheCMsystemMinDCFEER,%Nocodec0.00230.11Allcodecs0.17706.26Allcodecsex4,70.07762.76Codecs4,70.624222.0316kcodecs0.21707.6616kcodecsex4,70.05411.928kcodecs0.10703.82Consideringneuralcodecperformance,itisimportanttonotethatEnCodecutilizesaGAN-likedecoderwithlossessim-ilartoHiFi-GAN-stylevocoders.Currently,theprimaryappli-cationoftheseneuralcodecsisasafrontendsystemforLLM-basedTTSandVCsynthesizers,which,inthecontextofthischallenge,arecategorizedasspoofs.Giventhiscontext,weobserveasignificantdegradationintheCMsystem’sperfor-manceonEnCodec(codec-4)andMP3+EnCodec(codec-7).ThisdegradationcanbeexplainedbythefactthatthechallengeevaluationdatasetincludestheuseofEnCodecencodinganddecodingwithoutchangingthelabeloftheaudiosample.Incontrast,duringourtraining,wechangedthelabelsofbonafidesamplestoaspoof-onlylabelafterapplyingsuchprocessing.5.2.SASVfusionmetricsTable3illustratestheminimuma-DCF[33]valuesforthefusedsystemunderdifferentcodecconditions.Thesystemperformedoptimallywithouttheinfluenceofanycodec.Theinclusionofallcodecsresultedinreducedperformance,with16kcodecscausingaminordegradationand8kcodecshavingthemostsig-nificantimpact.Table3:Mina-DCFvaluesfortheFusionSystemMina-DCFNocodecs0.0504Allcodecs0.118416kcodecs0.10308kcodecs0.1300Theseresultshighlighttheinfluenceofcodecconditionsonthesystem’sperformanceanditsrobustnessinvaryingaudioqualityscenarios.Oursystemachievedthefollowingperfor-mancemetricsintheevaluationphase:•Minimuma-DCF:0.1156•t-EER:4.32%•Minimumt-DCF:0.45846.ConclusionsInthisreport,wepresentedoursolutionfortheopenconditionoftheSASVtrackoftheASVSpoof5challenge.Weshowedagaintheeffectivenessofthesubnetworkapproachforthede-tectionofspoofedaudioandanalyzedthecontributionofeachcodectosystemaccuracy.Besides,wecanseeReDimNetap-plicabilityfortheCMtask.Wefoundoutthat8kHzcodecsledtonoticeableaccuracydegradation,whiletheneuralcodecre-sultedinanevengreaterdecreaseinperformance.Ontopofthat,thedetailedresultsshowusstableerrorsacrossdifferenttypesofattacks(enginesA17-A32)ofourSASVsysteminano-codecsscenariowithMaryTTSbeingthehardesttypeofat-tackwhencodecsareapplied.7.References[1]XinWang,H´ectorDelgado,HemlataTak,Jee-weonJung,Hye-jinShim,MassimilianoTodisco,IvanKukanov,XuechenLiu,MdSahidullah,TomiKinnunen,NicholasEvans,KongAikLee,andJunichiYamagishi,“ASVspoof5:Crowdsourcedspeechdata,deepfakes,andadversarialattacksatscale,”inASVspoofWorkshop2024(accepted),2024.[2]AlexanderAlenin,NikitaTorgashov,AntonOkhotnikov,RostislavMakarov,andIvanYakovlev,“ASubnetworkApproachforSpoofingAwareSpeakerVerification,”inProc.Interspeech2022,2022,pp.2888–2892.
46

[3]IvanYakovlev,RostislavMakarov,AndreiBalykin,PavelMalov,AntonOkhotnikov,andNikitaTorgashov,“Re-shapedimensionsnetworkforspeakerrecognition,”arXivpreprintarXiv:2407.18223,2024.[4]NikitaTorgashov,RostislavMakarov,IvanYakovlev,PavelMalov,AndreiBalykin,andAntonOkhotnikov,“Theidr&dvoxcelebspeakerrecognitionchallenge2023systemdescription,”arXivpreprintarXiv:2308.08294,2023.[5]VassilPanayotov,GuoguoChen,DanielPovey,andSan-jeevKhudanpur,“Librispeech:anasrcorpusbasedonpublicdomainaudiobooks,”in2015IEEEinternationalconferenceonacoustics,speechandsignalprocessing(ICASSP).IEEE,2015,pp.5206–5210.[6]ThomasWolf,LysandreDebut,VictorSanh,JulienChau-mond,ClementDelangue,AnthonyMoi,PierricCistac,TimRault,R´emiLouf,MorganFuntowicz,etal.,“Hug-gingface’stransformers:State-of-the-artnaturallanguageprocessing,”arXivpreprintarXiv:1910.03771,2019.[7]ChanghanWang,YunTang,XutaiMa,AnneWu,SravyaPopuri,DmytroOkhonko,andJuanPino,“Fairseqs2t:Fastspeech-to-textmodelingwithfairseq,”arXivpreprintarXiv:2010.05171,2020.[8]AlexeiBaevski,YuhaoZhou,AbdelrahmanMohamed,andMichaelAuli,“wav2vec2.0:Aframeworkforself-supervisedlearningofspeechrepresentations,”Advancesinneuralinformationprocessingsystems,vol.33,pp.12449–12460,2020.[9]AlexeiBaevski,Wei-NingHsu,QiantongXu,ArunBabu,JiataoGu,andMichaelAuli,“Data2vec:Ageneralframeworkforself-supervisedlearninginspeech,visionandlanguage,”inInternationalConferenceonMachineLearning.PMLR,2022,pp.1298–1312.[10]JunichiYamagishi,XinWang,MassimilianoTodisco,MdSahidullah,JosePatino,AndreasNautsch,XuechenLiu,KongAikLee,TomiKinnunen,NicholasEvans,etal.,“Asvspoof2021:acceleratingprogressinspoofedanddeepfakespeechdetection,”inASVspoof2021Workshop-AutomaticSpeakerVerificationandSpoofingCoutermeasuresChallenge,2021.[11]ZhizhengWu,TomiKinnunen,NicholasEvans,JunichiYamagishi,CemalHanilc¸i,Md.Sahidullah,andAlek-sandrSizov,“ASVspoof2015:thefirstautomaticspeakerverificationspoofingandcountermeasureschallenge,”inProc.Interspeech2015,2015,pp.2037–2041.[12]XinWang,JunichiYamagishi,MassimilianoTodisco,H´ectorDelgado,AndreasNautsch,NicholasEvans,MdSahidullah,VilleVestman,TomiKinnunen,KongAikLee,etal.,“Asvspoof2019:Alarge-scalepublicdatabaseofsynthesized,convertedandreplayedspeech,”Com-puterSpeech&Language,vol.64,pp.101114,2020.[13]Zhen-HuaLing,XiaoZhou,andSimonKing,“Thebliz-zardchallenge2021,”inProc.BlizzardChallengeWork-shop,2021.[14]ZhongjieBa,QingWen,PengCheng,YuweiWang,FengLin,LiLu,andZhenguangLiu,“Transferringaudiodeep-fakedetectioncapabilityacrosslanguages,”inProceed-ingsoftheACMWebConference2023,2023,pp.2033–2044.[15]IvanYakovlev,AntonOkhotnikov,NikitaTorgashov,RostislavMakarov,YuriVoevodin,andKonstantinSi-monchik,“Voxtube:amultilingualspeakerrecognitiondataset,”inProc.Interspeech,2023,pp.2238–2242.[16]ChristopheVeaux,JunichiYamagishi,andSimonKing,“Thevoicebankcorpus:Design,collectionanddataanalysisofalargeregionalaccentspeechdatabase,”in2013internationalconferenceorientalCOCOSDA(O-COCOSDA/CASLRE).IEEE,2013,pp.1–4.[17]AlexandreD´efossez,JadeCopet,GabrielSynnaeve,andYossiAdi,“Highfidelityneuralaudiocompression,”arXivpreprintarXiv:2210.13438,2022.[18]NicolasMM¨uller,PavelCzempin,FranziskaDieckmann,AdamFroghyar,andKonstantinB¨ottinger,“Doesaudiodeepfakedetectiongeneralize?,”Interspeech,2022.[19]HaoxinMa,JiangyanYi,ChenglongWang,XinruiYan,JianhuaTao,TaoWang,ShimingWang,andRuiboFu,“Cfad:Achinesedatasetforfakeaudiodetection,”SpeechCommunication,p.103122,2024.[20]IgorSz¨oke,MiroslavSk´acel,LadislavMoˇsner,JakubPaliesek,andJanˇCernock`y,“Buildingandevaluationofarealroomimpulseresponsedataset,”IEEEJournalofSelectedTopicsinSignalProcessing,vol.13,no.4,pp.863–876,2019.[21]DavidSnyder,GuoguoChen,andDanielPovey,“Mu-san:Amusic,speech,andnoisecorpus,”arXivpreprintarXiv:1510.08484,2015.[22]JoachimThiemann,NobutakaIto,andEmmanuelVin-cent,“Thediverseenvironmentsmulti-channelacousticnoisedatabase(demand):Adatabaseofmultichannelen-vironmentalnoiserecordings,”inProceedingsofMeet-ingsonAcoustics.AIPPublishing,2013,vol.19.[23]AnnamariaMesaros,ToniHeittola,andTuomasVirtanen,“Amulti-devicedatasetforurbanacousticsceneclassifi-cation,”arXivpreprintarXiv:1807.09840,2018.[24]SangdooYun,DongyoonHan,SeongJoonOh,SanghyukChun,JunsukChoe,andYoungjoonYoo,“Cutmix:Reg-ularizationstrategytotrainstrongclassifierswithlocal-izablefeatures,”inProceedingsoftheIEEE/CVFinter-nationalconferenceoncomputervision,2019,pp.6023–6032.[25]HongyiZhang,MoustaphaCisse,YannNDauphin,andDavidLopez-Paz,“mixup:Beyondempiricalriskmini-mization,”arXivpreprintarXiv:1710.09412,2017.[26]HemlataTak,MadhuKamble,JosePatino,MassimilianoTodisco,andNicholasEvans,“Rawboost:Arawdataboostingandaugmentationmethodappliedtoautomaticspeakerverificationanti-spoofing,”inICASSP2022-2022IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP).IEEE,2022,pp.6382–6386.[27]DanielSPark,WilliamChan,YuZhang,Chung-ChengChiu,BarretZoph,EkinDCubuk,andQuocVLe,“Specaugment:Asimpledataaugmentationmethodforautomaticspeechrecognition,”arXivpreprintarXiv:1904.08779,2019.[28]IlyaLoshchilovandFrankHutter,“Decoupledweightdecayregularization,”arXivpreprintarXiv:1711.05101,2017.
47

[29]FengWang,JianCheng,WeiyangLiu,andHaijunLiu,“Additivemarginsoftmaxforfaceverification,”IEEESig-nalProcessingLetters,vol.25,no.7,pp.926–930,July2018.[30]AlexanderCowen-Rivers,WenlongLyu,RasulTutunov,ZhiWang,AntoineGrosnit,Ryan-RhysGriffiths,Alexan-dreMaravel,JianyeHao,JunWang,JanPeters,andHaithamBouAmmar,“Hebo:Pushingthelimitsofsample-efficienthyperparameteroptimisation,”JournalofArtificialIntelligenceResearch,vol.74,072022.[31]RichardLiaw,EricLiang,RobertNishihara,PhilippMoritz,JosephEGonzalez,andIonStoica,“Tune:Are-searchplatformfordistributedmodelselectionandtrain-ing,”arXivpreprintarXiv:1807.05118,2018.[32]MichaelJDPowell,“Aviewofalgorithmsforoptimiza-tionwithoutderivatives,”MathematicsToday-BulletinoftheInstituteofMathematicsanditsApplications,vol.43,no.5,pp.170–174,2007.[33]Hye-jinShim,Jee-weonJung,TomiKinnunen,NicholasEvans,Jean-FrancoisBonastre,andItshakLapidot,“a-dcf:anarchitectureagnosticmetricwithapplicationtospoofing-robustspeakerverification,”arXivpreprintarXiv:2403.01355,2024.