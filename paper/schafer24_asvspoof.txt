The Automatic Speaker Verification Spoofing Countermeasures Workshop (ASVspoof 2024)
31 August 2024, Kos, Greece

56

10.21437/ASVspoof.2024-9

RobustAudioDeepfakeDetection:ExploringFront-/Back-EndCombinationsandDataAugmentationStrategiesfortheASVspoof5ChallengeKarlaSch¨afer⋆,MatthiasNeu†,Jeong-EunChoi⋆⋆FraunhoferInstituteforSecureInformationTechnology,ATHENE†FederalOfficeforInformationSecurity(BSI){karla.schaefer,jeong-eun.choi}@sit.fraunhofer.de,matthias.neu@bsi.bund.deAbstractTherobustnessandgeneralizabilityofaudiodeepfakedetectorsarebecomingmoreimportantduetothetechnicaladvancesingenerationmethodsandthewidespreadusageofaudiodeep-fakes.TheASVspoof5challengeaddressesthisbyprovidinganewdataset.ThispaperpresentsFraunhoferSIT’santi-spoofingdetectorssubmittedtotheASVspoof5challenge.AASIST(-L),RawGAT-STanddataaugmentationwasusedintheclosedcon-dition.Intheopencondition,weevaluateddifferentSSL-basedfront-endsusingdiversetrainingdata.Theresultsindicatethattheutilisationofextensivedataaugmentationimprovethere-sultswhenusinganonSSL-basedfront-end,whereasitsincor-porationwithanSSL-basedfront-endledtoadeclineinperfor-mance.TheimplementationofalargeSSLfront-endimprovedtheresult.OurbestdetectorintheclosedsettingattainedaminDCFof0.589andthebestintheopencondition(usingSSL)aminDCFof0.174ontheASVspoof5evaluationset.1.IntroductionAstheamountofAI-generatedaudiocontentonsocialmediarapidlyincreases,thereisagrowingnecessitytodevelopde-tectorsthatcandistinguishbetweenAI-generatedandgenuineaudio.ToolslikeElevenLabsorRVC(RetrievalbasedVoiceConversion)allowpeoplewithminimalbackgroundknowledgetogenerateaudiodeepfakes,whichdecreasethebarrierforgen-eratingaudiodeepfakesandconsequentlyincreasethepotentialmisuseofspeechgenerationtechnologies.Animportantfactorofthesedetectorsistheirabilitytogeneralizeonnewgenera-tionmethodsandtoberobustagainstadaptiveoradversarialat-tacks,i.e.inreal-worldscenarios.Onecommonlyusedtestsetforevaluatingdetectorsintheirabilitytodetectreal-worlddataisthein-the-wilddataset[1].Itcontainsrealandspoofedau-diorecordingsofEnglish-speakingcelebritiesandpoliticians.Usingthein-the-wilddatasetastestset,mainlyself-supervisedlearning(SSL)-baseddetectorsprovedtobethebestperformingmethodsonsuchreal-worlddata.Forexample,[2]developedtheSOTAdetectoronthein-the-wilddatasetachievinganEERof4.07%.TheirmodelconsistsofanSSL-basedfront-endandneuronalnetworkback-endasclassifier.IntheASVspoof5challenge,anewdatasetisintroducedwiththegoalofdevelopingadetectorwithgreatergeneral-izabilityandrobustness.AssourcedataoftheASVspoof5dataset,theMultilingualLibrispeech(MLS)dataset[3]initsEnglish-languagesubsetwasused.Itcontainsspeechrecord-ingscollectedfromnumerousspeakersinvariousrecordingconditions.Asgenerationmethods,differenttext-to-speech(TTS)andvoiceconversion(VC)methodswereused.Thegenerationmethodsare,atthetimeofwritingthispaper,un-known.Theparticipantsareaskedtodevelopadetectorunderopenandclosedcondition.Forcreatingourdetectors,priortothechallenge,weconductedsomepreparatoryexperimentsonRawNet2[4],RawGAT-ST[5],AASIST(-L)[6]andPC-DARTS[7]toevaluatewhichdetectorstructureperformsbest.Forthis,wetraineddifferentfront-endandback-endcombi-nationsontwodifferentdatasets.ThebestcombinationwasthenusedintheclosedconditionoftheASVspoof5challenge.AsSSLmodelscurrentlyachievethebestresultswhenfacedwithgeneralizability,intheopencondition,wetesteddifferentSSLmodelsasfeatureextractionmethod.Asback-endweusedthebestclassifierfromtheclosedcondition,beingAASIST.Inbothconditions,weapplieddataaugmentationwiththeaimtoincreasethetrainingsetandtoimprovetherobustnessofthedetectorsagainstadaptiveattacks.Inthefollowing,first,thetaskofthechallengeisdescribedshortly(Section2).InSection3,ourmethodsconsistingoffeatureextraction,classifieranddataaugmentationmethodsarepresented.InSection4theexperimentsarepresented,followedbytheresults(5)andtheconclusion(Section6).2.TaskDescriptionTheASVspoof5challengeaimstogenerategeneralizableandrobustcountermeasures,followingthepreviouslyconductedASVspoof2021challenge.Therefore,thedetectorsareex-pectedtoperformreliablyevenonutterancesgeneratedwithneworunseenspoofingattacks,recordingswithnon-studio-qualityandpost-processeddata.Thechallengewasconductedintwoconditions:theclosedandopencondition.IntheclosedconditiononlythetrainingpartitionoftheprovidedASVspoof5wasallowedtobeused.Additionally,itwasnotallowedtousepre-trainedmodels.Intheopencon-dition,participantswereallowedtouseexternaldataandpre-trainedmodels.Hereby,anydatacontainedwithinLibriLight[8],MultilingualLibrispeech(MLS)Englishdatasets[3],andtheMUSANcorpusspeechsubset[9]wererestricted.There-strictionalsoappliedtoanyotherdataset,pre-trainedmodel,oranysourcederivedfromthesedatasets.3.MethodsWeanalyseddifferentcombinationsoffront-endfeatureextrac-tionmethodsandback-endclassifier.Furthermore,weapplieddifferentdataaugmentationtechniques.ForanoverviewoftheprocessseeFigure1.
57

Figure1:Overviewofourexperimentsconducted(Red:DataAugmentation).3.1.FeatureExtractionDependingontheconditionofthechallenge,weuseddif-ferentfeatureextractionmethods.Intheopenchallenge,wewereabletouseSSL-basedfront-ends.Asonlyfront-endsnotpre-trainedonMLSorotherdatabasessourcedfromLibriVoxaudiobooks[10]wereallowed,wechoosetouseHuBERTbase[11],HuBERT-MGR[12](hubertbaserobustmgr)andWav2Vec2.0large[13](wav2vec2large960),allpre-trainedontheLibriSpeech[14]corpora.Intheclosedcondition,nopre-trainedmodelswereal-lowed.InthepreparatorytestsweexaminedRawNet2,AASIST(-L),RawGAT-STandPC-DARTS.ForRawNet2,AASIST(-L)andRawGAT-ST,thefeatureextractionmethodsarebasedonSincNet.SincNet[15]isaCNNwhichcanbeappliedontherawwaveform.Itisbasedonparametrizedsincfunctions,whichimplementband-passfilters.IncontrasttostandardCNNs,thatlearnallelementsofeachfilter,onlylowandhighcutofffrequenciesaredirectlylearnedfromdatawiththeproposedmethod.Inthisway,SincNetencouragesthefirstconvolutionallayertodiscovermoremeaningfulfea-tures.Additionally,inourpreparatorytestsweusedlinear-frequencycepstrumcoefficients(LFCC),basedontheimple-mentationprovidedbytheauthorsofPC-DARTS.3.2.ClassifierAsback-ends,inourpreparatoryanalysis,weappliedRawNet2,RawGAT-ST,AASIST(-L)andPC-DARTS.Inthechallenge,weappliedthebestperformingmodelsofourpreparatoryanal-ysis,beingAASIST(-L)andRawGAT-ST.Inthefollowing,ashortexplanationofthemodelsusedaregiven.RawNet2isbasedontheRawNet2architecture,modifiedtobeappliedonanti-spoofing[4].NewtoRawNet2istheappli-cationoffilter-wisefeaturemapscaling(FMS)usingasigmoidfunctionappliedtoresidualblockoutputs.Theembeddingdi-mensionforRawNet2isalsogreatlyincreased,from128forRawNet1to1024forRawNet2.RawGAT-STisbasedontheideathatartefactsfordistin-guishingbonafidefromspoofedspeechresideinspecificsub-bandsandtemporalsegments[5].Theirdetectorusesaspectro-temporalgraphattentionnetwork(GAT)tolearntherelation-shipsbetweencuesspanningdifferentsub-bands(spectralandtemporal)andtemporalintervals.AASISTisbasedonRawGAT-STandfocusesalsoontheideatobuildonesystemfordetectingartefactsinboth,spectralandtemporalintervals[6].Itslightweightvariantwithonly85kparametersisAASIST-L,AASISTusestheRawNet2-baseden-coder(basedonSincNet)forextractinghigh-levelfeaturemapsfromrawinputwaveform.PC-DARTSappliesdifferentiablearchitecturesearch(DARTS)onthedeepfakedetectiontask[7].PC-DARTScon-sistoftwosteps:1)thearchitecturesearch,weregivenatrain-ingsettheoptimalarchitectureisdeterminedbyoptimisingthestructureofastackoftwonormalcellsandonereductioncell,and2)thetrainingofthefoundarchitecture.Thecellarchitec-tureisfixedinthefirststep,inthesecondsteponlythenetworkparametersareoptimized.WithPC-DARTS,theauthorswereabletoachievesubstantialsavingsintermsofcomputingcom-plexityandmemory.3.3.DataAugmentationFordataaugmentation,twodifferentmethodtypeswerealter-natedandcombinedappliedonthetrainingdata.First,Raw-Boost[16]wasappliedusingaseriesoflinearandnon-linearconvolutivenoiseandimpulsivesignaldependentnoise(algo:5inallsettings),seealsotheworkof[17].Secondly,usingtheaudiomentationslibrary1,thedataaugmentationtechniquesaddinggaussiannoise(p=0.1)andmp3compression(p=0.5)wereapplied.Furthermore,ineachcondition,onourbestperformingde-tectorstructure,weappliedBandStopFilter,SevenBandPara-metricEQandTanhDistortionfromtheaudiomentationslibraryonthetrainingset.Hereby,weconcatenatedtheexistingtrain-ingsetwithaugmented(p=0.6)versionsofthesamedatasetusingthethreeaugmentationtypes.Wehopedtoincreaseanddiversifythetrainingsetwiththisapproach.WithBandStop-Filterweaimedtopreventthemodelfromoverfittingtospe-cificfrequencyrelationships,helpingtomakethemodelrobusttodiverseaudioenvironmentsandscenarioswherefrequencylossescanoccur.WithSevenBandParametricEQwetriedtomakethemodelmorerobusttovariousfrequencyspectrumsandTanhDistortionwasincludedforaddingharmonics.4.ExperimentsPreviouslytothechallengeweperformedsomepreparatorytestsusingtheASVspoof2019LA[18]datasetandWaveFake[19]withitsgenuinesamplesoftheLJSpeechcorpora[20].TheASVspoof2021LAandDFsplits[21]weredevelopedtoevaluatetherobustnessandgeneralizabilityofdetectors.Inourpreparatoryanalysis,weusedthemastestsets.Furthermore,weusedthein-the-wilddataset[1]toevaluatethegeneraliz-abilityofthemodelsonunseenreal-worldattacks.ThebestmodelcombinationsofthesepreparatorytestswerethenusedduringthechallengeintheclosedsettingusingtheASVspoof5datasetastrainingset.1https://iver56.github.io/audiomentations/
58

4.1.DatasetsASVspoof2019isbasedupontheVCTKcorpus,withasetof107speakers.IntheLogicalAccess(LA)scenario,forthetrainingset,spoofedutterancesweregeneratedusingfourTTSandtwoVCmethods.Withthis,thetrainingsetcon-sistsof2,580bonafideand22,800spoofedutterances.Assecondtrainingset,weusedWaveFakeanditsgenuinecoun-terpartLJSpeech[20].TheWaveFake[19]datasetcontains117,985spoofedsamples.Mostly,thedatasetisbasedontheEnglishLJSpeechdataset[20].Additionally,theJSUTdatasetwasused,containingJapanesespeechsamples.TheLJSpeechdatasetconsistsof13,100shortaudioclipswithanaverageof6secondrecordingseach(24hoursintotal)readbyafemalespeaker.TheLJSpeechdatasetwasusedheretorepresentgen-uinesamples.IntheclosedsettingoftheASVpoof5challenge,weonlyusedtheASVspoof5trainingset.Intheopencondi-tion,weusedacombinationoftheWaveFake(+LJSpeech),LibriSeVoc[22],ASVspoof2019LA,Fake-or-Real(FoR)[23]andASVspoof5datasets.TheFake-or-Real(FoR)dataset[23]containsgenuineandspoofedaudiosamplesfromsevenSOTATTSmodels.ThegenuinedatawassourcedfromYouTubeandopen-sourcedatasetslikeArcticDataset,LJSpeechandVox-Forge.FourdifferentversionsoftheFoRdatasetexist:FoR-original,FoR-norm,FoR-2secondsandFoR-rerecorded.WeusedtheFoR-originalversion.FoR-originalcontainsthefilesascollectedfromthespeechsources,withoutanymodifica-tionorclass/genderbalancing.TheLibriSeVocDataset[22]wascreatedforvocoderartefactdetectionandcontains13,201genuineaudiosamplesand79,205spoofaudiosamplesgen-eratedthroughself-vocodingby6vocoders,eachrepresentedwith13,201samples.Thedatasetcontains34.92hoursofgen-uineaudiowithsamplesofaudiolengthsfrom5secondsto34secondsat24kHz.4.2.ImplementationDetailsInthepreparatorytestsandtheexperimentsintheclosedcon-dition,theoriginalimplementationsofthemodelsRawNet2,RawGAT-ST,AASIST(-L)2andPC-DARTS3wereused.IntheopenconditionweusedSSL-basedfeatureextraction.Forthis,wemodifiedthecodeof[17]4,usinganAASISTclassifierto-getherwithSSL-modelsasfront-end.FortheSSL-basedfront-endimplementation,weusedtheS3PRLlibrary5.Forallexperiments,weappliedaweightedsampler.There-fore,wemadesurethateachdatasetcontains50%ofbonafidesamples.Thelosswascalculatedusingcross-entropylossforallback-ends.Allrecordingsusedfortrainingweretrimmedandpaddedtoapprox.4secondsofaudioclips.Thenon-SSL-baseddetectorsweretrainedfor70epochs,withabatchsizeof16.PC-DARTSinthearchitecturesearchphasewastrainedfor50epochs,asproposedinthepaper.Thearchitecturewiththehighestdevelopmentandtrainingaccuracywastakenfortrain-ingthefinalclassifier.TheSSL-baseddetectorsweretrainedforanumberofepochsdependingonthetrainingsetused.Thenumbersofepochswerecalculatedbasedonthelengthofthetrainloader(100/(len(trainloader)/4000)).Depending2https://github.com/clovaai/aasist/tree/main?tab=readme-ov-file3https://github.com/eurecom-asp/pc-darts-anti-spoofing/tree/main4https://github.com/TakHemlata/SSL_Anti-spoofing5https://s3prl.github.io/s3prl/tutorial/upstream_collection.htmlonthetrainingset,thisresultedin15(ASVspoof5,ASVspoof2019LA,WaveFake+LJ,LibriSeVoc,FoR)to35(ASVspoof5)epochs.Thebatchsizewassetto16.4.3.EvaluationMetricsInourpreparatoryanalysisweusedthecommonlyusedEqualErrorRate(EER)asevaluationmetric.Inthechallenge,weusedtheprovidedCodaLabplatformforcalculatingthemini-mum/actualdetectioncostfunction(min/actDCF),thecostoflog-likelihoodratio(Cllr)andtheEER.5.ResultsInouranalysiswetesteddifferentfront-endandback-endcom-binations,wherebythefront-endconsistedofeitheranSSL-basedfeatureextractionmethodoraLFCC/SincNetbasedfea-tureextraction.Allresultswereobtainedgivingthewholesam-pleastestdata(notrimmingofthetestdata).Dataaugmen-tationwithRawBoostwasusedinalltheexperiments.Inapreparatoryanalysis,wetesteddifferentnon-SSL-baseddetec-tors.ThebestmethodwasusedintheclosedconditionoftheASVspoof5challenge.Fortheopensetting,weappliedAA-SIST,beingthebestmodelintheopencondition,withdifferentSSL-basedfront-ends.5.1.PreparatoryTestsInthepreparatorytests,fornon-SSLbaseddetection,weevalu-atedtwodifferentfront-ends,SincNetandLFCCusingtwodif-ferenttrainingsets(ASVspoof2019LAandWaveFake+LJ).Asclassifier,wetestedAASIST,AASIST-L,RawNet2,RawGAT-STandPC-DARTS.Withtheseexperiments,wewantedtofindoutwhichcombinationof(non-SSL-based)front-endandback-endleadtothebestresults,usingdifferenttrainingdata.Astestsets,weusedthein-the-wilddatasetandtheASVspoof2021LAandDFtestsplits.Additionally,weapplieddataaugmen-tationforenhancingourtrainingset,withtheaimtomakethedetectormorerobustagainstdataprocessingsuchasmp3com-pression.InTable1theresultsofthenon-SSL-baseddetectorstrainedwiththeASVspoof2019LAortheWaveFakedatasetandtestedonthein-the-wilddatasetandtheASVspoof2021LA/DFsplitaregiven.AsonecanseeinTable1,whenviewingtheresultsonthein-the-wilddataset,theresultswhenusingWaveFakeweresuperiorofthemusingASVspoof2019LAastrainingdata.WhenviewingtheASVspoof2021LAandDFtestsetsitwasthetrainingsetASVspoof2019LA.Forexample,thebestEERusingtheASVspoof2019LApartitionastrainingdatawasRawGAT-STwithaSincNetfront-end,withanEERof18.08%.WhenusingtheWaveFakedatasetastrainingdata,theRawGAT-STwithSincNetfront-endachievedanEERof13.07%onthein-the-wilddataset.But,usingWaveFakeastrainingdata,AASISTwiththeSincNetfront-endperformedevenbetterwithanEERof11.83%,beingthebestdetec-toronthein-the-wilddatasetinthistable.Usingthetrain-ingsetASVspoof2019LAprovidedthesuperiorresultsontheASVspoof2021LAandDFsplits,whichisnotsurprisingastheASVspoof2021LAdatasetisbasedontheASVspoof2019LApartitionandtheASVspoof2021DFpartiallyonASVspoof2019LA.ThebestEERsof20.72%(ASVspoof2021LA-all)and19.64%(ASVspoof2021DF-all)werehereachievedwithAASISTandtheSincNetfront-end.UsingtheWaveFakedatasetastrainingdataonlyscoresofupto38.10%(ASVspoof2021LA-all)and28.74%(ASVspoof2021DF-
59

Table1:Resultsinthepreparatoryanalysisonnon-SSL-baseddetectorsusingtheASVspoof2019LAandWaveFakeastrainingdata(dataaugmentation:GaussianNoise,Mp3compression).ThebestEERspertrainingsetaremarkedingrey.Front-endBack-endTrainingEER(%)EER(%)ASVspoof2021LAEER(%)ASVspoof2021DFDatain-the-wildall/eval/progress/hiddenall/eval/progress/hiddenSincNetAASISTASVspoof2019LA23.5520.72/20.81/19.75/20.5719.64/20.07/19.00/18.99SincNetAASIST-LASVspoof2019LA26.8626.07/26.12/24.82/25.8125.17/25.32/24.29/24.39SincNetRawGAT-STASVspoof2019LA18.0821.39/21.39/21.06/21.9021.43/21.85/21.56/21.37SincNetRawNet2ASVspoof2019LA34.7223.71/23.14/22.85/26.2326.73/27.06/22.53/25.90LFCCPC-DARTSASVspoof2019LA38.9533.80/33.07/31.62/40.3635.26/36.27/29.94/36.92SincNetAASIST-LWaveFake+LJ14.5138.10/38.09/37.41/39.0328.74/27.25/36.46/38.29SincNetAASISTWaveFake+LJ11.8341.49/41.14/41.04/44.9032.14/30.92/39.79/43.49LFCCAASISTWaveFake+LJ54.9450.95/50.85/50.84/51.3850.17/48.23/50.28/52.02SincNetRawGAT-STWaveFake+LJ13.0740.97/40.75/40.57/42.8031.86/30.40/39.58/41.62LFCCRawGAT-STWaveFake+LJ56.1048.83/48.78/48.39/49.6447.91/45.71/46.24/47.98SincNetRawNet2WaveFake+LJ23.1441.45/41.37/40.81/42.8637.10/36.67/40.07/41.12LFCCPC-DARTSWaveFake+LJ43.1145.71/45.98/45.65/44.7033.80/33.07/31.62/40.36all)wereachieved,usingAASIST-L(alightweightedversionofAASIST)andaSincNetfront-end.Regardingthefront-ends,LFCCperformedratherpoorcomparedtoSincNet.Forexam-ple,withtheWaveFaketrainingdatausingAASISTandcom-paringtheperformanceofitwithaLFCCandaSincNetfront-end,withLFCCanEERof54.94%onthein-the-wilddatasetwasreached,withSincNetitwas11.83%.ThesamehappenedwhenevaluatingRawGAT-STonthesetwodifferentfront-ends.Comparedtotheotherclassifiers,PC-DARTSperformedworsewithanEERof38.95%onthein-the-wilddatasetwhentrainedontheASVspoof2019LAdatasetand43.11%whentrainedonWaveFake.However,sinceLFCCwasusedasthefront-endhere,theresultsarenotasbadwhencomparedtotheotherdetectorsusingtheLFCCfront-end.OneideaforfutureworkwouldbetotestPC-DARTSwithaSincNetfront-end.Overall,themodelsperformedratherdifferently,depend-ingonthetrainingsetsused.But,theSincNetbasedfront-endprovidedbetterresultswhentrainedonbothtrainingsets.Con-sequently,weemployedSincNetbasedfront-endsintheclosedconditionoftheASVspoof5challenge.Furthermore,weevalu-atedAASIST(-L)andRawGAT-STastheseback-endsprovidedthebestresults.5.2.ChallengeResultsInthechallenge,weevaluateddifferentcombinationsanddataaugmentationtechniquesinthetwoconditions:closedandopen.5.2.1.ClosedConditionBecauseoftheresultsinourpreparatoryanalysis(seeTable1)wetestedAASIST(-L)andRawGAT-STwiththeiralreadyim-plementedSincNetbasedfront-endontheASVspoof5progressset.AllmodelsweretrainedontheASVspoof5trainingsetonly.Thedataaugmentationtypeswerevaried,seeTable2fortheresultsofthedifferentcombinationsontheASVspoof5progressset.Withoutdataaugmentation(onlyusingRaw-Boost),RawGAT-STandAASISTperformednearlythesamewithaminDCFof0.423and0.422.AASIST-Lperformedworse,withamin-DCFof0.518,whichiswhyAASIST-Lisnotconsideredfurther.Usinggaussiannoiseandmp3compres-sion,AASISTperformedbetterthenRawGAT-STwithaminDCFof0.416(RawGAT-ST:0.436).UsingAASIST,theuseofthesedataaugmentationmethodsimprovedtheresults.Us-ingRawGAT-STtheresultsworsen.Therefore,wealsotestedAASISTwhenincreasingthetrainingssetwithdataaugmen-tationsusingBandStopFilter,SevenBandParametricandTan-hDistortion.ThisimprovedtheresultstoaminDCFof0.354,beingalsoourbestscoreinthissetting.Inalaststep,wecombinedbothdataaugmentationtechniques,addinggaussiannoise,mp3compressionandBandStopFilter,SevenBandPara-metricandTanhDistortion.Thisdeterioratedtheresultsslightly,leadingtoaminDCFof0.358.5.2.2.OpenConditionIntheopenconditionwewereallowedtouseadditionaldatasetsandpre-trainedmodels.Therefore,weappliedthreedifferentSSL-basedfront-ends,usingAASISTaspreviousbestmodelasback-end.SeeTable3fortheresultsontheASVspoof5progressset.Fordataaugmentationwefirstappliedgaus-siannoiseandmp3compression,asitimprovedtheresultsinthefirstsetting.Additionally,wetesteddifferenttrainingsetsusingtheASVspoof5,WaveFake(+LJSpeech),LibriSe-Voc,ASVspoof2019LAandtheFoRdataset,inthehopetoimprovetheresultsofthedetectorwhenitistrainedondiversedata.Testingthreedifferentfront-ends,theresultswhenusingallthesetrainingdatadeteriorated,comparedtowhenonlyus-ingtheASVspoof5trainingset.Forexample,withaHuBERTbasefront-end,aminDCFof0.102wascalculatedwhentrainedonASVspoof5,whentrainedonallthetrainingdatadescribedaminDCFof0.155wasreached.Weassume,itmaybethecasethattheuseofasubstantialquantityoftrainingdatadoesnotnecessarilyleadtoenhancedoutcomes.Alternatively,itispossiblethatthemodeldimensionisinsufficientlysized.Overall,thefront-endWav2Vec2.0(large960),beingalsotheonewiththelargestembeddingdimension(1024)andmostnumbersofparameters(317M)[13],performedbetterthantheothertwofront-endstested(Wav2Vec2.0trainedonASVspoof5:0.066;HuBERTbasetrainedonASVspoof5:0.102;HuBERTbaserobustmgrtrainedonASVspoof5:0.130).HuBERTbaserobustperformedworst.AsWav2Vec2.0beingthebestfront-end,wetesteditsperformanceusingtheASVspoof5trainingsetandcombiningitwiththeASVspoof2019LAset,asthistrainingsetisfromapreviouschallenge,wehopedtoimprovetheresultswiththispossiblysimilarstruc-turedbutadditionaldata.Usingthisadditionaldataset,theminDCFslightlyimprovedto0.063(previously,withoutASVspoof2019LA:0.066).Thiswasalsoourbestresultinthissetting.Additionally,weagainextendedthedifferenttrainingsetcombinationsbyusingBandStopFilter,SevenBandPara-metricandTanhDistortion.Incontrasttotheclosedcon-dition,inallsettings,theresultsworsenwiththeuseofthesedataaugmentationtechniques(ASVspoof5:0.081/0.066;ASVspoof5+ASVspoof2019LA:0.074/0.063;alldata:0.144/0.097).Interestingly,theresultswhenusingthesethreedataaugmentationtypesonASVspoof5onlyandonacom-
60

Table2:ResultsintheclosedconditionontheASVspoof5progressset.Thebestresultsintermsofmin-DCFaremarkedingrey.Back-endDataAugmentationminDCFactDCFCllrEERGaussianNoise,Mp3compressionBandStopFilter,SevenBandParametric,TanhDistortionRawGAT-ST--0.4230.5731.06617.66RawGAT-ST✓-0.4360.6591.01118.09AASIST-L✓-0.5180.6671.33321.44AASIST--0.4220.550.84416.89AASIST✓-0.4160.5920.98317.16AASIST-✓0.3540.4790.73113.71AASIST✓✓0.3580.5900.88014.19Table3:ResultsintheopenconditionontheASVspoof5progressset.Asclassifier,weusedAASISTinallsettings.Thebestresultsintermsofmin-DCFaremarkedingrey.Abbreviation:alldata:ASVspoof5,WaveFake(+LJ),LibriSeVoc,ASVspoof2019LA,FoR.Front-endDataAugmentationTrainingDataminDCFactDCFCllrEERGaussianNoise,Mp3compressionBandStopFilter,SevenBandParametric,TanhDistortionhubertbase✓-ASVspoof50.1020.2480.3933.90hubertbase✓-alldata0.1550.1560.2225.43hubertbaserobustmgr✓-ASVspoof50.1300.2230.3765.14hubertbaserobustmgr✓-alldata0.1510.1560.2265.35wav2vec2large960✓-ASVspoof50.0660.2910.4752.58wav2vec2large960✓-ASVspoof5,ASVspoof2019LA0.0630.1130.1642.27wav2vec2large960✓-alldata0.0970.1010.1363.37wav2vec2large960-✓ASVspoof50.0810.2820.5163.07wav2vec2large960-✓ASVspoof5,ASVspoof2019LA0.0740.1780.2242.66wav2vec2large960-✓alldata0.1440.1550.2085.04binationofASVspoof5andASVspoof2019LA,theuseofASVspoof2019LAimprovedtheminDCFfrom0.081to0.074.Withoutthesedataaugmentation,theadditionaluseofASVspoof2019LAonlyimprovedtheminDCFslightly(0.066to0.063).Intheopenandclosedsetting,wetestedAASISTwhentrainedontheASVspoof5trainingsetonlyandapplyinggaus-siannoiseandmp3compressionfordataaugmentation.Theonlydifferencewasthefront-endused,beingSincNetbasedintheclosedconditionandSSL-basedintheopencondition.ThissettingallowsustoseetheeffectoftheSSL-basedfront-end.WiththeSincNetbasedfront-endaminDCFof0.416wascalculated.AsSSL-basedfront-ends,wetestedthreedif-ferentmodelsinthesamesetting.WiththeWav2Vec2.0front-endaminDCFof0.066wascalculated,withHuBERTbase0.102andwithHuBERTbaserobustmgraminDCFof0.130.ThroughtheSSL-basedfeatureextractiontheresultshaveim-provedheavily.5.2.3.ASVspoof5EvaluationSetLastly,thebestdetectorsontheASVspoof5progresssetweretestedontheASVspoof5evaluationset.SeeTable4forthere-sults.Bothdetectorsperformedworseontheevaluationsetthanontheprogressset.AASISTtrainedonASVspoof5withdataaugmentationusingBandStopFilter,SevenBandParametricandTanhDistortionachievedaminDCFof0.589ontheevaluationset(progressset:0.354).AASISTwithaWav2Vec2.0front-endandtrainedonASVspoof5andASVspoof2019LAwithdataaugmentationusinggaussiannoiseandmp3compressionscoredaminDCFof0.174ontheevaluationset(progressset:0.063).Forthecreationoftheevaluationset,16attacksand11codecswereused[24].Here,besidestheoverallpooledminDCF,wealsoevaluatedtheperformanceofthetwobestdetec-torsdependentonthedifferentattacktypesandcodecsused.SeeTable5fortheevaluationintheclosedconditionandTableTable4:ResultsofthebestdetectorsontheASVSpoof5evalu-ationset.SystemDescriptionminDCFactDCFCllrEERClosed:AASISTtrainedonASVspoof5withBandStop-Filter,SevenBandParametric,TanhDistortion0.5890.6881.32824.59Open:AASISTwithWav2Vec2trainedonASVspoof5andASVspoof2019LAwithGaussianNoise,Mp3Compression0.1740.3090.4766.066intheopenconditionusinganSSL-basedfront-end.InTable5themostchallengingattackcodeccombinationsaremarkedingrey.Asmostchallengingcombinations,wedescribecombi-nationswithaminDCFhigherthan0.9.Thenon-SSLbaseddetectorhadthemostproblemsidentifyingtheattacksA19,A20andA30,onallthreeattacksaminDCFof1wasreached,evenwhenusingnocodecs.A19isanattackusingMaryTTS[25],A20andA30areadversarialattacks[24].Themostchal-lengingcodecsforournon-SSLbaseddetectorswerecodec-4(Encodec[26]),7(mp3+Encodec)and10(speex)withpooledminDCFof0.675,0.683and0.701.Allthreecodecsscorehigherthan0.9,combinedwithsixdifferentattacks.Addition-ally,wemarkedallbestidentifiedattack-codeccombinations,beingtheonewithaminDCFsmaller0.1.Overall,attackA29(pre-trainedXTTS[27])wasthebestrecognizedattackwithapooledscoreof0.075andhavingscoressmaller0.1incom-binationwitheightofthe11codecs.Thesecondbestrecog-nizedattackwasA21(ToucanTTS[28]+BigVGAN[29])withapooledminDCFof0.148.Ascodecs,nocodecsandcodec-5(mp3)wasrecognizedbest,havingaminDCFsmaller0.1onfiveandfourofthe16attacks.TheSSL-baseddetectorperformedoverallbetter(seeTable6).Therefore,inthissetting,wedefinedthebestattack-codeccombinationswithascoresmaller0.01andthemostchalleng-ingcombinationshavingaminDCFgreater0.5.Withthisdefi-
61

Table5:MinDCFresultsintheclosedconditionontheASVspoof5evaluationsetbyattacktypeandcodec.Themostchallengingattack-codeccombinationsintermsofmin-DCF(heredefinedas>0.9)aremarkedindarkgrey.Thebestcombinations(heredefinedas<0.1)aremarkedinlightgrey(pld:pooled).pld.-codec-1codec-2codec-3codec-4codec-5codec-6codec-7codec-8codec-9codec-10codec-11pld.0.5890.5060.5500.5780.6090.6750.5210.5280.6830.6670.6290.7010.5539A170.1830.0470.1740.1230.1960.2520.0830.1110.2800.3360.3020.2890.104A180.7970.6440.7670.8460.8880.8440.7010.6700.8730.8640.90410.760A191111111111111A2011111110.996110.9020.9461A210.1480.0370.1300.0900.1520.2240.0520.0970.2280.3020.1730.1930.085A220.1930.0910.1520.1530.1810.2900.0990.1390.2990.3190.2590.2670.136A230.4560.3680.3940.4050.5240.5710.3580.3700.5750.5350.4470.5720.385A240.2450.0980.2070.1670.2850.2900.1540.2060.3220.3740.3810.4530.191A250.4780.2120.5020.4930.5200.6810.2850.3300.6730.7230.6920.6720.415A260.7230.5090.7030.7230.7890.9190.5300.6010.9130.8670.9330.9450.715A270.8490.7690.7260.9010.8910.9650.7810.7610.9690.8650.8630.9840.760A280.4650.2470.4070.4740.4800.5640.2870.3760.5990.6410.7040.7790.454A290.0750.0060.0840.0220.0440.0770.0120.0320.1100.2440.1050.0990.023A30110.9741111110.9990.99611A310.4570.4080.3390.4140.4470.5780.4090.4260.6060.4920.4080.5250.420A320.7780.7440.7140.8000.8030.9460.7710.7150.9360.7570.6330.7880.613Table6:MinDCFresultsintheopenconditionontheASVspoof5evaluationsetbyattacktypeandcodec.Themostchallengingattack-codeccombinationsintermsofmin-DCF(heredefinedas>0.5)aremarkedindarkgrey.Thebestcombinations(heredefinedas<0.01)aremarkedinlightgrey(pld:pooled).pld.-codec-1codec-2codec-3codec-4codec-5codec-6codec-7codec-8codec-9codec-10codec-11pld.0.1740.0560.1280.1190.1570.3340.0650.0820.4140.2910.1660.2090.078A170.0330.0020.0100.0050.0200.0830.0010.0030.1110.0880.0150.0310.006A180.1020.0130.0520.0530.0900.2370.0160.0300.3110.2070.1060.1390.026A190.0250.0020.0080.0050.0160.0630.0010.0040.0860.0470.0090.0180.002A200.0930.0180.0490.0520.0670.2150.0220.0310.2850.1400.0660.0840.021A210.0410.0040.0190.0100.0330.0910.0030.0070.1260.0940.0150.0310.008A220.1310.0300.0840.0750.1200.2870.0320.0450.3750.2520.1080.1630.041A230.1060.0190.0630.0570.0950.2250.0220.0340.2830.2050.1000.1350.028A240.2070.0780.1550.1350.1980.3820.0840.1080.4420.3300.1770.2410.080A250.0770.0100.0370.0320.0540.1880.0140.0180.2520.1590.0700.0950.017A260.0530.0040.0190.0170.0470.1230.0050.0100.1760.1150.0410.0630.009A270.1610.0330.1060.1130.1450.3700.0450.0650.4640.3000.1580.2050.046A280.4700.2420.4180.3610.4460.6690.2620.3310.7610.7190.5360.6600.423A290.0240.0040.0080.0050.0140.0430.0040.0040.0560.0850.0080.0180.007A300.1740.0430.1230.1230.1610.3970.0520.0760.4840.3120.1760.2200.050A310.2690.0880.2390.2560.2800.5050.1140.1350.6150.4440.3090.3560.100A320.1640.0320.1170.1220.1600.4110.0420.0580.5030.2980.1710.2070.037nition,weidentifiedtheattacksA28(pre-trainedYourTTS[30])andA31(malacopula+[31])asthemostchallenging,havingapooledminDCFof0.470and0.269andhavingscoresgreater0.5incombinationwithfiveandtwocodecs.Again,codec-4and7werethemostchallengingcodecs.TheattacksA19(MaryTTS)andA29(pre-trainedXTTS)wereidentifiedastheonesbeingthemostwellrecognizedattacksbythisSSL-baseddetector.AttackA19hasapooledminDCFof0.025andA29apooledminDCFof0.024.BothattackshaveaminDCFsmaller0.01whencombinedwithsixofthe11codecs.Interestingly,A29wasidentifiedaswellrecognisablebybothdetectors.TheattackA19wasoneofthemostchallengingattacksinthefirstsettingusinganon-SSLbaseddetectorandoneofthebestrec-ognizedinthesecondsettingusinganSSL-baseddetector.Fur-thermore,attackA28(pre-trainedYourTTS)standsout,whichwasbetterrecognisedintheclosedcondition(pooledminDCF:0.465)thanintheopen(pooledminDCF:0.470).6.ConclusionWeexamineddetectorswithdifferentfront-/back-endcombi-nationsandfound,thatAASISTandRawGAT-STwithaSinc-Netbasedfront-endperformsbetterondifferenttrainingssetsthanRawNet2,PC-DARTSandusingaLFCCfront-end.More-over,ourfindingsindicatethatanabundanceoftrainingdatamaynotalwaysbebeneficialandcouldpotentiallyleadtosub-optimaloutcomes.Also,thedetectors’architectureincom-binationwiththetrainingdataeffectstheresults.Whenus-inganSSL-basedfront-endheavydataaugmentationwasnotnecessary.Contrarily,withoutanSSL-basedfront-endthere-sultsimprovedwithheavydataaugmentation.Overall,anSSL-basedfront-endhighlyimprovedtheresults,atleastontheASVspoof5progresssplit.Itappearsthattheembeddingdi-mensionandthenumberofparametersoftheSSLmodelhaveanimpactontheresults,withthelargestmodeldemonstratingthemostfavourableoutcomes.Thisisevidencedbythesu-periorperformanceofWav2vec2.0(large960)astheoptimalfront-endamongthethreetested.Whenviewingtheresultsontheevaluationset,forbothdetectorsthecodec-4and7,bothbasedonEncodec,werethemostchallenging.Theattackus-ingpre-trainedXTTS(A29)werewellrecognizedinbothset-tings.MaryTTS(A19)provedtobedifficulttodetectforthenonSSL-baseddetectorintheclosedsetting(pooledminDCFof1)butwerewellrecognizedintheopensetting(pooledminDCFof0.025).Theinverseistruefortheattackusingpre-trainedYourTTS(A28).7.AcknowledgementThisresearchworkwassupportedbytheNationalResearchCenterforAppliedCybersecurityATHENEandwithinthejointprojectSecMedIDwiththeBSI.
62

8.References[1]NicolasMichaelM¨uller,PavelCzempin,FranziskaDieck-mann,AdamFroghyar,andKonstantinB¨ottinger,“Doesaudiodeepfakedetectiongeneralize?,”inInterspeech,2022.[2]XinWangandJunichiYamagishi,“Canlarge-scalevocodedspoofeddataimprovespeechspoofingcounter-measurewithaself-supervisedfrontend?,”ArXiv,vol.abs/2309.06014,2023.[3]VineelPratap,QiantongXu,AnuroopSriram,GabrielSynnaeve,andRonanCollobert,“Mls:Alarge-scalemultilingualdatasetforspeechresearch,”arXivpreprintarXiv:2012.03411,2020.[4]HemlataTak,JosePatino,MassimilianoTodisco,AndreasNautsch,NicholasEvans,andAnthonyLarcher,“End-to-endanti-spoofingwithrawnet2,”inICASSP2021-2021IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP).IEEE,2021,pp.6369–6373.[5]HemlataTak,Jee-weonJung,JosePatino,MadhuKam-ble,MassimilianoTodisco,andNicholasEvans,“End-to-endspectro-temporalgraphattentionnetworksforspeakerverificationanti-spoofingandspeechdeepfakedetection,”arXivpreprintarXiv:2107.12710,2021.[6]Jee-weonJung,Hee-SooHeo,HemlataTak,Hye-jinShim,JoonSonChung,Bong-JinLee,Ha-JinYu,andNicholasEvans,“Aasist:Audioanti-spoofingusingin-tegratedspectro-temporalgraphattentionnetworks,”inICASSP2022-2022IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP).IEEE,2022,pp.6367–6371.[7]WanyingGe,MichelePanariello,JosePatino,Massimil-ianoTodisco,andNicholasEvans,“Partially-connecteddifferentiablearchitecturesearchfordeepfakeandspoof-ingdetection,”arXivpreprintarXiv:2104.03123,2021.[8]JacobKahn,MorganeRiviere,WeiyiZheng,EvgenyKharitonov,QiantongXu,Pierre-EmmanuelMazar´e,JulienKaradayi,VitaliyLiptchinsky,RonanCollobert,ChristianFuegen,etal.,“Libri-light:Abenchmarkforasrwithlimitedornosupervision,”inICASSP2020-2020IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP).IEEE,2020,pp.7669–7673.[9]DavidSnyder,GuoguoChen,andDanielPovey,“Mu-san:Amusic,speech,andnoisecorpus,”arXivpreprintarXiv:1510.08484,2015.[10]JodiKearns,“Librivox:Freepublicdomainaudiobooks,”ReferenceReviews,vol.28,no.1,pp.7–8,2014.[11]Wei-NingHsu,BenjaminBolte,Yao-HungHubertTsai,KushalLakhotia,RuslanSalakhutdinov,andAbdelrah-manMohamed,“Hubert:Self-supervisedspeechrepre-sentationlearningbymaskedpredictionofhiddenunits,”IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,vol.29,pp.3451–3460,2021.[12]KuanPoHuang,Yu-KuanFu,YuZhang,andHung-yiLee,“Improvingdistortionrobustnessofself-supervisedspeechprocessingtaskswithdomainadaptation,”arXivpreprintarXiv:2203.16104,2022.[13]AlexeiBaevski,YuhaoZhou,AbdelrahmanMohamed,andMichaelAuli,“wav2vec2.0:Aframeworkforself-supervisedlearningofspeechrepresentations,”Advancesinneuralinformationprocessingsystems,vol.33,pp.12449–12460,2020.[14]VassilPanayotov,GuoguoChen,DanielPovey,andSan-jeevKhudanpur,“Librispeech:anasrcorpusbasedonpublicdomainaudiobooks,”in2015IEEEinternationalconferenceonacoustics,speechandsignalprocessing(ICASSP).IEEE,2015,pp.5206–5210.[15]MircoRavanelliandYoshuaBengio,“Speakerrecogni-tionfromrawwaveformwithsincnet,”in2018IEEEspokenlanguagetechnologyworkshop(SLT).IEEE,2018,pp.1021–1028.[16]HemlataTak,MadhuKamble,JosePatino,MassimilianoTodisco,andNicholasEvans,“Rawboost:Arawdataboostingandaugmentationmethodappliedtoautomaticspeakerverificationanti-spoofing,”inICASSP2022-2022IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP).IEEE,2022,pp.6382–6386.[17]HemlataTak,MassimilianoTodisco,XinWang,Jee-weonJung,JunichiYamagishi,andNicholasEvans,“Automaticspeakerverificationspoofinganddeepfakedetectionusingwav2vec2.0anddataaugmentation,”inTheSpeakerandLanguageRecognitionWorkshop,2022.[18]XinWang,JunichiYamagishi,MassimilianoTodisco,H´ectorDelgado,AndreasNautsch,NicholasEvans,MdSahidullah,VilleVestman,TomiKinnunen,KongAikLee,etal.,“Asvspoof2019:Alarge-scalepublicdatabaseofsynthesized,convertedandreplayedspeech,”Com-puterSpeech&Language,vol.64,pp.101114,2020.[19]JoelFrankandLeaSch¨onherr,“Wavefake:Adatasettofacilitateaudiodeepfakedetection,”2021.[20]KeithItoandLindaJohnson,“Theljspeechdataset,”https://keithito.com/LJ-Speech-Dataset/,2017.[21]XuechenLiu,XinWang,MdSahidullah,JosePatino,H´ectorDelgado,TomiKinnunen,MassimilianoTodisco,JunichiYamagishi,NicholasEvans,AndreasNautsch,etal.,“Asvspoof2021:Towardsspoofedanddeepfakespeechdetectioninthewild,”IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,2023.[22]ChengzheSun,ShanJia,ShuweiHou,andSiweiLyu,“Ai-synthesizedvoicedetectionusingneuralvocoderar-tifacts,”inProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR)Work-shops,June2023,pp.904–912.[23]RicardoReimaoandVassiliosTzerpos,“For:Adatasetforsyntheticspeechdetection,”in2019InternationalConferenceonSpeechTechnologyandHuman-ComputerDialogue(SpeD).IEEE,2019,pp.1–10.[24]XinWang,H´ectorDelgado,HemlataTak,Jee-weonJung,Hye-jinShim,MassimilianoTodisco,IvanKukanov,XuechenLiu,MdSahidullah,TomiKinnunen,NicholasEvans,KongAikLee,andJunichiYamagishi,“ASVspoof5:Crowdsourcedspeechdata,deepfakes,andadversarialattacksatscale,”inASVspoofWorkshop2024(accepted),2024.[25]IngmarSteinerandS´ebastienLeMaguer,“Creatingnewlanguageandvoicecomponentsfortheupdatedmaryttstext-to-speechsynthesisplatform,”arXivpreprintarXiv:1712.04787,2017.
63

[26]AlexandreD´efossez,JadeCopet,GabrielSynnaeve,andYossiAdi,“Highfidelityneuralaudiocompression,”arXivpreprintarXiv:2210.13438,2022.[27]EdressonCasanova,KellyDavis,ErenG¨olge,G¨orkemG¨oknar,IulianGulea,LoganHart,AyaAljafari,JoshuaMeyer,ReubenMorais,SamuelOlayemi,etal.,“Xtts:amassivelymultilingualzero-shottext-to-speechmodel,”arXivpreprintarXiv:2406.04904,2024.[28]FlorianLux,JuliaKoch,andNgocThangVu,“Low-resourcemultilingualandzero-shotmultispeakertts,”arXivpreprintarXiv:2210.12223,2022.[29]Sang-gilLee,WeiPing,BorisGinsburg,BryanCatan-zaro,andSungrohYoon,“Bigvgan:Auniversalneu-ralvocoderwithlarge-scaletraining,”arXivpreprintarXiv:2206.04658,2022.[30]EdressonCasanova,JulianWeber,ChristopherDShulby,ArnaldoCandidoJunior,ErenG¨olge,andMoacirAPonti,“Yourtts:Towardszero-shotmulti-speakerttsandzero-shotvoiceconversionforeveryone,”inInternationalCon-ferenceonMachineLearning.PMLR,2022,pp.2709–2720.[31]FlorianLux,JuliaKoch,andNgocThangVu,“Ex-actprosodycloninginzero-shotmultispeakertext-to-speech,”in2022IEEESpokenLanguageTechnologyWorkshop(SLT).IEEE,2023,pp.962–969.