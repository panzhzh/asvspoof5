The Automatic Speaker Verification Spoofing Countermeasures Workshop (ASVspoof 2024)
31 August 2024, Kos, Greece

36

10.21437/ASVspoof.2024-6

TheSHADOWTeamSubmissiontotheASVSpoof2024ChallengeJes´usVillalba1,2,TiantianFeng3,ThomasThebaud1,2,JihwanLee3,ShrikanthNarayanan3,NajimDehak1,21CenterforLanguageandSpeechProcessing,JohnsHopkinsUniversity,Baltimore,MD,USA2HumanLanguageTechnologyCenterofExcellence,JohnsHopkinsUniversity,Baltimore,MD,USA3SignalAnalysisandInterpretationLaboratory,UniversityofSouthernCalifornia,LosAngeles,CA,USAjvillal7@jhu.eduAbstractThispaperpresentstheSHADOWteam’ssubmissiontotheASVSpoof2024challenge.Weevaluatedvariousmod-els,includingECAPA-TDNN,ResNet34,ConvNeXt,andS4Structured-State-SpaceModels.2Dconvolution-basedmod-elsoutperformedothertypes,withthebestProgresssetresultsachievedusingFwSE-ResNet34withcodecaugmentations.IntheTrack1Evalset,thissystemachievedminDCF=0.44,a47%improvementoverthechallengebaseline.ForTrack2,wecontributeastraightforwardmethodforcombiningwell-calibratedspeakerandspoofingdetectionscoresintoasinglesystem.Thisinvolvescalculatingtheposteriorprobabilityforatrialbeingbothsame-speakerandbonafide.However,thesignificantmismatchbetweentheDevandProgress/EvalsetsnotonlycomplicatedtheselectionofthebestsystemsandcodecsbutalsoimpactedtheEvalsetcali-brationandscorecombination.Nevertheless,weachieveda-DCF=0.397inTrack2,a42%improvementoverthebaseline.1.IntroductionInrecentyears,therapidadvancementofspeechsynthesisandvoiceconversiontechnologieshasledtosignificantimprove-mentsinthequalityandaccessibilityofsyntheticspeech.Whilethesetechnologiesoffernumerousbenefits,theyalsoposeagrowingthreatintheformofspoofingattacks,wheresyntheticormanipulatedspeechisusedtodeceiveautomaticspeakerver-ification(ASV)systems.TheASVSpoof2024challenge[1]aimstoaddressthesechallengesbyfosteringthedevelopmentofrobustanti-spoofingtechniquescapableofdistinguishingbe-tweengenuineandspoofedspeech.Inthispaper,wepresentoursubmissiontotheASVSpoof2024challenge.Variousarchitectureshavebeenproposedintheliteratureforthespoofingdetectiontask(Track1),in-cludingsqueeze-excitationResNetslikeASSERT[2],Spectro-TemporalGraphAttentionNetworks(AASIST)[3],andRawNet2[4].Foroursubmission,wefocusedonevaluat-ingwell-knownneuralnetworkarchitecturesthathavedemon-stratedstrongperformanceinspeakerdetectiontasks,suchasECAPA-TDNN[5],Thin-ResNet34[6],FwSE-ResNet34[7](similartoASSERT),besidesofConvNeXt[8],andStructuredState-SpaceModels[9].InTrack2,weintegratedspeakerandspoofingdetectionmodelsintoaunifiedsystemthatoutputsauniquescore,rep-resentingthelog-likelihoodratiobetweenthetargetbonafidehypothesisandthenon-targetorspoofinghypothesis.Variousmethodsforthisintegrationareproposedin[10],includingbothlinearandnon-linearfusionofthespeakerandspoofinglog-likelihoodratios.Instead,weproposeastraightforwardmethodthatinvolvescomputingtheposteriorprobabilitythattheenroll-mentandtestspeakersarethesameandthatthetestsampleisbonafide,giventhetrial’sdata,i.e.,P(same,bonafide|xe,xt).Thiscanbeaccomplishedusingbasicprobabilityrules,assum-ingthatwehavewell-calibratedspeakerandspoofingdetectionscores.Theproposedsystemsachievedcompetitiveperformanceinthechallenge,demonstratingsignificantimprovementsofover40%relativetothebaselinesystemsprovidedbytheorganizers.Thepaperisorganizedasfollows:Section2detailsthespoofingdetectionsystems,whileSection3outlinesthemathematicalframeworkforcombiningspeakerandspoofingsystems.Sec-tion4describestheexperimentalsetupandthespeakerrecog-nitionsystemusedinTrack2.Finally,Section5presentsadis-cussionoftheresults,andSection6concludesthepaperwithasummaryofourfindings.2.SpoofingDetectionModels2.1.FeatureExtractionWeutilizedan80-dimensionallog-Melfilterbankcomputedat16kHzsamplingfrequencywith25ms.windowlengthand10ms.shift.Thefeatureswereshort-timemeannormalizedwitha3-secondwindow.Silenceframeswereretainedintheprocessing.2.2.ECAPA-TDNNECAPA-TDNN,whichstandsforEmphasizedChannelAtten-tion,Propagation,andAggregationTimeDelayNeuralNet-work[5],enhancestraditionalTimeDelayNeuralNetworks(TDNNs)[11]byincorporatingseveraladvancedfeatures.ItutilizesRes2Netblocks[12]basedon1-dimensionalconvolu-tions.EachRes2NetblockintegratesaSqueeze-and-Excitation(SE)mechanism[13],whichdynamicallyrecalibrateschannel-wisefeatureresponses,emphasizingthemostimportantchan-nels.Additionally,ECAPA-TDNNemploysamulti-layerfea-tureaggregationstrategybyconcatenatingoutputsfromvariousnetworklayers,followedbyalinearprojection.Theframe-levelfeaturesarepooledintoasinglevectorusingachannel-wiseattentivepoolingmechanism.Thispooledvectoristhenlin-earlyprojectedintoabottleneckfeatureof192dimensions.Fortheoutputlayer,weusedthesubcenteradditiveangularmarginsoftmax(Subcenter-AAM-Softmax)[14],withtwooutputsrep-resentingbonafideandspoofcategories.Toenhanceclasssep-
37

Table1:DetailedarchitecturespecificationsforThin-ResNet34,FwSE-ResNet34andConvNext.Thenumbersinsidetheparenthesesdenotethefiltersizesandtheoutputchannelsizes.Themultiplicationfactorindicatesthenumberofresidualblocksateachstage.TheinnerbracketsfollowingbyFwSEindicatestheoutputdimensionofthetwofullyconnectedlayersinanSEmodule.GRNstandsforGlobalResponseNormalization.LayerNameThin-ResNet34FwSE-ResNet34ConvNext2d-AttoConvNext2d-PicoStem3×3,16,stride=13×3,64,stride=17×7,40,stride=17×7,64,stride=1Res1(cid:20)3×3,163×3,16(cid:21)×33×3,643×3,64FwSE[20,80]×3d7×7,401×1,1601×1,40GRN×2d7×7,641×1,2561×1,64GRN×2Res2(cid:20)3×3,32,s=23×3,32(cid:21)×13×3,128,s=23×3,128FwSE[10,40]×1(cid:2)2×2,80,s=2(cid:3)×1(cid:2)2×2,128,s=2(cid:3)×1(cid:20)3×3,323×3,32(cid:21)×33×3,1283×3,128FwSE[10,40]×3d7×7,801×1,3201×1,80GRN×2d7×7,1281×1,5121×1,128GRN×2Res3(cid:20)3×3,64,s=23×3,64(cid:21)×13×3,256,s=23×3,256FwSE[5,20]×1(cid:2)2×2,160,s=2(cid:3)×1(cid:2)2×2,256,s=2(cid:3)×1(cid:20)3×3,643×3,64(cid:21)×53×3,2563×3,256FwSE[5,20]×5d7×7,1601×1,6401×1,160GRN×6d7×7,2561×1,10241×1,256GRN×6Res4(cid:20)3×3,128,s=23×3,128(cid:21)×13×3,512,s=23×3,512FwSE[2,10]×1(cid:2)2×2,320,s=2(cid:3)×1(cid:2)2×2,512,s=2(cid:3)×1(cid:20)3×3,1283×3,128(cid:21)×23×3,5123×3,512FwSE[2,10]×2d7×7,3201×1,12801×1,320GRN×2d7×7,5121×1,20481×1,512GRN×2PoolingChannel-wiseAttentivePoolingProjectionFullyConnected,dim=192OutputSubcenter-AAM-Softmax,dim=2,subcenters=2,margin=0.2,scale=30aration,weappliedanangularmarginof0.2.Eachclasswasassignedtwosubcenterstomitigatetheimpactofmislabelinginthetrainingdata.2.3.ResNet34Weexperimentedwithtwovariantsoftheresidualnetwork[6],namely,ThinResNet34andFwSE-ResNet34,asdetailedinTa-ble1.TheResNet34architectureconsistsofaninputstemlayerfollowedby16Basicresidualblocksutilizing2Dconvolutions.Thisdesigndownscalesthefeaturemapsthroughstride-2con-volutions(resultinginanoveralldownsamplingof8×)whilesimultaneouslydoublingthenumberofchannels.Theoutputofthisnetworkisafour-dimensionaltensor(B,C,F/8,T/8),whereBdenotesthebatchsize,Cisthenumberofchannels,FisthenumberofMelfilters,andTisthetemporaldimension.Beforepassingthefeaturestothepoolinglayer,thechannelandfrequencydimensionsareflattenedto(B,C×F/8,T/8).Giventhelimitedtrainingdata,weexploredThin-ResNet34,whichreducesthenumberofconvolutionchannelsbyafac-torof4comparedtoregularResNet34,therebyreducingthemodel’sparametersby16×.However,weachievedbetterper-formancewiththelargerFwSE-ResNet34.FwSE-ResNet34enhancesResNet34byincorporatingfrequency-wisesqueezeexcitationintheresidualblocks.TheFwSEmechanism,aspro-posedin[7],performsasqueezeoperationbypoolingfeaturemapsacrossthetimeandchanneldimensionstoproduceanex-citationscalarforeachfrequencydimension.Thesemodelsalsoutilizedchannel-wiseattentivepoolingandaSubcenter-AAM-Softmaxoutputlayer.2.4.ConvNeXt2dConvNeXt[15]isamoderndeeplearningarchitecturethatre-visitstraditionalconvolutionalneuralnetwork(CNN)designswithinsightsfromrecentadvancementsinvisiontransformers.AsapureCNNmodel,ConvNeXtintegrateskeyfeaturessuchaslargekernelsizes,depthwiseconvolutions,andinvertedbot-tlenecks,drawinginspirationfromtransformermodels.UnlikethetraditionalbottleneckblocksusedinmanyResNetmodels,whichfirstreducethenumberofchannelsbeforeexpandingthem,theinvertedbottleneckblockinConvNeXtexpandsthenumberofchannelsfirstandthenreducesthem.Thisblockbe-ginswithadepthwiseconvolutionwithalargekernel(7×7),capturingspatialinformationforeachchannelindependentlywithoutsignificantlyincreasingthenumberofparameters.Thisisfollowedbyapointwiseconvolutionthatexpands×4thefea-turemapdimensions,andanotherpointwiseconvolutionthatreducesthenumberofchannelsbacktothebottlenecksize.Additionally,ConvNeXtreplacesBatchNormwithLayerNormandReLUactivationswithGELU.ConvNeXtlayersmirrorthestructureoftransformerswheredepthwiseconvolutionsfunc-tionsimilarlytoself-attentionlayers,andthepointwiseconvo-lutionsresemblethetransformerfeedforwardblock.Thisde-signenablestheblocktocapturerichspatialinformationwhilemaintainingcomputationalefficiencyandamanageableparam-etercount.FollowingtheConvNeXt-v2scheme[8],wealsoincorporatedGlobalResponseNormalization(GRN)asare-placementforsqueeze-excitation.GRNnormalizesthefeaturemapbasedontheL2normofeachchannelcomputedseparatelyacrossthespatialdimensions.,ensuringbalancedresponsemag-
38

nitudesandpreventinganysinglechannelfromdominatingthenetwork’soutput.LiketheECAPA-TDNNandResNetmod-els,ourConvNeXtutilizeschannel-wiseattentivepoolingandaSubcenter-AAM-Softmaxoutput.2.5.S4WealsoexperimentedwiththeState-spacemodels(SSM)[9]forspoofingdetection.SSMisarecentpopulardeepneu-ralnetworkarchitecturethatisspecificallydesignedtomodellong-termsequences,suchasthosewithlongdurationorex-tremelyhighsamplingfrequency.Inpractice,wecanregardtheSSMsasspecialcasesofconvolutionalandrecurrentlayers.Notably,weleveragetheStructuredStateSpaceforSequenceModeling(S4)thatwasintroducedin[16].Comparedtonor-malSSMs,S4wasinitializedusinganapproachcalledHippo[17],whichempiricallyyieldsamorestabletrainingprocess.Inourexperiments,weadoptamodifiedversionoftheoriginalS4.WeexperimentwiththeS4modelconsistingofS4blocksin{3,6,12},eachwithaLayerNormalizationmodule,thekernel¯Kestimationmodule,aGELU[18]activation,Dropout,andalinearprojection.Weempiricallyexperimentwiththeinputandoutputdimensionin{128,256}andadropoutrateof0.1.ThefinaloutputfromtheS4layersisfedthroughaclassifierlayerwithtwolinearlayersandaGELUlayerinbetween.WeadopttheimplementationfromtheGitHublink1describedinourpreviouswork[19].3.CombinationofSpeakerandSpoofingDetectionWeaimtocombinethespeakerverificationandspoofingde-tectionmodelsintoasinglesystemthatproducesauniquelikelihoodratiothatcanbeusedtodecidebetweenthetar-getandnon-targethypothesisinapplicationswherespoofingisarisk.Todothis,weproposetousetherulesofprob-ability.Inessence,weareinterestedinobtainingtheposte-riorprobabilityforthecasethatenrollmentandtestspeak-ersarethesameandthetestisbonafidegiventhetrial’sdata(xe,xt),i.e.,P(same,bonafide|xe,xt).Usingthechain-rule,andassumingindependencebetweentheenrollmentandthetestbonafide/spoofstatus,wewritetheposteriorasP(same,bonafide|xe,xt)=P(same|xe,xt,bonafide)P(bonafide|xt).(1)WenotenowthatP(same|xe,xt,bonafide)istheregularposteriorofanunprotectedspeakerverificationsystemsinceitassumesthatxtisbonafideandcanbewrittenasafunctionoftheregularASVlog-likelihoodratiologRASV,P(same|xe,xt,bonafide)==σ(cid:18)logP(xe,xt|same,bonafide)P(same|bonafide)P(xe,xt|diff,bonafide)P(diff|bonafide)(cid:19)(2)=σ(cid:18)logRASV+logP(same|bonafide)P(diff|bonafide)(cid:19)(3)whereσisthesigmoidfunction.NotethatwearegivenP(target)=0.9405,P(nontarget)=0.0095,P(spoof)=0.0095,andCmiss=1,Cfa=Cfa−spoof=10.Fromthese1https://github.com/klean2050/tilesecgmodelpriors,wederive,P(same|bonafide)=P(target)P(target)+P(nontarget)=0.99(4)P(diff|bonafide)=1−P(same|bonafide)=0.01.(5)Nevertheless,weneedtotakeintoaccountthecostsofthedif-ferentalternativesandderivateanequivalenteffectiveprioras,Peff(same|bonafide)1−Peff(same|bonafide)=CmissP(same|bonafide)CfaP(diff|bonafide)(6)obtainingPeff(same|bonafide)=0.90.Peff(same|bonafide)isthepriorweneedtopluginto(3).Also,weneedtousethiseffectivepriortotrainthelogisticregressioncalibratorthattransformsthespeakerverificationscoreintoawell-calibratedlikelihoodratiologRASV.Thiscalibratorhastobetrainedonjustbonafidetrialssincealltheprobabilitiesinvolvedaregiventhebonafidecase.Equivalently,thebonafideposteriorcanbewrittenasP(bonafide|xt)=σ(cid:18)logRbona/spoof+logP(bonafide)P(spoof)(cid:19)(7)whereRbona/spoofisthelikelihoodratiobetweenthebonafidevsspoofhypothesizes,andP(bonafide)=1−P(spoof).Again,weneedtoconsiderthedecisioncoststocalculatetheeffectivepriorPeff(bonafide)1−Peff(bonafide)=CmissP(bonafide)Cfa−spoofP(spoof)(8)andusethiseffectivepriortocalibratingthescoreofthespoof-ingdetectionsystemintologRbona/spoofinEq.(7).Thus,theproductof(3)and(7)giveusthedesiredposteriorP(same,bonafide|xe,xt).Ifwewantthecombinedoutputintheformofalog-likelihoodratioinsteadofaposterior,wecantransformtheposteriorintoacombinedlog-likelihoodratiologRASV−spoof,byinvertingthelogisticfunction,P(same,bonafide|xe,xt)==σ(cid:18)logRASV−spoof+logPeff(target)1−Peff(target)(cid:19)(9)withPeff(target)1−Peff(target)=CmissP(target)CfaP(nontarget)+Cfa−spoofP(spoof).(10)Notethatthespeakerverificationandspoofingdetectionlikelihoodratiosareapplication-independent,i.e.,iftheyarewellcalibratedacrossawiderangeofoperatingpoints,wecanuseanypriortotransformingthemintoposteriorprobabili-ties.However,thecombinedlikelihoodratioRASV−spoofisnolongerapplication-independentsinceweneedtoplugthepriorsintoEq.(3)and(7)tocalculateit.Ifwechangethepriors,weneedtorepeatallthecalculations.Therefore,forthecombinedsystem,producingalikelihoodratioratherthanaposteriorisnotasadvantageous.
39

Table2:Codecsusedinthefinalsubmission,probabilityofus-ingeachcodecandcodecoptionsCodecProb.OptionsTelephoneA-law0.17Highpass:100-300Hzµ-law0.17G723.10.03Lowpass:3400-3700HzG7260.13MediaG7220.10MP30.25CBR(50%):128-320kbps,VBR(50%):QScale=0-3,Compr=0-3AC30.08Vorbis0.02Compression=6-10Opus0.05Compression=6-104.ExperimentalSetup4.1.ECAPA-TDNN,ResNet,ConvNextECAPA-TDNN,ResNets,andConvNeXtsweretrainedonthefixedconditiondatausing2-secondchunksandaneffectivebatchsizeof256.TheactualbatchsizevarieddependingonGPUmemoryandnetworksize,withgradientaccumulationemployedtoreachthedesiredeffectivebatchsize.WeutilizedtheAdamWoptimizerwithaweightdecayof0.1.Thelearningratewaswarmedupover1.5kstepstoamaximumof0.001,andafter3ksteps,itwashalvedevery3ksteps.TheAAM-Softmaxmarginwasgraduallyincreasedfrom0to0.2overfiveepochs.Earlystoppingwasbasedonperformanceonthedevelopmentandprogressdatasets.ThefinalmodelsubmittedfortheEvalphasewastrainedforfiveepochs.Thedatawasaugmentedusingnoise(N)fromMusanmu-sicandenvironmentalnoises2,aswellassyntheticandrealreverberation(R)fromRIR3.Wealsoexperimentedwithme-diacodecs(MC),telephonecodecs(TC),andacombinationoftelephoneandmediacodecs(TMC),whichwasusedinourfinalsubmission.Thecodecswereappliedon-the-flyto20%oftheutterancesusingtorchaudio4.Weutilizedallffmpegcodecscompatiblewithtorchaudio.Eachcodectypewasap-pliedwithadifferentprobability:codecswithlongerprocess-ingtimes,suchasG723.1andVorbis,wereusedlessfrequently,whileMP3,whichoffersmoreconfigurationoptions,wasap-pliedmoreoften.Telephonecodecsalsoincludedresamplingto8kHzandbackto16kHz,high-passfilteringwithacut-offfrequencybetween100-300Hz,andlow-passfilteringwithacut-offfrequencybetween3400-3700Hz.Table2liststhecodectypes,theirapplicationprobabilities,andthespecificop-tionsusedforeachcodec.4.2.S4Similartothepriorexperiments,wetrainedtheS4with2-secondspeechchunks.Weusedalearningrateandaweightdecayof0.0002and0.001,respectively.Weselectthebest-performingmodelbasedontheperformancefromthedevset.WedidnotsubmitthemodelintheEvalphaseduetotherel-ativelylowperformanceontheprogresssetcomparedtoothermodelarchitectures.Weperformedspeechaugmentationusing2http://www.openslr.org/resources/173http://www.openslr.org/resources/284https://pytorch.org/audio/stable/index.htmlAudiomentations5.NoiseaugmentationusednoisesfromESC-50[20],excludinghuman-relatedsounds.Wealsoappliedotheraugmentationtechniques,suchastimemasking,timestretch-ing,andpitchshifting,withaprobabilityof0.5andRIRfrom6.4.3.SpeakerVerificationSystemThespeakerverificationsystemusedinTrack2wasaFwSE-ResNet100withtheconfigurationdescribedin[21].LiketheSpoofingDetectionsystems,ituseschannel-wiseatten-tivepooling,192-dimensionalembedding,andaSubcenter-AAM-Softmaxoutputlayerwithmargin=0.2.ThenetworkwastrainedonVoxCeleb2using2-secondchunksandaneffectivebatchsizeof256.WeusedAdamoptimizerwithamaximumlearningrateof0.01warmed-upfor15ksteps.After65ksteps,thelearningratewashalvedevery40ksteps.Following,thenet-workwasfine-tunedon4-secondchunksusinghard-prototypemining[22]andincreasingthemarginto0.3andInter-Topmar-gin=0.1withK=5[23].FinetuningusedSGDoptimizerwithlr=0.001andmomentum=0.9.Thelearningratewaswarmedupfor8ksteps,held8ksteps,andhalvedevery32ksteps.Trialswereevaluatedjustbydoingcosinescoringbetweenenrollmentandtestsegments.4.4.CalibrationandFusionSpoofingdetectionscoreswerecalibratedusinglogisticregres-sionontheASVSpoof2024Devdata.SpeakerverificationscoreswerecalibratedontheASVSpoof2024Devbonafidetriallist,excludingspoofingtrials.Weusedeffectivepriorsforcal-ibrationasoutlinedinSection3.Wealsoexploredfusingmul-tiplespoofingdetectionsystems,butdidnotobservesignificantimprovementsineithertheDevelopmentorProgresssetscom-paredtothebestsinglesystem,FwSE-ResNet34.5.Results5.1.Track1Table3presentstheresultsofdifferentsystemsonASVSpoofDev,Progress,andEvalsets.WetestedthreesizesofECAPA-TDNN,rangingfromthree256-dim.layerstofour1024-dimlayers.Noadvantagewasobservedwithlargernetworksizes,likelyduetothelimitedtrainingdata.Attemptstomitigateoverfittingbyincreasingdropoutwereunsuccessful.TheS4modelperformedcomparablytotheECAPA-TDNNwithfour1024-dimensionallayers.ConvNeXtoutperformedECAPA-TDNN,withConvNeXtPicoshowingslightimprove-mentsoverConvNeXtAtto,particularlyintermsofEqualEr-rorRate(EER).However,theEERontheDevelopmentsetdidnotreliablypredictperformanceontheProgressset,makingminDCFamoreaccurateindicator.Thin-ResNet34performedbetterthanConvNeXTintermsofDCFandworseintermsofEER.Apparently,ResNet34waslessimpactedbyoverfittingthanpreviousarchitectures,allowingustoscaleupthesize×4fromThin-ResNet34toFwSE-ResNet34,significantlyoutper-formingallpreviousnetworksonDevandProgresssets.Ingeneral,itseemsthat2dconvolutionnetworksoutperformothertypesofmodelsinthistask.Ourexperimentswithcodecaugmentationwereunsuccess-fulonthedevset,alwaysdegradingperformancew.r.t.systems5https://github.com/iver56/audiomentations6https://github.com/RoyJames/room-impulse-responses
40

Table3:ResultsofindividualsystemsonASVSpoof2024Track1DevelopmentandProgress/Evaluation.AugmentationsareNoise(N),Reverberation(R),TelephoneCodecs(TC),MediaCodecs(MC),Telephone+MedicaCodecs(TMC).SystemASVSpoof2024Dev.ASVSpoof2024Progress/EvalNNetAugment.minDCFactDCFCllrEERminDCFactDCFCllrEERProgressPhaseS4R+N0.21312.960.4690.5000.72518.40ECAPA-TDNN256x3R+N0.1690.1690.34511.91ECAPA-TDNN512x3R+N0.1940.1940.35511.51ECAPA-TDNN1024x4R+N0.2180.2190.38311.94ConvNext2dAttoR+N0.1650.1710.35911.56ConvNext2dPicoR+N0.1600.1610.3009.94ThinResNet34R+N0.1550.1560.33712.090.2880.3290.47012.77FwSE-ResNet34R+N0.1390.1390.29510.6250.2030.3860.5248.68FwSE-ResNet34R+N+TC0.1480.1480.29910.640.1810.4930.6737.92FwSE-ResNet34R+N+MC0.1510.1510.31111.170.1740.3040.4386.89EvalPhaseRawNet2(B01)0.8270.9924.09436.04AASIST(B02)0.71109304.00129.12FwSE-ResNet34R+N+TMC0.1460.1470.30110.7680.4390.6330.85317.09Table4:minDCFonASVSpoof2024Track1Evalbreakdown:attacksvscodecsminDCFpooled-codec-1codec-2codec-3codec-4codec-5codec-6codec-7codec-8codec-9codec-10codec-11pooled0.439020.115520.362210.301360.364890.586990.172030.348290.610120.450440.550840.585440.20854A170.120730.002380.055820.045790.103640.187850.057280.092980.214070.156640.087070.133750.01135A180.260110.004620.129110.113190.239310.358990.046870.174270.397290.294180.460230.493950.04842A191.000000.999831.000001.000001.000001.000000.999531.000001.000001.000001.000001.000001.00000A200.882720.200240.710560.629730.534051.000000.213440.542581.000000.724300.935000.827390.33669A210.126320.000110.036850.027540.123200.247570.034830.080650.257520.179130.069560.103750.00589A220.263490.011620.161250.144060.308570.460630.184180.214560.426580.204660.238840.296840.03871A230.232590.004700.108020.072230.182660.371890.043120.141910.413970.220250.200030.242530.03202A240.217750.004930.149490.093300.314280.367160.064170.061780.384930.270660.341660.473280.04524A250.288850.000120.186280.221800.292730.785290.104930.129890.808100.300490.347600.273540.00505A260.396400.015820.308820.289020.399470.625650.167150.255570.645120.529600.746890.764630.13869A270.495970.030580.448080.296570.295460.570740.050370.366440.640020.676560.891680.829370.23279A280.461310.065600.359200.366570.460740.628200.305180.390110.633500.503280.584380.734380.24889A290.036200.001020.010430.003490.016650.063930.002690.002900.090570.095080.012310.031530.00421A300.504220.031530.402250.238060.358320.646310.086740.401360.723610.539460.754150.779920.17138A310.555790.057050.401280.230850.402880.650100.120790.431740.685610.443860.643850.724120.24023A320.531260.032750.489090.282480.309810.745280.053260.382360.803610.563640.712410.656100.17139withoutcodecs.Eventually,wesubmittedsystemswithtele-phone(TC)andmedia(MC)codecstotheProgressset,observ-ingsignificantgainsforboth.Mediacodecsachievedthebestperformance.Unfortunately,wedidnothavetimetosubmitthesystemstrainedonacombinationoftelephoneandmediacodecs(TMC)totheProgressset.Nevertheless,wethoughtthatTMCwouldbethebestoptionandsubmittedthissystemtotheEvalphase,achievingminDCF47%betterthantheorga-nizerbaselineAASIST.WedetectedasignificantmismatchbetweenDevandProgress/Evalsets.ThisrenderedtheDevsetuselessonceweachievedminDCF≤0.15.Thismismatchalsoresultedinbadlycalibratedsystems.WecanobserveinthetablethatactDCFandCllrbecomeworseasweimproveminDCFandEER.Table4breaksdownminDCFperattackmethods,andcodectypeintheEvalsetfortheFwSE-ResNet34.ThetablesforactDCF,Cllr,andEERweresimilar.AttackandCodecsaredescribedinthechallengesummarypaper[1].AttacksA01-8wereincludedintheTrainingset,A09-16intheDevset,andA17-32intheEvalset.Therefore,theEvalattackswereallunseenattraining.Thiscausesalargedisparityintheperformancefordifferentattacktypes.A17(ZMM-TTS),A21(BigVGAN),A29(XTTS)havepooledminDCF<0.12.ThesesystemsallhaveavocoderbasedonHiFi-GAN,thesameasA08(VITS)includedinthetraining,whichcouldexplainthisgoodperformance.Meanwhile,A20(in-houseunitselec-tion)hasminDCF=0.88andA19(MaryTTS)rendersthesystemuselesswithminDCF=1.A19-20arebothclassicalconcate-nativeunitselectionsystems.Meanwhile,thetrainingattackswerealldeep-learning-basedTTSmodels.Anothersetofat-tackswithhighminDCF∼0.5,areA28,30,31,32thatusetheMalacopulaadversarialattack(AT)ontopofTTSorVC.TheequivalentTTSandVCmethodswithoutATareA26,18,22,25,withminDCFsof0.26-0.39.Thus,theATincreasedminDCFby17-115%relative.Regardingcodectypes,thetraininganddevelopmentsetsdidnotincludeanycodecsotherthanthosewesimulated.Oursystemperformedexceptionallywellonnon-codectrials,achievingapooledminDCFof0.11,withonlyfailinginA19.Italsoshoweddecentperformancewithcodec5(MP3),yield-ingaminDCFof0.187,andcodec11(varied8kHztelephone),withaminDCFof0.20.ThisperformancecanbeattributedtoaugmentingwithMP3andvaried8kHztelephonecodecsduringtraining.However,theMP3codecsusedduringtrain-inghadhigherbitratescomparedtothoseintheEvalset,aslowbitratecodecsnegativelyimpactedperformanceintheDev
41

Table5:ResultsofindividualsystemsonASVSpoof2024Track2DevelopmentandEvaluationSystemASVSpoof2024Dev.ASVSpoof2024EvalSpk-NNetSpoof-NNetSpoof-Augment.a-DCFt-DCFt-EERa-DCFt-DCFt-EER(B03)ECAPA-TDNNAASIST0.6810.92928.78FwSE-ResNet100ConvNeXtPicoR+N0.1430.2426.24FwSE-ResNet100FwSE-ResNet34R+N0.1320.1226.39FwSE-ResNet100FwSE-ResNet34R+N+TC0.1340.2206.47FwSE-ResNet100FwSE-ResNet34R+N+MC0.1350.2236.74FwSE-ResNet100FwSE-ResNet34R+N+TMC0.1340.2196.530.3970.70015.09set.Codecs1(Opus),2(ARM-WB),3(Speex),and6(MP4)exhibitedintermediateperformance,withminDCFvaluesrang-ingfrom0.30to0.36.Amongthese,onlyOpuswasincludedintraining.However,AMR-WB(G722.2)hashigherqual-itythanG722,seenintraining,sothatcouldhavehelpedtoavoidfurtherdegradation.Thecodecsthatdegradedperfor-mancemorewere4(Encodec),7(MP3+Encodec),9(AMR8kHz),and10(Speex8kHz),eachwithaminDCFgreaterthan0.55.Encodecwasnotincludedintraining,highlightingthatnewdeep-learningcodecscansignificantlyimpairspoof-ingdetectionsystems.Additionally,for8kHzcodecs,weonlyconsideredtypicaltelephonecodecslikeA-law,µ-law,G723,andG726,withoutincludingnarrow-bandversionsofwide-bandcodecs(AMR,Speex),whichmightexplainsomeoftheobserveddegradation.Wealsoobservethattherearecomplicatedinteractionsbetweenattackmethodsandcodecs.WhileA19performedbadlyforanycodec,A20haddecentperformanceforno-codecandcodec5(MP3)buthadverybadperformancewhenpairedwithcodecs1,7,8,9,and10.Also,A25goesfromminDCF=0.00012withoutcodecto0.78withcodec4(En-codec).5.2.Track2Table5presentstheresultsforTrack2,wherespeakerandspoofingdetectionsystemsarecombinedintoasinglesystem.WeobservedasignificantperformancedegradationintheEvalsetcomparedtotheDevset,likelyduetomismatchesinattacktypesandcodecsbetweenthetwosets.Asmentionedintheprevioussection,thesemismatchesleadtomiscalibrationinthespoofingdetectionscores.Sinceourcombinationmethod,de-scribedinSection3,reliesoncalibratedspeakerandspoofingdetectionscores,suchmiscalibrationcannotablyimpactthea-DCF.Nevertheless,oursystemachieveda44%improvementoverthebaselineB03.6.ConclusionThispaperdescribestheSHADOWteam’ssubmissiontoASVSpoof2024.Oursystemswerebuiltonnetworkscom-monlyusedforspeakerverification,suchasECAPA-TDNNandResNet34.Wefoundthat2Dconvolution-basedmodelsoutperformed1Dconvolutionmodelsandstate-spacemodels(SSMs).FwSE-ResNet34emergedasourbest-performingsys-tem,andwedidnotachieveanygainsfromcombiningitwithothersystems.Tointegratespeakerandspoofingdetectionsys-tems,weproposeastraightforwardmethodthatcalculatestheposteriorprobabilityofatrialbeingbothtargetandbonafide,usingprobabilityrulesandwell-calibratedscores.Thesub-stantialmismatchbetweentheDevandEvalsetscomplicatedmodeltuning,particularlyinselectingtheoptimalmodelandcodeccombination.Weobservedastrongdependencybe-tweencodecsandperformance;trialswithoutcodecsorcodecsseenduringtrainingperformedsignificantlybetterthanunseencodecs.WealsonotedthatunitselectionTTSbecomeunde-tectedbyspoofingdetectorstrainingoncurrentdeep-learningTTSmodels.Despitethesechallenges,ourmodelsachievedanotablerelativeimprovementof44-47%overthechallengebaselines.6.1.AcknowledgmentsTiantianFeng,JihwanLeeandShrikanthNarayananarepar-tiallysupportedbyNSF.7.References[1]X.Wangetal.,“ASVspoof5:Crowdsourceddata,deep-fakesandadversarialattacksatscale,”inASVspoof2024workshop(submitted),2024.[2]Cheng-ILai,NanxinChen,Jes´usVillalba,andNajimDe-hak,“ASSERT:Anti-SpoofingwithSqueeze-ExcitationandResidualNetworks,”inProc.Interspeech2019,2019,pp.1013–1017.[3]Jee-weonJung,Hee-SooHeo,HemlataTak,Hye-jinShim,JoonSonChung,Bong-JinLee,Ha-JinYu,andNicholasEvans,“Aasist:Audioanti-spoofingusingin-tegratedspectro-temporalgraphattentionnetworks,”inICASSP2022-2022IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),2022,pp.6367–6371.[4]HemlataTak,JosePatino,MassimilianoTodisco,AndreasNautsch,NicholasEvans,andAnthonyLarcher,“End-to-endanti-spoofingwithrawnet2,”inICASSP2021-2021IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),2021,pp.6369–6373.[5]B.Desplanques,J.Thienpondt,andK.Demuynck,“ECAPA-TDNN:EmphasizedChannelAttention,Prop-agationandAggregationinTDNNBasedSpeakerVerifi-cation,”inInterspeech2020,2020.[6]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun,“Deepresiduallearningforimagerecognition,”in2016IEEEConferenceonComputerVisionandPatternRecognition(CVPR),2016,pp.770–778.[7]JentheThienpondt,BrechtDesplanques,andKrisDe-muynck,“IntegratingFrequencyTranslationalInvarianceinTDNNsandFrequencyPositionalInformationin2DResNetstoEnhanceSpeakerVerification,”inProc.Inter-speech2021,2021,pp.2302–2306.
42

[8]SanghyunWoo,ShoubhikDebnath,RonghangHu,XinleiChen,ZhuangLiu,InSoKweon,andSainingXie,“Con-vnextv2:Co-designingandscalingconvnetswithmaskedautoencoders,”in2023IEEE/CVFConferenceonCom-puterVisionandPatternRecognition(CVPR),2023,pp.16133–16142.[9]AlbertGu,IsysJohnson,KaranGoel,KhaledSaab,TriDao,AtriRudra,andChristopherR´e,“Combiningre-current,convolutional,andcontinuous-timemodelswithlinearstatespacelayers,”Advancesinneuralinformationprocessingsystems,vol.34,pp.572–585,2021.[10]XinWang,TomiKinnunen,LeeKongAik,Paul-GauthierNoe,andJunichiYamagishi,“RevisitingandImprov-ingScoringFusionforSpoofing-awareSpeakerVerifica-tionUsingCompositionalDataAnalysis,”inProc.Inter-speech,2024,p.(accepted).[11]D.Snyder,D.Garcia-Romero,G.Sell,D.Povey,andS.Khudanpur,“X-Vectors:RobustDNNEmbeddingsforSpeakerRecognition,”inProceedingsoftheIEEEIn-ternationalConferenceonAcoustics,SpeechandSignalProcessing,ICASSP2018,Alberta,Canada,apr2018,pp.5329–5333,IEEE.[12]Shang-HuaGao,Ming-MingCheng,KaiZhao,Xin-YuZhang,Ming-HsuanYang,andPhilipTorr,“Res2Net:ANewMulti-ScaleBackboneArchitecture,”IEEETransac-tionsonPatternAnalysisandMachineIntelligence,vol.43,no.2,pp.652–662,Feb2021.[13]J.Hu,L.Shen,andG.Sun,“Squeeze-and-ExcitationNet-works,”in2018IEEE/CVFConferenceonComputerVi-sionandPatternRecognition,2018,pp.7132–7141.[14]J.Deng,J.Guo,T.Liu,M.Gong,andS.Zafeiriou,“Sub-centerArcFace:BoostingFaceRecognitionbyLarge-ScaleNoisyWebFaces,”102020,pp.741–757.[15]ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFe-ichtenhofer,TrevorDarrell,andSainingXie,“Aconvnetforthe2020s,”in2022IEEE/CVFConferenceonCom-puterVisionandPatternRecognition(CVPR),2022,pp.11966–11976.[16]AlbertGu,KaranGoel,andChristopherR´e,“Efficientlymodelinglongsequenceswithstructuredstatespaces,”arXivpreprintarXiv:2111.00396,2022.[17]AlbertGu,TriDao,StefanoErmon,AtriRudra,andChristopherR´e,“Hippo:Recurrentmemorywithopti-malpolynomialprojections,”AdvancesinNeurIPS,vol.33,pp.1474–1487,2020.[18]DanHendrycksandKevinGimpel,“Gaussianerrorlinearunits(gelus),”arXivpreprintarXiv:1606.08415,2016.[19]KleanthisAvramidis,DominikaKunc,BartoszPerz,KrantiAdsul,TiantianFeng,PrzemysławKazienko,Sta-nisławSaganowski,andShrikanthNarayanan,“Scalingrepresentationlearningfromubiquitousecgwithstate-spacemodels,”IEEEJournalofBiomedicalandHealthInformatics,2024.[20]KarolJPiczak,“Esc:Datasetforenvironmentalsoundclassification,”inProceedingsofthe23rdACMinterna-tionalconferenceonMultimedia,2015,pp.1015–1018.[21]NikitaTorgashov,RostislavMakarov,IvanYakovlev,PavelMalov,AndreiBalykin,andAntonOkhotnikov,“Theidrdvoxcelebspeakerrecognitionchallenge2023systemdescription,”2023.[22]J.Thienpondt,B.Desplanques,andK.Demuynck,“Cross-LingualSpeakerVerificationwithDomain-BalancedHardPrototypeMiningandLanguage-DependentScoreNormalization,”inProc.Interspeech2020,2020,pp.756–760.[23]MiaoZhao,YufengMa,YiweiDing,YuZheng,MinLiu,andMinqiangXu,“Multi-querymulti-headattentionpoolingandinter-topkpenaltyforspeakerverification,”inICASSP2022-2022IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP).IEEE,2022,pp.6737–6741.