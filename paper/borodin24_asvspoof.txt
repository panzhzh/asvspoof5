The Automatic Speaker Verification Spoofing Countermeasures Workshop (ASVspoof 2024)
31 August 2024, Kos, Greece

48

10.21437/ASVspoof.2024-8

AASIST3:KAN-EnhancedAASISTSpeechDeepfakeDetectionusingSSLFeaturesandAdditionalRegularizationfortheASVspoof2024ChallengeKirillBorodin2,∗,VasiliyKudryavtsev2,∗,DmitriiKorzh1,3,∗,AlexeyEfimenko2,∗,†GrachMkrtchian2,MikhailGorodnichev2,OlegY.Rogov1,31AIRI,2MTUCI,3Skoltech,Moscow,Russiakorzh@airi.netAbstractAutomaticSpeakerVerification(ASV)systems,whichidentifyspeakersbasedontheirvoicecharacteristics,havenumerousap-plications,suchasuserauthenticationinfinancialtransactions,exclusiveaccesscontrolinsmartdevices,andforensicfraudde-tection.However,theadvancementofdeeplearningalgorithmshasenabledthegenerationofsyntheticaudiothroughText-to-Speech(TTS)andVoiceConversion(VC)systems,exposingASVsystemstopotentialvulnerabilities.Tocounteractthis,weproposeanovelarchitecturenamedAASIST3.Byenhanc-ingtheexistingAASISTframeworkwithKolmogorov-Arnoldnetworks,additionallayers,encoders,andpre-emphasistech-niques,AASIST3achievesamorethantwofoldimprovementinperformance.ItdemonstratesminDCFresultsof0.5357intheclosedconditionand0.1414intheopencondition,significantlyenhancingthedetectionofsyntheticvoicesandimprovingASVsecurity.1.IntroductionAutomaticSpeakerVerification(ASV)systemsaredesignedtoidentifyspeakersbasedontheirvoicecharacteristics.Thesesystemshaveavarietyofapplications,includinginthefinan-cialsectorforuserauthenticationduringtransactions,insmartdevicestoensureexclusiveaccessfortheownertocontroltheirequipment,andinforensicanalysistodetectfraudcases.However,theadventofdeeplearningalgorithmshasren-deredASVsystemssusceptibletomanyassaults.Thepub-licaccessibilityofText-to-Speech(TTS)andVoiceConversion(VC)systemswithpre-trainedweightspermitsanyuserwithaccesstocomputationalresources,includingcloudGPUs,torefinethesemodelsforpotentiallymalevolentobjectives.Toeffectivelycountersuchattacks,theimplementationofanti-spoofingsystemsisimperative.TheASVSpoofcommu-nityisengagedinactiveresearchinthisfield,asevidencedbythecompilationofdiversedatacorporaforthedevelop-mentofbothcountermeasure(CM)systemsandspoofing-awarespeakerverification(SASV)systems.Thisresearchisdocu-mentedinthefollowingpublications:[1,2,3,4,5].Moreover,theSingfakedataset[6]wasdevelopedtodetectAI-generatedvocalswithinthemusicaldomain.Additionally,theSVDDprojecthassignificantlycontributedtoadvancingvoicespoof-ingcountermeasures.Today,manytechniquesareutilizedtodetectvoicespoof-ing,includingthosebasedonConvolutionalNeuralNetworks(CNN)[7,8],ResNet-likearchitectures[9,10,11,12],TimeDelayNeuralNetworks(TDNN)[13,14],andtransformers∗Equalcontribution.†ContributedduringinternshipinAIRI.[15].TheAASISTarchitecture[16]hasdemonstratedpartic-ularrobustness,asconfirmedbynumerousstudies.Variousmodificationshavebeenproposedtoenhancethegeneraliza-tioncapabilityofAASIST,includingtheuseofaRes2Neten-coder[17],wav2vec[18],fusionofdifferentaudiorepresenta-tions[19],applicationofspecifictrainingschemessuchasSAM[20],ASAM[21],SWL[22],aswellastheuseofalternativelossfunctions[23,24].Inthepresentstudy,weproposeaninnovativearchitec-ture,AASIST3,developedonanAASISTbasetodetectspeechdeepfakes.Themainmodificationsinclude:•ThemodificationofattentionlayersinGAT,GraphPool,andHS-GALutilizingKAN[25]isbasedontheprimaryPreLUactivationfunctionandlearnableB-splines,al-lowingfortheextractionofmorerelevantfeatures.•ThescalingofthemodelinwidthusingtheproposedKAN-GAL,KAN-GraphPool,andKAN-HS-GALtech-niquesenabledtheextractionofmorecomplexparame-ters,resultinginenhancedmodelperformance.•Theprimarymethodsofdatapre-processingemployedwerediverseaugmentationsandpre-emphasis,intend-ingtoobtainmoremeaningfuldiscriminativefrequencyinformation.2.Preliminaries2.1.Kolmogorov-ArnoldNetworkTheKolmogorov-Arnoldtheorem[26,27,28]postulatesthatanycontinuousmultivariatefunctionf:[0,1]n→Rcanberepresentedasafinitecompositionofcontinuousunaryfunc-tionsandabinaryadditionoperation.Morespecifically:f(x)=f(x1,x2,...,xn)=2n(cid:88)q=0Φq2n(cid:88)p=1ϕq,p(xp),(1)whereϕq,p:[0,1]→RandΦq:R→RAsallthefunctionstobelearnedbythemodelareone-dimensional,Liuetal.[25]proposedthateach1DfunctionbeparameterizedasaB-splinecurveandabasisfunction:ϕ(x)=wbb(x)+wsspline(x),(2)whereb(x)representsthelocalbasisfunction(eq.3),whilewbandwsdenotetrainableparametersthathavebeeninitializedfollowingKaiminginitialization.b(x)=PReLU(x)=max(0,x)+a·min(0,x)(3)whereaisthetrainableparameter,spline(x)-linearcombina-tionofB-splines:spline(x)=4(cid:88)i=0ciBi(x)(4)
49

wherecirepresentsthetrainableparameterandBidenotestheuniquespline.Foreachsplineoforderβd=4,atotalofGpointswereutilized:G=2βd+βN+1,(5)wheregridsizeβNissetto16.Thepointsarelocatedontheinterval[θ1,θ2]eq.6,7θ1=−βdh+α1(6)θ2=(N+βd+1)h+α1(7)h=α2−α1βN,(8)where[α1,α2]isthegridrange.Wesetα2=1andα1=−1.AKANlayerwithinputdimensionalityninandoutputdimensionalitynoutcanberepresentedasamatrixofone-dimensionalfunctions:Φ{ϕq,p},p={1,2,...,nin},q={1,2,...,nout}.(9)Inmatrixform,theKANlayercanbeexpressedasfollows:xl+1=ϕ1,1(·)ϕ1,2(·)···ϕ1,nl(·)ϕ2,1(·)ϕ2,2(·)···ϕ2,nl(·).........ϕnl+1,1(·)ϕnl+1,2(·)···ϕnl+1,nl(·)(cid:124)(cid:123)(cid:122)(cid:125)Φlxl,(10)WhereΦisthematrixofthefunctionoftheKANlayer.Thus,theKANlayercanbedenotedas:KAN(X)=ΦX.(11)2.2.AudiopreprocessingInlightofthehypothesisthathighfrequenciesfacilitatethemodel’sabilitytodifferentiatebetweenbonafideandspoofut-terances,weemployedapre-emphasistechniqueontheinputsignal:xl=xl−0.97·xl−1,(12)wherelequals1,2,3,..,L,Lrepresentsthelengthoftheaudiosignal,and0.97isthepre-emphasisfactor.Thepre-emphasisprocesssuppresseslowandenhanceshighfrequencies,facil-itatingthemodel’sabilitytofocusonmorerelevantfeaturesspecifictospoofingorbonafideutterances.2.3.SincConvfrontendFollowingAASIST[16],weusethenon-trainableSincConv[29]toextractfeaturesfrompreprocessedaudio.SincConvappliesthefunctiong(n,f1,f2)tothespeechsignalchunksx(n)usingtheHammingwindowfunctionwn:y(n)=x(n)·g(n,f1,f2)∗w(n)(13)g(n,f1,f2)=2f2sinc(2πf2n)−2f1sinc(2πf1n)(14)sinc(x)=sin(x)x(15)w(n)=0.54−0.46cos2πnL,(16)wheref1,f2arefixedparametersequaltotheminimumandmaximumpossiblefrequenciesintheMel-spectrogramofthepassedsignal2.4.Wav2Vec2frontendWav2Vec2[30],developedbyFacebookAI,isastate-of-the-artmethodforconvertingaudiototext.ThismodelutilizestheTransformerarchitecturefirstintroducedin[31].TheWav2Vec2[30]architectureconsistsoftwomaincomponents:EncoderLayer:Thislayertransformstheinputaudiodataintoasequenceofhiddenstates.Itconsistsofconvolutionallayersthattransformtheinputaudiodataintoasequenceofhiddenstates.PredictorLayer:Thislayertakesthesequenceofhiddenstatesfromtheencoderlayerandpredictsthenexthiddenstate.ItusestheTransformerarchitecture,whichallowsthemodeltoconsidercontextwhenpredictingthenextstate.AkeyfeatureofWav2Vec2isthatitistrainedunsupervised,meaningitdoesnotrequirelabeleddatafortraining.Instead,itusesamethodcalledcontrastivelearningtolearnaudiorepresentations.Asafront-endcomponentforascientificpaper,Wav2Vec2canauto-maticallytranscribeaudiototext,whichcanhelpanalyzeaudiodataorcreatetextversionsofaudiorecordings.2.5.EncoderTheencodercomprisessixconvolutionalunits.Exceptfortheinitialunit,eachsubsequentunitcomprisestwoconvolutionunits.Theinitialunit,however,comprisesasingleconvolu-tionunitinconjunctionwithanotherunit.Theconvolutionunitimplementsthefollowingtransformation:ConvUnit(x)=Conv(SELU(BatchNorm(x)))(17)Eachunit’sinputisaddedtotheoutputfollowingthesec-ondconvolutionalunitinthatblockanddownsampledusingaconvolutionallayerifnecessary.Subsequently,MaxPoolingisappliedfollowingthisskipconnection.2.6.KAN-GALInourwork,wewerealsoinspiredbyAASIST,whichisbasedonthepremisethatgraphsarefullyconnectedbecauseitisim-possibletodeterminethedegreeofimportanceofeachnodetoagiventaskinadvance.IncontrasttoRawGat[32],theactiva-tionfunctionswerenotemployedduetothenovelutilizationofKANs.Theinitialoperationistoapplyadropoutwithaprobabilityof0.2.Subsequently,theattentionmaskisobtainedbynode-wisemultiplication(denotedas”×”)ofthenodesh,h∈RN,DandN–thenumberofnodes,D–nodedimensionality,andsubsequentpassingthroughtheKANlayer.Followingthis,thehyperbolictangentisapplied.TheresultingexpressionisthenmatrixmultipliedbytheattentionweightsWatt,whichhavebeeninitializedusingXavierinitialization.TheresultingvaluesarethendividedbythetemperatureT,resultinginanattentionmapAconsistingofthecorrespondingprobabilities,whichisobtainedusingthesoftmaxfunction:A=softmax(cid:18)tanh(KAN1(h×h))WattT(cid:19).(18)TheresultingattentionmapisprojectedusingKAN,andinparallel,itismultipliedbythematrixandprojected.Theresultingprojectionsarethenaddedtogetherandnormalizedbybatch:KAN-GAL(h)=BatchNorm(KAN2(Ah)+KAN3(h)).(19)
50

2.7.KAN-GraphPoolAsdescribedintheprevioussection,theinitialoperationistoapplyadropout,afterwhichtheresultingoutputpassesthroughtheKANlayerandistransformedbythesigmoidfunction,rep-resentedbythesymbolσ(×).Theresultingvalueisthenmulti-pliedelementwise(denotedas”⊙”)bytheoriginalgraph.Thedimensionalityissubsequentlyreducedusingtherankfunc-tion,whichreturnsthekmostsignificantnodesintheresultinggraph:KAN-GraphPool=rank((σ(KAN(h))⊙h),k).(20)2.8.KAN-HS-GALThelayeracceptsthreeinputs:ht,whichhasanodedimension-alityofDt(temporalgraph),hs,whichhasanodedimension-alityofDs(spatialgraph),andS(stacknode).InputgraphsareprojectedintoanotherlatentspaceusingKANlayerstoequalizetheirdimensionsandthenmergedtoformafullyconnectedhet-erogeneousgraphhstwithnodedimensionalityDst.Adropoutwithaprobabilityof0.2isthenappliedtotheresultinggraph:hst=CONCAT(KAN1(ht),KAN2(hs)).(21)TheprimaryattentionmapAisderivedbymultiplyingeachnodeinthegraphhstbyeveryothernode,withtheprojectionthroughtheKANlayerundergoingahyperbolictangenttrans-formation:A=tanh(KAN3(hst×hst)).(22)ToderiveasecondaryattentionmapB,theinitialatten-tionmapispartitionedintofourmatricesinaccordancewiththethresholdDt,definedasthenumberofnodesintheunder-lyinggraphhs.Subsequently,thesesegmentsaremultipliedbytheweightsW11,W12andW22:B=(cid:80)Dt+Dsm=1Aijm·W11m,∀i≤Dtandj≤Dt(cid:80)Dt+Dsm=1Aijm·W22m,∀i≥Dtandj≥Dt(cid:80)Dt+Dsm=1Aijm·W12m,otherwise.(23)ThematrixisthendividedbythetemperaturevalueTandpassedthroughtheSoftmaxfunction,therebyobtainingaprob-abilitymap:ˆB=softmax(cid:18)BT(cid:19).(24)Toproducetheattentionmapforthestacknodeupdate,theheterogeneousgraphhstistakenandmultipliedbytheStackNodeSnode-wise.TheresultinggraphisthenprojectedthroughtheKANlayer,passedthroughthetangent,andmatrixmultipliedbytheweightsWm.Thevalueobtainedissubse-quentlydividedbythetemperatureandpassedthroughsoftmax:Am=softmax(cid:18)tanh(KAN4(hst⊙S))T(cid:19).(25)CombiningtwoprojectionsobtainedusingKANlayerstoupdateastacknodeisnecessary.Thesearetheprojectionofthematrix-multipliedattentionmapAmandgraphhstandtheprojectionofthestacknode:ˆS=KAN5(Amhst)+KAN6(S)(26)TheupdateofhstisachievedbycombiningtwoprojectionsderivedfromKANlayers:theprojectionofthematrixmulti-pliedsecondaryattentionmapˆBandtheheterogeneousgraphhstandtheprojectionofthegraphhstitself.Theexpressionobtainedisthensubjectedtobatchnormalization:(cid:99)hst=BatchNorm(KAN7(ˆBhst)+KAN8(hst)).(27)TheresultingheterogeneousgraphisthendividedbackintotwocomponentsbymultiplicationwiththemaskmatricesM1andM2:(cid:98)ht=(cid:99)hstMt(28)Mt=(cid:18)It0s(cid:19),It∈RN×Dt,0s∈RN×Ds(29)(cid:99)hs=(cid:92)hstMs(30)Ms=(cid:18)0tIs(cid:19),0t∈RN×Dt,Is∈RN×Ds.(31)2.9.ModelsArchitectures2.10.AASIST3Intheclosedcondition,thefront-endisSincConv,whereas,intheopencondition,itisWav2Vec2XLS-R[30]withadditionallinearorconvolutionallayers,whichmaintainsthedimension-ality.TheapplicationofMaxPooling,BatchNormalization,andSELUprecededtheencoder:ˆx=Encoder(SELU(BatchNorm(MaxPool(x))),(32)wherexisaninputpre-emphasizedaudio.Subsequently,theacquiredfeaturesweredividedintotem-poralandspatialcomponents,afterwhichpositionalembedding(PE)wasincorporated.Inthismanner,graphswereformed,whichweresubsequentlypassedthroughaKAN-GALandaKAN-GraphPool:ht=KAN-GraphPool(KAN-GAL(maxt(abs(ˆx)+PEt)))(33)hs=KAN-GraphPool(KAN-GAL(maxs(abs(ˆx)+PEs))).(34)Theresultinggraphsandthepreviouslyinitializedstacknodewerepassedinparallelthroughfourbranches.Theini-tialstepistoapplyKAN-HS-GALineachbranch:(cid:98)ht2(cid:99)hs2(cid:98)S2=KAN-HS-GAL(cid:98)ht1(cid:99)hs1(cid:98)S1.(35)ThegraphsarethenpassedthroughKAN-GraphPool,andanotherKAN-HS-GALisappliedsimilarly:(cid:98)ht3(cid:99)hs3(cid:98)S3=KAN-HS-GALKAN-GraphPool((cid:98)ht2)KAN-GraphPool((cid:99)hs2)(cid:98)S2.(36)Toproducethefinalpredictions,allpreviouslyobtainedgraphsandStackNodesarestacked:Ht=(cid:98)ht1+(cid:98)ht2+(cid:98)ht3(37)Hs=(cid:99)hs1+(cid:99)hs2+(cid:99)hs3(38)Sf=(cid:98)S1+(cid:98)S2+(cid:98)S3.(39)Adropoutwithaprobabilityof0.2isappliedtoallobtainedgraphsandtheStackNodeafterfourbranches.Subsequently,
51

fortemporalandspatialgraphs,thenode-wisemaximumHmaxandmeanHmeanareidentified,aswellasthemaximumStacknodeSmaxf.Theresultingvaluespassthroughthedropoutwithaprobabilityof0.5andarethenconcatenatedintothefinalhid-denlayerL:L=CONCAT(Hmaxt,Hmeant,Hmaxs,Hmeant,Smaxf)(40)AfterL,aKANlayerreturnslogitsforeachclass.2.11.Wav2Vec2-Conv-AASIST-KANInadditiontotheproposedAASIST3,weutilizedthepre-trainedWav2Vec2encoderforthefeatureandproceededwith1DconvolutionstoprovidetheAASISTmodelwithaKANclassificationlayer.Itwasmotivatedbyinductivebiasesfromapre-trainedspeechencoderonalarge-scaledatasetinself-supervised(SSL)training,whichispreferablefortheopen-setcondition.3.ExperimentsandResults3.1.DescriptionoffinalapproachesFortheClosedCondition,weconsideredourdescribedAA-SIST3model.Themodelwasconstrainedtoacceptonlyfoursecondsofaudioasinput,whichprovedinsufficientforthemodelstoachieveadeepercomprehensionoftheaudioasawhole.Toaddressthislimitation,theaudiowasfedintothemodelinfour-secondpartssequentiallywithatwo-secondover-lapbetweenthem.Weappliedpre-emphasisforallaudioswithnoaugmentations.Theoptimalmodelswereidentifiedupontestingthemodelsonaclosedtestsubset:onewithtwobranchesandonewithfourbranches.Additionally,basedonthehypothesisthatSWLcanenhancetheresults,themodelincorporatingSWLwasutilized.Thepredictionsofthesemodelswereaveraged.Table1:FinalevaluationresultsofsubmittedpredictioninclosedandopenconditionCMtrack.conditionmodelt-DCFEERclosedAASIST30.535722.67open˜f0.14144.89Asillustratedinthetable2,manyofourmodificationspro-ducednotablysuperioroutcomescomparedtoAASIST,evenduringthevalidationphase.However,usingvarioustechniquestoenhancethequalityofanti-spoofingmodelsprovedineffec-tive,withallapproachesultimatelyresultinginadeclineintheobservedresults.FortheOpenCondition,toprovidethefinalpredictionorprobabilitythatgivenaudioxisbonafide,weaveragedthepre-dictionsoftwoofourmodelstraineddifferentlytoincreasegen-eralizationability.˜f=f′1(x)+f′2(x)2,(41)wheref′1(x)=m(cid:88)if1(xi)(42)f′2(x)=l(cid:88)jf2(xj),(43)and{xi}mi=1and{xj}lj=1aresomepartsoftheoriginalau-diox,specifically,sequentialpartswithintersection(forex-ample,0-4sand3-7saudiointervals)asinthesubmissionforClosedCondition.Thef1isAASIST3butwithapre-trainedWav2Vec2featureencoder.Thef2isoursecondmodelWav2Vec2+Conv+AASIST+KAN.f1wastrainedsimilarlyasinclosedconditiononlyontheprovidedtrainingset,whereasf2wastrainedontheunionofthegiventrainingsetplusadditionalbonafideaudiosfromMozillaCommonVoice,trainpartifVoxCeleb2.Forf2training,acom-binationofweightedCross-Entropy,Focal[33],andLibAUCM[34,35]losseswithAdamoptimizer.AugmentationmethodssuchasRIR,environmentalandGaussiannoises,VAD,andpitchshiftingwererandomlyapplied.Pre-emphasizingwasusedbeforeaugmentations.3.2.ExperimentswithdifferentfrontendsGiventheresultspresentedby[19],itwashypothesizedthatcombiningmultiplerepresentationsmightimprovethere-sult.CombiningtherawwaveformwiththeCQTandMel-spectrogramswasattempted,butnoimprovementwasseen.Inlightofthefindingsin[36],weinvestigatedusingthef0subbandindependentlyandinconjunctionwithSincConv.Inaddition,giventheevidencepresentedinthestudyreferencedin[37],whichindicatedthatLeafoutperformedSincConv,wecompareditsperformancewithourmodel’s.However,noneofthemodificationsresultedinaperformanceimprovement.Foropenconditions,thebestresultwasshownbyWav2Vec2[30]pre-trainedonXLSR-300,asfront-endbasedonatransformerandconvolutionalneuralnetworks,whichallowssuitableen-codingofbothtemporalandspatialinformationinaudio.Also,experimentswithXEUS[38]didnotprovidebetterresultscom-paredtootherfront-ends.3.3.ExperimentswithaugmentationsThisstudyemployedaseriesofaugmentations,includingpitchshift,speedchange,Gaussianandenvironmentalnoise,differ-entroomimpulseresponses,harmonicandpercussivecompo-nents,stretching,voiceactivitydetection(VAD)application,andpre-emphasis.Arandomportionoftheaudiosamplewasextractedorpaddedduringtraining.Insomeexperiments,thepre-emphasiswasusedasanaugmentation,inothersitwasap-pliedtoeachaudio.Additionally,weemployedAttentionAug-mentedConvolutionalNetworks[39]andRawboost[40].Thefindingsindicatethatpre-emphasisoneachaudiotrackrepre-sentsaneffectiveapproach.3.4.ExperimentswithdifferentbaseKANfunctionsAcomparisonofAReLU[41],PReLU,SELU,andRReLUshowedthatPReLUgavethemostrobustresults.3.5.ExperimentswithdifferentKAN-basedencodersAsoneofthekeyideasofourmodelistheuseofKAN,wechosetoexplorethepotentialofKAN-baseden-
52

KAN-HS-GAL-OperationRawwaveformnode stackingstacknodenode-wisemaximum & averageconcatenationandclassificationspoofedbona-fideXmaxt(abs(X))maxs(abs(X))GraphcombineKAN-HS-GAL-OperationKAN-HS-GAL-OperationKAN-HS-GAL-OperationPre-emphasisEncodergraphformationgraphformationKAN-GALKAN-GraphPoolKAN-GALKAN-GraphPoolFigure1:Architectureoftheclosedconditionmodel.stacknodeHS-GALhetero ATTstacknodeHS-GALhetero ATTKAN-HS-GAL-OperationKAN-GraphPoolFigure2:TheKAN-HS-GALOperation.coders,includingKAN+tokenization[42],ReluConvKANandWavKANConv[43],bothinstand-aloneformandcombinedwithasharpness-awareminimisation[20]mechanism.WhenevaluatedusingavalidationsetwithSAM,WavKanConvshowedfavorableresults,table2OurexperimentswithdifferentKANencodersfoundthatthemodelusingtheclassicRawNet2-basedencoderperformedbest.3.6.ExperimentswithdifferentKANsAcomparisonwasconductedbetweenparametricB-splinefunctionsandothertypesofpolynomials,includingBesselpolynomials,Chebyshevpolynomialsofthesecondkind,Gaus-sianradialbasisfunctions,Fibonaccipolynomials,radial-basisfunctions,andJacobipolynomials.TheresultsshowedthattheapproximationwithaB-splineoforder4gavethemostaccurateresults.3.7.ExperimentswithAASISTmodificationInordertotesttheeffectivenessoftheproposedmethodol-ogy,severalmodificationswereapplied.Theseincludedtheinsertionofthethirdadditionalbranchwithchannel-wisemax-imum,whichwasusedtoallowtheextractionofmorecom-plexfeatures.Furthermore,GraphPoolswerereplacedbyGALstoavoidremovingasignificantamountofinforma-tion.Theminimumwasusedinsteadofthemaximumtoformbranches,andfourbrancheswithHS-GALandSE[44]intheencoderwereused.SixbrancheswerealsousedwithHS-GAL,andpositionalencodingwasintroducedinsteadofpositionalembedding[?].Finally,acomparisonwasmadebetweentheresultsobtainedwithtwobranchesandthoseobtainedwiththeproposedmethodology.TheresultsshowthatthebestresultisobtainedusingfourbrancheswithHS-GAL.3.8.ExperimentswithscalingofHS-GALbranchesAdditionalHS-galswithdifferenttemperaturevalueswereap-pliedtotwobranches.Asshowninthetable2,theobtainedresultsdidnotexhibitaconsiderablyenhanceddegreeofim-provement.Theresultsallowustoconcludethatscalingthemodelinwidthismoreoptimalthanscalingitindepth.3.9.ExperimentswithdifferentencodersGiventhattheoriginalAASISTemploysaRawNet2-basedencoder,wepostulatedthataRawNet3-basedencoder[45]wouldenhancethemodel.Concurrently,weanticipatethatS2pecNet[19],astheauthorshavedemonstrated,willimprovetheresultthroughthisintegrationofsoundrepresentations.Ad-ditionally,weexploredthepotentialofWaveNet[46]asafront-end,butunfortunately,noneoftheexperimentsyieldedasig-nificantresult.3.9.1.ExperimentswithRes2Net-basedencodersFollowingAASIST2[17],weattemptedtoutilizeRes2Netinvariousconfigurationswithdisparatelearningrates.Concur-rently,weevaluatedSRLARES2net[36]asamoresophisti-catedanalog.TheresultsofourinvestigationsuggestthattheapplicationofRes2NetwiththeproposedAASISTconfigura-tionisnotaviableapproach.3.9.2.ExperimentswithResNet-basedencodersUtilizingalternativeencodersandmodificationstotheRes2Netencoderyieldednoperceptibleimprovementinresults.There-fore,aninvestigationwasconductedintoalternativechangestotheResNetencoder.Theseincludedtheutilizationofthef0sub-bandinsteadoftheSincConv,theuseoftwoResNetencodersfordifferentsegmentsofaudio,withandwithoutRawBoost,theintegrationofELA[47],thesubstitutionofBatchNormwithLayerNorm,theutilizationofpreluastheactivationfunction,andtheintegrationofSE.Theexperimentalresultsdemonstratethatmodifyingtheencoderdoesnotimproveoutcomes.3.10.ExperimentswithdifferentlossfunctionsFurthermore,inlinewithAASIST2[17],AM-Softmaxanditspredecessor,ArcFace[48],werealsotested.Baseduponthere-sultsofthestudy[49],focallosswasalsotested.Furthermore,weattemptedtoutilizegeneralizedcrossentropy[23],theeffec-
53

tivenessofwhichhasbeenpreviouslyestablishedforAASIST.Additionally,asintheoriginalAASIST,weattemptedtoutilizeweightedcross-entropy.Finally,weselectedmultitasklosses,hypothesizingthatthemodelwouldbecapableofextractingmorecomplexfeatures.However,ourfindingsindicatedthatregularcross-entropywasindeedefficacious.Consequently,thedeploymentoflosstoaddressclassimbalanceinourmodelcanbeconsideredineffective.However,forbettergeneralization,oursecondmodelwastrainedusingacombinationofweightedcross-entropy,focalloss,andLibAUCM[34,35]loss,whichisimpliedforthex-riskminimization.3.11.ExperimentswithdifferentoptimizersThefollowingoptimizerswereselectedforourexperiments:AdamW,Lion,NAdam,RAdam,andAdam.Asillustratedinthetable2,theoutcomeswithAdamWandLionexhibitedanotabledeclineinperformance.Inaddition,theresultswithRAdamwerefoundtobeunsatisfactory.Theresultsonthede-velopmentsubsetarepresentedinthetable2.ThefindingsindicatethatAdamistheoptimaloptimizerforourmodel.3.12.ExperimentswithdifferentlearningmethodsToenhancethegeneralizationcapacity,weemployedavarietyoftechniques,includingSAM[20],ASAM[21],andSWL[22].ThesewereinitiallyutilizedintheoriginalpaperaboutAASISTandledtoanotableenhancementinthequalityofthemodels.Furthermore,acosineannealingschedulerandaweightedran-domsamplerareemployed.Asillustratedinthetable2,thesestrategieshavealsobeendemonstratedtobeineffective.Itwasfoundthatnoneoftheproposedlearningmethodsimprovedtheresult.4.ConclusionTherapiddevelopmentofvariousdeeplearningalgorithmshascreatednewopportunitiesforgeneratingsyntheticaudious-ingTTSandVCsystems.Thisprogress,however,hasintro-ducedacorrespondingvulnerabilityinASVsystems,neces-sitatingthedevelopmentofaCMsystemtodetectsyntheticvoices.Inthispaper,weproposedanovelarchitecture,AA-SIST3,whichenhancestheoriginalAASISTframeworkbyincorporatingKolmogorov-Arnoldnetworks,additionallayers,andpre-emphasis.Furthermore,weintroducedmodificationsusingB-splinefeaturesastrainingfeaturesinspiredbyprevi-ousenhancementsinsyntheticspeechdetectionmodels.Inad-dition,weutilizedadditionaldata,scoresfusion,andaself-supervisedpre-trainedmodelasanencodertoachievethebestresults.Ourfindingsindicatedthatthesemodificationssig-nificantlyimprovemodelperformance,achievingamorethantwofoldimprovementoverAASIST.ThemodeldemonstratedminDCFresultsof0.5357underclosedconditionsand0.1414underopenconditions,affirmingtheeffectivenessofourcon-figuration.Table2:ResultsofexperimentswithAASIST3ondevsubset.ReferencedAASIST3modificationt-DCF[16]ClassicAASIST0.5671sec.3.1AASIST30.2657sec.3.2LeafinsteadofSincConv0.52sec.3.2F0subband+SincConv0.4406sec.3.2F0subbandinsteadofSincConv0.4225sec.3.2SincConv+CQT0.4083sec.3.3AttentionAugmentedConv2dinencoder0.4762sec.3.3Augmentationswithoutpre-emphasis0.4495sec.3.3Allaugmentations0.3624sec.3.4AReLUinsteadofPReLUinKAN0.3371sec.3.4SELUinsteadofPReLUinKAN0.3295sec.3.4RReLUinsteadofPReLUinKAN0.3045sec.3.5ReLUConvKANinsteadofConv0.699sec.3.5UKANinencoder0.3062sec.3.5WavKANConvinencoder0.3047sec.3.5WavKANConvinencoder+SAM0.2801sec.3.6BesselpolynomialsinKAN0.6805sec.3.6JacobiPolynomialsinKAN0.5666sec.3.6LegendrePolynomialsinKAN0.4665sec.3.6GegenbauerpolynomialsinKAN0.4051sec.3.6RBFinKAN0.3994sec.3.62ndChebyshevpolynomials0.3392sec.3.6FibonaccipolynomialsinKAN0.492sec.3.7UtilisingGATsinsteadofGraphPool0.4375sec.3.7GaussianRBF0.3536sec.3.74branchesof2HS-GALs+SE0.2905sec.3.73rdbranchwithchannel-wisemaximum0.4992sec.3.82branchesof3HS-GALswithtemp=1000.3077sec.3.82branchesof3HS-GALswithtemp=1500.2661sec.3.82branchesof3HS-GALswithtemp=2000.2862sec.3.9S2pecNetwith40batchsize0.4291sec.3.9S2pecNetwith28batchsize0.4225sec.3.9S2pecNetwith20batchsize0.4185sec.3.9RawNet3insteadofRawNet20.4901sec.3.9.1Res2Netencoderwithlr=1e-60.9066sec.3.9.1SRLARes2Netencoder0.7203sec.3.9.1Res2Netencoderwithlr=1e-50.6413sec.3.9.1SRLARes2Net+f0subband0.5463sec.3.9.1Res2Netencoder+PRELUwithlr=1e-40.485sec.3.9.2LayerNorminsteadofBatchNorm0.3591sec.3.9.2ResNet+effictivelocalattention0.3542sec.3.9.2ResNetwithPReLU0.3216sec.3.9.2ResNet+SE0.2902sec.3.10GeneralizedCrossEntropyLoss0.8438sec.3.10ArcFaceLoss0.4389sec.3.10MultitaskLoss0.3933sec.3.10FocalLoss0.3489sec.3.10AMSoftmax0.3363sec.3.11LioninsteadofAdam0.3702sec.3.11AdamWinsteadofAdam0.3200sec.3.11NAdaminsteadofAdam0.2889sec.3.11RAdaminsteadofAdam0.3006sec.3.12SAMrho=0.50.5012sec.3.12SAMrho=0.050.3727sec.3.12ASAM0.3532sec.3.12SWL0.2694sec.3.12Cosineannealingscheduler0.2991sec.3.12Weightedrandomsampler0.2989
54

5.References[1]NicholasEvans,TomiKinnunen,andJunichiYamagishi,“Spoofingandcountermeasuresforautomaticspeakerverification,”inINTERSPEECH2013,14thAnnualCon-ferenceoftheInternationalSpeechCommunicationAs-sociation,August25-29,2013,Lyon,France,ISCA,Ed.,Lyon,2013.[2]ZhizhengWuetal.,“Asvspoof2015:thefirstautomaticspeakerverificationspoofingandcountermeasureschal-lenge,”inINTERSPEECH2015,ISCA,Ed.,Dresden,2015.[3]TomiKinnunen,Md.Sahidullah,H´ectorDelgado,etal.,“TheASVspoof2017Challenge:AssessingtheLimitsofReplaySpoofingAttackDetection,”inProc.Interspeech2017,2017,pp.2–6.[4]XinWang,JunichiYamagishi,MassimilianoTodisco,etal.,“Asvspoof2019:Alarge-scalepublicdatabaseofsynthesized,convertedandreplayedspeech,”2020.[5]XuechenLiuetal.,“Asvspoof2021:Towardsspoofedanddeepfakespeechdetectioninthewild,”IEEE/ACMTrans-actionsonAudio,Speech,andLanguageProcessing,vol.31,pp.2507–2522,2023.[6]YongyiZang,YouZhang,MojtabaHeydari,andZhiyaoDuan,“Singfake:Singingvoicedeepfakedetection,”2024.[7]GalinaLavrentyeva,SergeyNovoselov,AndzhukaevTseren,MarinaVolkova,ArtemGorlanov,andAlexandrKozlov,“STCAntispoofingSystemsfortheASVspoof2019Challenge,”inProc.Interspeech2019,2019,pp.1033–1037.[8]SunmookChoi,Il-YoupKwak,andSeungsangOh,“OverlappedFrequency-DistributedNetwork:Frequency-AwareVoiceSpoofingCountermeasure,”inProc.Interspeech2022,2022,pp.3558–3562.[9]MoustafaAlzantot,ZiqiWang,andManiB.Srivastava,“DeepResidualNeuralNetworksforAudioSpoofingDe-tection,”inProc.Interspeech2019,2019,pp.1078–1082.[10]Cheng-ILai,NanxinChen,Jes´usVillalba,andNajimDe-hak,“ASSERT:Anti-SpoofingwithSqueeze-ExcitationandResidualNetworks,”inProc.Interspeech2019,2019,pp.1013–1017.[11]DiegoCastanetal.,“Speaker-TargetedSyntheticSpeechDetection,”inProc.TheSpeakerandLanguageRecogni-tionWorkshop(Odyssey2022),2022,pp.62–69.[12]Il-YoupKwaketal.,“Voicespoofingdetectionthroughresidualnetwork,maxfeaturemap,anddepthwisesep-arableconvolution,”IEEEAccess,vol.PP,pp.1–1,012023.[13]XinhuiChen,YouZhang,GeZhu,andZhiyaoDuan,“URChannel-RobustSyntheticSpeechDetectionSystemforASVspoof2021,”inProc.2021EditionoftheAutomaticSpeakerVerificationandSpoofingCountermeasuresChal-lenge,2021,pp.75–82.[14]LeiWuandYeJiang,“Attentionalfusiontdnnforspoofspeechdetection,”in20225thInternationalConferenceonPatternRecognitionandArtificialIntelligence(PRAI),2022,pp.651–657.[15]AwaisKhanandKhalidMalik,“Spotnet:Aspoofing-awaretransformernetworkforeffectivesyntheticspeechdetection,”in2ndACMInternationalWorkshoponMul-timediaAIagainstDisinformation(MAD’23),062023.[16]JeeweonJungetal.,“Aasist:Audioanti-spoofingus-ingintegratedspectro-temporalgraphattentionnetworks,”2021.[17]YuxiangZhang,JingzeLu,ZengqiangShang,WenchaoWang,andPengyuanZhang,“Improvingshortutteranceanti-spoofingwithaasist2,”2024.[18]HemlataTaketal.,“Automaticspeakerverificationspoof-inganddeepfakedetectionusingwav2vec2.0anddataaugmentation,”2022.[19]PenghuiWenetal.,“RobustAudioAnti-SpoofingwithFusion-ReconstructionLearningonMulti-OrderSpectro-grams,”inProc.INTERSPEECH2023,2023,pp.271–275.[20]PierreForet,ArielKleiner,HosseinMobahi,andBehnamNeyshabur,“Sharpness-awareminimizationforefficientlyimprovinggeneralization,”2021.[21]JungminKwon,JeongseopKim,HyunseoPark,andInKwonChoi,“Asam:Adaptivesharpness-awaremin-imizationforscale-invariantlearningofdeepneuralnet-works,”2021.[22]ZhiyongWang,RuiboFu,ZhengqiWen,YuankunXie,YukunLiu,XiaopengWang,XuefeiLiu,YongweiLi,JianhuaTao,YiLu,XinQi,andShuchenShi,“General-izedfakeaudiodetectionviadeepstablelearning,”2024.[23]HyejinShim,MdSahidullah,JeeweonJung,ShinjiWatanabe,andTomiKinnunen,“Beyondsilence:Biasanalysisthroughlossandasymmetricapproachinaudioanti-spoofing,”2024.[24]SiwenDing,YouZhang,andZhiyaoDuan,“Samo:Speakerattractormulti-centerone-classlearningforvoiceanti-spoofing,”2022.[25]ZimingLiu,YixuanWang,SachinVaidya,FabianRuehle,JamesHalverson,MarinSoljaˇci´c,ThomasY.Hou,andMaxTegmark,“Kan:Kolmogorov-arnoldnetworks,”2024.[26]A.N.Kolmogorov,“OntheRepresentationofcontinuousfunctionsofseveralvariablesassuperpositionsofcontin-uousfunctionsofasmallernumberofvariables.,”Dokl.Akad.Nauk,vol.108,no.2,1956.[27]J¨urgenBraunandMichaelGriebel,“OnaconstructiveproofofKolmogorov’ssuperpositiontheorem,”Construc-tiveapproximation,vol.30,pp.653–675,2009,Publisher:Springer.[28]A.N.Kolmogorov,“Ontherepresentationofcontinu-ousfunctionsofmanyvariablesbysuperpositionofcon-tinuousfunctionsofonevariableandaddition,”inDok-ladyAkademiiNauk.1957,vol.114,pp.953–956,Rus-sianAcademyofSciences.[29]MircoRavanelliandYoshuaBengio,“Speakerrecogni-tionfromrawwaveformwithsincnet,”2019.[30]AlexeiBaevski,HenryZhou,AbdelrahmanMohamed,andMichaelAuli,“wav2vec2.0:Aframeworkforself-supervisedlearningofspeechrepresentations,”2020.
55

[31]AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin,“Attentionisallyouneed,”Ad-vancesinneuralinformationprocessingsystems,vol.30,2017.[32]HemlataTak,JeeweonJung,JosePatino,MadhuKam-ble,MassimilianoTodisco,andNicholasEvans,“End-to-endspectro-temporalgraphattentionnetworksforspeakerverificationanti-spoofingandspeechdeepfakedetection,”2021.[33]Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,andPiotrDoll´ar,“Focallossfordenseobjectdetection,”inProceedingsoftheIEEEinternationalconferenceoncomputervision,2017,pp.2980–2988.[34]TianbaoYang,“Algorithmicfoundationofdeepx-riskoptimization,”arXivpreprintarXiv:2206.00439,2022.[35]ZhuoningYuan,DixianZhu,Zi-HaoQiu,GangLi,Xuan-huiWang,andTianbaoYang,“Libauc:Adeeplearninglibraryforx-riskoptimization,”in29thSIGKDDConfer-enceonKnowledgeDiscoveryandDataMining,2023.[36]CunhangFan,JunXue,JianhuaTao,JiangyanYi,Chen-glongWang,ChengshiZheng,andZhaoLv,“Spatialreconstructedlocalattentionres2netwithf0subbandforfakespeechdetection,”2023.[37]NeilZeghidour,OlivierTeboul,F´elixdeChau-montQuitry,andMarcoTagliasacchi,“Leaf:Alearnablefrontendforaudioclassification,”2021.[38]WilliamChenetal.,“Towardsrobustspeechrepresenta-tionlearningforthousandsoflanguages,”arXivpreprintarXiv:2407.00837,2024.[39]IrwanBello,BarretZoph,AshishVaswani,JonathonShlens,andQuocV.Le,“Attentionaugmentedconvo-lutionalnetworks,”2019.[40]HemlataTak,MadhuKamble,JosePatino,MassimilianoTodisco,andNicholasEvans,“Rawboost:Arawdataboostingandaugmentationmethodappliedtoautomaticspeakerverificationanti-spoofing,”2021.[41]DengshengChen,JunLi,andKaiXu,“Arelu:Attention-basedrectifiedlinearunit,”2020.[42]ChenxinLietal.,“U-kanmakesstrongbackboneformed-icalimagesegmentationandgeneration,”2024.[43]IvanDrokin,“Kolmogorov-arnoldconvolutions:Designprinciplesandempiricalstudies,”2024.[44]JieHu,LiShen,SamuelAlbanie,GangSun,andEnhuaWu,“Squeeze-and-excitationnetworks,”2017.[45]JeeweonJung,YouJinKim,Hee-SooHeo,Bong-JinLee,YoungkiKwon,andJoonSonChung,“Pushingthelimitsofrawwaveformspeakerrecognition,”2022.[46]AaronvandenOord,SanderDieleman,HeigaZen,KarenSimonyan,OriolVinyals,AlexGraves,NalKalchbrenner,AndrewSenior,andKorayKavukcuoglu,“Wavenet:Agenerativemodelforrawaudio,”2016.[47]WeiXuandYiWan,“Ela:Efficientlocalattentionfordeepconvolutionalneuralnetworks,”2024.[48]JiankangDeng,JiaGuo,JingYang,NiannanXue,IreneKotsia,andStefanosZafeiriou,“Arcface:Additiveangu-larmarginlossfordeepfacerecognition,”2018.[49]QiaoweiMa,JinghuiZhong,YitaoYang,WeihengLiu,YingGao,andWingW.Y.Ng,“Convnextbasedneuralnetworkforaudioanti-spoofing,”2022.