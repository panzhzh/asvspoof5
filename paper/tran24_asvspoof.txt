The Automatic Speaker Verification Spoofing Countermeasures Workshop (ASVspoof 2024)
31 August 2024, Kos, Greece

9

10.21437/ASVspoof.2024-2

ParallelChainLab’sAnti-spoofingSystemsforASVspoof5ThienTran,ThanhBui,PanagiotisSimatisParallelChainLab,Singapore{thien.tran,bui.ducthanh,panos}@parallelchain.ioAbstractTherecentriseofgenerativeAImakesdetectingau-diodeepfakesincreasinglychallenging.DeeplearningtechniquesproducehighlyrealisticfakeaudiothatcandeceivebothhumansandAutomaticSpeakerVerifica-tion(ASV)systems.ThispaperpresentsParallelChainLab’ssubmissionstotheASVspoof5challenge,namelyavoiceanti-spoofingsystemandaspoofing-robustASVsystem.Wedevelopedanensemblearchitecturecompris-ingmodelstrainedwithvariousaugmentationtypes,in-cludingwaveformaugmentations,mel-spectrogramaug-mentations,andvocodersynthesis.Anextensiveexper-imentalevaluationconfirmstheefficacyofoursystems,achievingminDCFof0.2660forthedeepfakedetectionsystemandmina-DCFof0.3173forthespoofing-robustASVsystemintheclosedcondition.1.IntroductionAdvanceddeeplearningtechniquesproducesyntheticvoicescapableofspoofingsecuritysystemsandhumansalike.Furthermore,thewidespreadavailabilityofText-To-Speech(TTS)andVoiceConversion(VC)toolsin-creasestheprevalenceofdeepfakemedia.Withoutro-bustanti-spoofingmeasures,ASVsystemsareatriskofunauthorizedaccess,compromisingusertrustanddataintegrity.ASVspoofchallengesleadthedevelopmentofcoun-termeasuresagainstaudiodeepfakes.Thisyear’sitera-tion,ASVspoof5,featurestwotracks:1)deepfakede-tectionand2)Spoofing-robustAutomaticSpeakerVerifi-cation(SASV).Eachtrackoffersclosedandopencondi-tions,withtheopenconditionpermittingtheuseofex-ternaldataandpre-trainedmodels[1].Moreover,theASVspoof5databaseposesagreaterchallengecomparedtopreviousyears,withrecordingscapturedwithdiversedevicesandacousticconditions,aswellasincorporatingstate-of-the-artTTS[2,3],VC[4],andAdversarialAt-tack[5].Similartothepreviousiteration,ASVspoof5providesfourdatasplitsforeachtrack:training,devel-opment,andprogresssetsformodeldevelopment,andevaluationsetforthefinalsubmission.Weparticipateintheclosedconditionoftrack1andtrack2todevelopinnovative,data-independenttechniques,andbuildresilientdeepfakedetectionandSASVsystems.Thisapproachensuresthatoursys-temscanbefurtherimprovedusingpubliclyavailabledatasetsandpre-trainedmodels.Toachievethisgoal,wecombinefindingsfrompastcompetitions[6,7,8]withhand-crafteddataaugmentationsdesignedspecif-icallyforvoiceanti-spoofing,aswellasourexpertiseintrainingfastandrobustdeeplearningmodels.Con-cretely,ourcontributionsare:•Asecuredeepfakedetectorforvoiceanti-spoofing.•ArobustSASVsystemforuserauthentication.•Apipelineofaugmentationsfordeepfakedetectionwithlimitedtrainingdata.Theremainderofthepaperisorganizedasfollows.Section2describesoursystemarchitectureandmethodsused.Section3outlinesthemodeltrainingsetups,exper-iments,andresultsanalysis.Lastly,Section4concludesthepaper.2.SystemOverview2.1.Track1:Speechdeepfakedetection(closedcon-dition)2.1.1.InputfeaturesThetwomostpopularinputfeaturesforvoiceanti-spoofingarerawwaveformandmel-spectrogram.Wechoosemel-spectrogramsinceitcanbeviewedasa2Dimagewithasinglechannel.Thisenablesustoleverageestablishedvision-relatedarchitecturesandtrainingtech-niquesformodeloptimization.Table1demonstratesthedifferentsettingsofthemel-spectrogramtransform.Thetransformparametersarevariedacrossmodelstoincreasefeaturediversityforlatermodelensembling.Inaddition,wetakethelogarithmofthemel-spectrogramtoobtainlog-energies,andapplymeannormalizationalongthetimeaxis.Thelaststepisequivalenttogainnormalizationforeachmelfeature.Thenumberofmelfiltersusedisconsiderablylargerthanthetypicallychosenvaluesof80orsmaller[7,8].Wefindthistobebeneficialoverusinglongeraudiosam-plesandasmallerhopsize,suggestingthatspectralres-olutionisimportantfordeepfakedetection.Neverthe-less,usingmorethan160melfiltersyieldsdiminishing
10

Table1:Mel-spectrogramparameters.ParameterValueWindowtypeHann,Povey[9]Windowsize400,512Hopsize160FFTsize512No.ofmelfilters120,128,160returnswithsubstantiallylongertrainingtime.Theseex-perimentsareomittedforbrevity.2.1.2.ModelarchitectureForbothtracks,weuseamodifiedvariantoftheResidualNetwork(ResNet)[10].Inpreliminaryexperimentation,wefoundthatResNetisnottoosensitivetotraininghy-perparameters,thusallowingustoconcentrateondesign-ingeffectivedataaugmentations.TheResNetvariantusedoriginatesfrompriorspeakerverificationworks[11],andhasbeenadaptedfordeepfakedetection[7].Thefirstconvolutionlayer(withstride2)andthemaxpoolinglayerarereplacedwithasinglestride1convolutionlayer,reducingthetotalstrideofthemodelfrom32to8.Withfewerdownsamplingoperations,themodelcapturesmorefine-graineddetailsfromthemel-spectrogram,whichiscrucialinidentifyingdeepfakeartifacts.Duetotimeconstraints,wetrainfromscratchthesmallResNet-34architecture(i.e.,7millionparameters)fordeepfakedetection.OurimplementationisadaptedfromtheWeSpeakertoolkit[12].WebrieflyexperimentedwithothermodernimageclassificationnetworkssuchasVisionTransformer(ViT)[13]andConvNeXt[14].Althoughthesearchitecturesshowimpressiveresultsforvision-relatedtasks,wefindthemlesssuitableforvoiceanti-spoofing.Figure1showsResNet-34’ssuperiorityintermsof(i)convergencespeed(lowerlossatthesamenumberoftrainingsteps),(ii)computationalefficiency(fastertotrain),and(iii)train-ingstability(smallerfluctuationsinlosscurves).Thisobservationagreeswithpreviousworksindicatingthatspecificarchitecturesrequiretailor-madeaugmentationsforeffectivelearning[15,16].2.1.3.DataaugmentationToimprovethesystem’srobustnessandgeneralizationtounseendata,weemployawiderangeofaugmentationmethods.Figure2illustratestheeffectsofourdataaug-mentationsonthemel-spectrogram.Waveformaugmentations.Theseaugmentationsareappliedontherawwaveformbeforethemel-spectrogramtransformation.1.Timemasking:Werandomlyreplacetimeintervals0.00.51.01.52.02.5Elapsedtime(hr)0.00.20.40.60.8TraininglossResNet-34ViT-TinyFigure1:Trainingloss(i.e.,binarycrossentropy)curvesofResNet-34(7Mparameters)andViT-Tiny(6Mparam-eters)usingthesamedataaugmentationsandtraininghyperparameters(10ksteps).Thecurvesaresmoothedwithexponentialmovingaverageforclearervisualiza-tion,withtheshadedregionbeingonestandarddeviationawayfromthemean.withzeros.ThiscanbeseenasanextensionofCutOut[17]toaudiodata,whichhelpsthemodelbemoreresistanttodatacorruption.2.Backgroundnoise:Wesamplearandomsignal-to-noiseratio(SNR)valuefromapre-definedrange,selectarandomnoiseaudio,andaddittothetrain-ingdatawiththedesiredSNR.WeusethenoiseandmusicportionsoftheMUSANcorpus[18],aswellasASVspoof5’sbona-fidesamplesfromthetrainingsplitasbackgroundnoise.Toadheretothechallenge’srules,weexcludethevocalsamplesfromMUSAN’smusicportion.3.RawBoost[19]:Followingthepaper’srecommen-dations,weapplylinearandnon-linearconvolu-tivenoise,andimpulsivesignal-dependentadditivenoise,butnotstationarysignal-independentaddi-tivenoise.4.Speedperturbation[20]:Weadjusttheaudiospeedusingaudioresamplingwiththeratiorangingfrom0.9to1.1.5.Codec:Weencodeanddecodetheaudiowithdif-ferentcodecs,includingMP3,G.722,OGG,AAC,OPUS,Vorbis,a-law,andµ-law.Foreachcodec,werandomlysampleatargetbitratefromapre-definedrange.6.Audioshuffling:Wedivideaspoofaudiosampleintosmallsegmentsof0.1secondseach,andshuf-flethem.Thismethodteachesthemodelspeech
11

Original(T0000077083)TimemaskingMUSANnoise(SNR=5)RawBoostSpeedperturbation(0.8x)Codec(MP316kbps)AudioshufﬂeCutMixFigure2:Visualizationofdifferentaugmentationtech-niquesonthemel-spectrogram.Someaugmentationsareexaggeratedforbettervisualization.consistencywithoutexplicitsupervision.Thisismotivatedbythefactthatshuffledaudiosoundsgibberishtohumans,butappearsindistinguishableinitsmel-spectrogram(seeFigure2).Mel-spectrogramaugmentation.WeapplyCutMix[21]tothemel-spectrogramsofthespoofsamples.Thistechniqueincreasesdeepfakediversity,allowingasin-glesampletocontainmultipleattacks.Wealsoexperi-mentedwithSpecAugment[22]butitdidnotyieldim-provements.Vocodersynthesis.Inspiredby[6],wegeneratenewspoofdatausingavarietyoftraditionalandneuralvocoders.Sincemostmodernvoicesynthesissystemsrelyonvocodersastheirlaststeptoconvertspectrogramstoaudiowaveforms,theabilitytodistinguishvocoderar-tifactsenhancesthemodel’sperformance.Weselectthefollowingvocoders:1.Traditionalvocoders(notlearned):Griffin-Lim[23]andWORLD[24].Wedirectlyapplythesevocodersonbona-fidesamples.2.Neuralvocoders(learned):HiFiGAN[25]andPar-allelWaveGAN[26].Wetrainthesevocodersfromscratchonbona-fidedataandinferthemonspec-trogramofspoofdata.WeusetheimplementationfromCoquiTTS1.Labelambiguity.Certainaugmentationsareappliedexclusivelytospoofsamplestoavoidlabelambiguity.1https://github.com/coqui-ai/TTSInsimpleterms,augmentedbona-fidesamplescannolongerbeconsideredgenuine,suchasthosealteredbyaudioshufflingandmel-spectrogramCutMix.Addition-ally,neuralvocodersmayproduceaudiothatistoosim-ilartotherealdatathattheyaretrainedon(possiblyacaseofoverfitting),thusconfusingthedeepfakedetec-tionmodel.Bytrainingneuralvocodersonthebona-fidesubsetandinferringonthedeepfakesubset,weensurethatthegeneratedaudioisindeedspoof.OnlineandOfflineaugmentation.Dependingontheprocessingspeedofeachmethod,weapplyaugmen-tationsonline(on-the-flyduringtraning)oroffline(pre-processbeforetraining).Onlineprocessingallowsinfi-nitegenerationofaugmenteddata,whiletheofflinecoun-terpartonlyproducesasingleversionofalteredsamples.Onlyspeedperturbation,audioshuffling,andvocodersynthesisaredoneofflineduetotheirheavycomputa-tionalrequirements.2.1.4.Post-processingTest-timeaugmentation(TTA).TTAisapopularmethodtoboostsystemperformanceinmanycomputervisiontasks[27].Weadaptthistechniqueforspeechdeepfakedetection.Foreachaudiosample,wemakesev-eralrandomfour-secondcrops,andscorethemindepen-dently.Thecropsmayoverlap.Thefinalscoreforthesampleisobtainedbyaveragingtheindividualscores.2.1.5.EnsemblingModelensemblingisacommontechniquetoimproveasystem’sgeneralizationonunseendata.Tothisend,wetrainseveralmodelswithdifferentinputfeatures,dataaugmentations,andtraininghyperparameters,andselectthe10bestperformers.Tocombinethemodelscores,weexperimentwithtwomethods:linearregressionandscoreaveraging.Weonlyconsiderlinearregressionasalearning-basedfusionapproachtoavoidoverfitting.However,evenwithsuchasimplemodel,weconsistentlyobservedegradedperformanceontheprogresssetwhentrain-ingthefusionmodelonthetrainingordevelopmentset.WepostulateadomaingapbetweenvarioussplitsoftheASVspoof5dataset.Consequently,weuseasimpleweightedaverageofindividualmodelscoresasourfinalsystemprediction.2.2.Track2:Spoofing-robustAutomaticSpeakerVerification(closedcondition)OurSASVsystemconsistsoftwosub-systems:deepfakedetectionandspeakerverification.SincethecompetitionpermitstheuseoftheVoxCeleb2[28]intrack2,wecon-sideredfine-tuningexistingspeakerembeddingmodels
12

pre-trainedonVoxCeleb22,inadditiontotrainingmod-elsfromscratch.2.2.1.Deepfakedetectionsub-systemWereusethebestperformingmodelsfromtrack1asdeepfakedetectionmodelsfortrack2.Additionally,wefine-tuneResNet-152andResNet-293pre-trainedonVoxCeleb2withaspeakerverificationobjectivefordeep-fakedetection.Thefine-tuningprocedurecloselyfollowsthemethodsdescribedinSubsection2.1.ThelargeanddiverseVoxCeleb2corpususedinthepre-trainingstagehelpedthesemodelslearndiscriminativespeechfeatures,enablingthemtogeneralizebetterthanmodelstrainedsolelyonASVspoof5.Intotal,thedeepfakedetectionoftrack2utilizesfourResNet-34modelstrainedfromscratchintrack1,oneResNet-152andoneResNet-293fine-tunedfromaVox-Celeb2checkpoint.2.2.2.ASVsub-systemWeexperimentedwithdomainadaptationforthepre-trainedResNetmodels.Thiswasdonebyfine-tuningthemonASVspoof5withtheobjectiveofspeakerverifi-cation(i.e.,CosFace[29]).However,fine-tuningresultedinworseASVperformanceonthedevelopmentset.Wesuspectthatthelackofspeakerdiversity(400uniquespeakersinASVspoof5trainingsetcomparedto5,994uniquespeakersinVoxCeleb2)accountsforthedropinaccuracy.Consequently,wedirectlyusetheVoxCeleb2pre-trainedResNet-221andResNet-293forourspeakerverificationsub-system.2.2.3.SASVscorefusionToobtainasingledeepfakedetectionscore,alsocalledcountermeasure(CM)scoreinthecompetition,weav-eragescoresfromindividualmodels.WedothesameproceduretoobtainasingleASVscore.DirectlytakingtheaverageofASVandCMscoresisimpracticalduetotheirdifferentrangeofvalues.ASVscorevariesfrom-1to1asitisthecosinesimilaritybe-tweentheembeddingsofthespeakerandtrialutterance,whileCMscore,beingthelogitofbona-fideprobability,hasanunboundedrangeof(−∞,∞).Totacklethisproblem,wemodelSASVscoretobealinearcombinationofASVandCMscores.Sincethereareonlytwolearnableweights,theriskofoverfittingisminimal.COBYQAalgorithm[30]isusedtolearnthelinearweightswhileminimizingmina-DCFonthede-velopmentset.3.Experiments3.1.DatasetsTheASVspoof5database[1]isbuiltbasedontheMLSEnglishdataset[31].Thetrainingsplitconsistsof18,797bona-fideand163,560spoofsamples,comingfromeightdifferentattacktypes,whilethedevelopmentsplitcon-tains31,334genuineand109,616deepfakesamples,ag-gregatedacrosseightvoicesynthesissystems.Thereisnooverlapofattacktypesacrossdifferentsplits.Fordevelopingourdeepfakedetectionsystem,weusetheASVspoof5trainingsplitfortrainingandthedevel-opmentsplitforvalidation.Theprogresssplitisfurtherusedforthefinalmodelselection,basedonthecompe-tition’sonlineevaluationduringtheprogressphase.Theprimarymetricforeachtrack(i.e.,minDCFandmina-DCFfortrack1andtrack2respectively)isusedforrank-ingthemodels.3.2.TraininghyperparametersWetrainourmodelswithAdamWoptimizer[32],learn-ingrate3.0e−4,weightdecay1.0e−2,binarycrossen-tropyloss,batchsize64.Sincetherearesignificantlymorespoofsamplesthanbona-fideones(roughly9:1),weaddressclassimbalancebyupsamplinggenuinein-stances(i.e.,makingthemappearfivetimesmorefre-quentlyinthetrainingset).Forfine-tuningpre-trainedmodels,weusealowerlearningrateof3.0e−5andweightdecayof1.0e−4.Apartfromthecommontrainingconfigurationabove,weapplydifferenttraininghyperparameterstoencouragediversityinmodelensembling.Firstly,wecreatemel-spectrogramswithavarietyofparameters(seeTable1).Inaddition,werangethenumberoftrainingstepsfrom200,000to500,000(effectively50-120epochs).Finally,weusecosinedecay[33]andconstantlearningschedule,bothofwhichhavelinearwarm-upfromzero.Forthelatterschedule,weadditionallyapplyExponentialMov-ingAverage(EMA)ofmodelweightstoproducecheck-points[34].3.3.Results3.3.1.Track1:Speechdeepfakedetection(closedcon-dition)Singlemodelcomparisons.Table2showsourbestmodelsontheprogresssetandtheirmaintraininghy-perparameters.X1-X10areResNet-34modelstrainedfromscratchfortrack1,whileY1andY2arefine-tunedResNet-152andResNet-293fortrack2.ComparingtheXmodels,weobservethatmetricsonthedevelopmentsetdonotcorrelatestronglywithprogresssetresults.For2https://github.com/wenet-e2e/wespeaker/blob/master/docs/pretrained.md
13

Table2:Performanceofourdeepfakedetectionmodels.X1-X10areusedintrack1and2,whileY1-Y2areexclusivelyusedintrack2.Onlythemaindifferencesbetweenmodelsarehighlighted.ForX1-X10,thebestandthesecondbestresultsarestylized.DataaugmentationTrainingparams.ProgresssetDevelopmentsetModelArch.TimeMaskNoise†SpeednmelsnstepsminDCFEER(%)minDCFEER(%)X1R34N120200k0.07412.670.09073.88X2R34N,M✓120480k0.06782.340.11404.57X3R34✓N120200k0.07542.720.10574.26X4R34✓N,M✓120320k0.07432.600.08593.26X5R34✓N,M,S128200k0.06232.280.10084.20X6R34✓N,M,S128280k0.07282.700.12325.13X7R34✓N,M,S✓120400k0.07572.620.09393.66X8R34✓N,M,S✓128220k0.05692.200.10684.49X9R34✓N,M,S✓160200k0.06142.210.09714.16X10*R34✓N,M,S✓160200k0.07392.620.07953.23Y1R152**✓N,M,S✓8020k0.05612.020.03311.35Y2R293**✓N,M,S8025k0.05822.100.08313.43†Backgroundnoise:[N]oiseand[M]usicfromMUSAN,[S]peechfromASVspoof5bona-fide.*X10istrainedwithSGDoptimizer.**Thesemodelsarefine-tunedfromVoxCeleb2checkpoints.example,thebestmodelontheprogressset(i.e.,X8)per-formssignificantlyworsethanthebestmodelonthede-velopmentset(i.e.,X10).Thisdiscrepancymaybeduetothelackofgeneralizationtounseendata.Therefore,werelyonheavyaugmentationsandmodelensemblingforrobustpredictionsontheevaluationset.Modelensembling.BasedontheresultsfromTable2,weassignaweightof0.125forthetopfourmodelsontheprogressset(i.e.,X8,X9,X5,X2),andaweightof0.083totherest(i.e.,X6,X10,X1,X4,X3,X7).Thisstrategyensuresthatthebetter-performingmodelscon-tributeslightlymoretothefinalpredictions.Table3:DeepfakedetectionresultsontheevaluationsetModelminDCFEER(%)ASVspoof5baseline10.827036.04ASVspoof5baseline20.711029.12Ours0.26609.18Evaluationsetresults.Table3showstheresultsofourmodelensemblefortrack1(i.e.,deepfakedetec-tion).WeachieveminDCFof0.2660andEERof9.18%,aroundthreetimesbetterthanthecompetitionbaselines,showingtheeffectivenessofourdataaugmentationstrat-egyandensembling.3.3.2.Track2:Spoofing-robustAutomaticSpeakerVeri-fication(closedcondition)Deepfakedetectionsub-system.Comparingthefine-tunedmodels(i.e.,Y1andY2)withthebesttrained-from-scratchResNet-34(i.e.,X8),althoughtheyarecomparableontheprogressset,Y1achievessignificantlylowerminDCFonthedevelopmentset.Unsurprisingly,fine-tuningpre-trainedmodelsisbeneficial.ToobtainthefinalCMscore,weaveragethescoresofY1,Y2,andthetopfourResNet-34models(i.e.,X8,X9,X5,X2).Table4:SASVresultsontheevaluationsetModelmina-DCFASVspoof5baseline0.6810Ours0.3173Evaluationsetresults.Table4showsourSASVsys-temachievesmina-DCFof0.3173,whichismorethantwotimeslowerthanthecompetitionbaseline.4.ConclusionThispaperpresentsParallelChainLab’ssubmissionsfortheASVspoof5challenge,focusingondeepfakedetec-tionandSASVsystems.OurdeepfakedetectionutilizesamodifiedResNetarchitectureandaplethoraofdataaug-mentationtechniques,whiletheSASVsystemcombinespre-trainedmodelswithourdeepfakedetectionsystem.Ourfinalresults,withaminDCFof0.2660fordeepfakedetectionandamina-DCFof0.3173forSASVintheclosedcondition,outperformtheASVSpoof5baselinesanddemonstratetheireffectivenessagainstsophisticateddeepfakeattacks.
14

5.References[1]XinWang,H´ectorDelgado,HemlataTak,Jee-weonJung,Hye-jinShim,MassimilianoTodisco,IvanKukanov,XuechenLiu,MdSahidullah,TomiKin-nunen,NicholasEvans,KongAikLee,andJunichiYamagishi,“ASVspoof5:Crowdsourcedspeechdata,deepfakes,andadversarialattacksatscale,”inASVspoofWorkshop2024(accepted),2024.[2]VadimPopov,IvanVovk,VladimirGogoryan,Tas-nimaSadekova,andMikhailKudinov,“Grad-TTS:ADiffusionProbabilisticModelforText-to-Speech,”inProceedingsofthe38thInternationalConferenceonMachineLearning,MarinaMeilaandTongZhang,Eds.18–24Jul2021,vol.139ofProceedingsofMachineLearningResearch,pp.8599–8608,PMLR.[3]JaehyeonKim,JungilKong,andJuheeSon,“Con-ditionalvariationalautoencoderwithadversariallearningforend-to-endtext-to-speech,”inPro-ceedingsofthe38thInternationalConferenceonMachineLearning,MarinaMeilaandTongZhang,Eds.18–24Jul2021,vol.139ofProceedingsofMachineLearningResearch,pp.5530–5540,PMLR.[4]SanggilLee,WeiPing,BorisGinsburg,BryanCatanzaro,andSungrohYoon,“BigVGAN:AUni-versalNeuralVocoderwithLarge-ScaleTraining,”inTheEleventhInternationalConferenceonLearn-ingRepresentations,2023.[5]MichelePanariello,WanyingGe,HemlataTak,MassimilianoTodisco,andNicholasEvans,“Malafide:anoveladversarialconvolutivenoiseattackagainstdeepfakeandspoofingdetectionsystems,”inProc.INTERSPEECH2023,2023,pp.2868–2872.[6]HaochenWu,ZhuhaiLi,LuzhenXu,ZhentaoZhang,WentingZhao,BinGu,YangAi,YexinLu,JieZhang,ZhenhuaLing,etal.,“TheUSTC-NERCSLIPSystemforthetrack1.2ofAudioDeepfakeDetection(ADD2023)Challenge.,”inDADA@IJCAI,2023,pp.119–124.[7]AlexanderAlenin,NikitaTorgashov,AntonOkhot-nikov,RostislavMakarov,andIvanYakovlev,“ASubnetworkApproachforSpoofingAwareSpeakerVerification,”inProc.Interspeech2022,2022,pp.2888–2892.[8]AntonTomilov,AlekseiSvishchev,MarinaVolkova,ArtemChirkovskiy,AlexanderKondratev,andGalinaLavrentyeva,“STCAntispoofingSys-temsfortheASVspoof2021Challenge,”inProc.2021EditionoftheAutomaticSpeakerVerificationandSpoofingCountermeasuresChallenge,2021,pp.61–67.[9]DanielPovey,ArnabGhoshal,GillesBoulianne,LukasBurget,OndrejGlembek,NagendraGoel,MirkoHannemann,PetrMotlicek,YanminQian,PetrSchwarz,JanSilovsky,GeorgStemmer,andKarelVesely,“TheKaldiSpeechRecognitionToolkit,”inIEEE2011WorkshoponAutomaticSpeechRecognitionandUnderstanding.Dec.2011,IEEESignalProcessingSociety,IEEECatalogNo.:CFP11SRW-USB.[10]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun,“Deepresiduallearningforimagerecog-nition,”inProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,2016,pp.770–778.[11]DanielGarcia-Romero,GregSell,andAlanMc-cree,“MagNetO:X-vectorMagnitudeEstimationNetworkplusOffsetforImprovedSpeakerRecog-nition,”inProc.TheSpeakerandLanguageRecog-nitionWorkshop(Odyssey2020),2020,pp.1–8.[12]HongjiWang,ChengdongLiang,ShuaiWang,ZhengyangChen,BinbinZhang,XuXiang,YanleiDeng,andYanminQian,“Wespeaker:Aresearchandproductionorientedspeakerembeddinglearn-ingtoolkit,”inIEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP).IEEE,2023,pp.1–5.[13]AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,andNeilHoulsby,“AnImageisWorth16x16Words:TransformersforImageRecognitionatScale,”inInternationalConferenceonLearningRepresentations,2021.[14]ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSainingXie,“AConvNetforthe2020s,”inProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),June2022,pp.11976–11986.[15]AndreasPeterSteiner,AlexanderKolesnikov,Xi-aohuaZhai,RossWightman,JakobUszkoreit,andLucasBeyer,“HowtotrainyourViT?Data,Aug-mentation,andRegularizationinVisionTransform-ers,”TransactionsonMachineLearningResearch,2022.[16]HugoTouvron,MatthieuCord,andHerv´eJ´egou,“DeiTIII:RevengeoftheViT,”inComputerVi-sion–ECCV2022,ShaiAvidan,GabrielBrostow,
15

MoustaphaCiss´e,GiovanniMariaFarinella,andTalHassner,Eds.,Cham,2022,pp.516–533,SpringerNatureSwitzerland.[17]TerranceDeVriesandGrahamWTaylor,“Im-provedregularizationofconvolutionalneu-ralnetworkswithcutout,”arXivpreprintarXiv:1708.04552,2017.[18]DavidSnyder,GuoguoChen,andDanielPovey,“MUSAN:AMusic,Speech,andNoiseCorpus,”2015,arXiv:1510.08484v1.[19]HemlataTak,MadhuKamble,JosePatino,Massim-ilianoTodisco,andNicholasEvans,“Rawboost:Arawdataboostingandaugmentationmethodappliedtoautomaticspeakerverificationanti-spoofing,”inICASSP2022-2022IEEEInternationalConfer-enceonAcoustics,SpeechandSignalProcessing(ICASSP).IEEE,2022,pp.6382–6386.[20]TomKo,VijayadityaPeddinti,DanielPovey,andSanjeevKhudanpur,“Audioaugmentationforspeechrecognition,”inProc.Interspeech2015,2015,pp.3586–3589.[21]SangdooYun,DongyoonHan,SeongJoonOh,SanghyukChun,JunsukChoe,andYoungjoonYoo,“CutMix:RegularizationStrategytoTrainStrongClassifiersWithLocalizableFeatures,”inProceed-ingsoftheIEEE/CVFInternationalConferenceonComputerVision(ICCV),October2019.[22]DanielS.Park,WilliamChan,YuZhang,Chung-ChengChiu,BarretZoph,EkinD.Cubuk,andQuocV.Le,“SpecAugment:ASimpleDataAug-mentationMethodforAutomaticSpeechRecogni-tion,”inProc.Interspeech2019,2019,pp.2613–2617.[23]D.GriffinandJaeLim,“Signalestimationfrommodifiedshort-timeFouriertransform,”inICASSP’83.IEEEInternationalConferenceonAcoustics,Speech,andSignalProcessing,1983,vol.8,pp.804–807.[24]MasanoriMorise,FumiyaYokomori,andKenjiOzawa,“WORLD:AVocoder-BasedHigh-QualitySpeechSynthesisSystemforReal-TimeApplica-tions,”IEICETRANSACTIONSonInformationandSystems,vol.99,no.7,pp.1877–1884,2016.[25]JungilKong,JaehyeonKim,andJaekyoungBae,“HiFi-GAN:GenerativeAdversarialNetworksforEfficientandHighFidelitySpeechSynthesis,”inAdvancesinNeuralInformationProcessingSys-tems,H.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin,Eds.2020,vol.33,pp.17022–17033,CurranAssociates,Inc.[26]RyuichiYamamoto,EunwooSong,andJae-MinKim,“ParallelWaveGAN:Afastwaveformgenerationmodelbasedongenerativeadversar-ialnetworkswithmulti-resolutionspectrogram,”inICASSP2020-2020IEEEInternationalConfer-enceonAcoustics,SpeechandSignalProcessing(ICASSP).IEEE,2020,pp.6199–6203.[27]DivyaShanmugam,DavisBlalock,GuhaBalakr-ishnan,andJohnGuttag,“Betteraggregationintest-timeaugmentation,”inProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,2021,pp.1214–1223.[28]J.S.Chung,A.Nagrani,andA.Zisserman,“Vox-Celeb2:DeepSpeakerRecognition,”inINTER-SPEECH,2018.[29]HaoWang,YitongWang,ZhengZhou,XingJi,Di-hongGong,JingchaoZhou,ZhifengLi,andWeiLiu,“CosFace:LargeMarginCosineLossforDeepFaceRecognition,”inProceedingsoftheIEEEcon-ferenceoncomputervisionandpatternrecognition,2018,pp.5265–5274.[30]T.M.Ragonneau,Model-BasedDerivative-FreeOptimizationMethodsandSoftware,Ph.D.the-sis,DepartmentofAppliedMathematics,TheHongKongPolytechnicUniversity,HongKong,China,2022.[31]VineelPratap,QiantongXu,AnuroopSriram,GabrielSynnaeve,andRonanCollobert,“MLS:ALarge-ScaleMultilingualDatasetforSpeechRe-search,”inProc.Interspeech2020,2020,pp.2757–2761.[32]IlyaLoshchilovandFrankHutter,“Decoupledweightdecayregularization,”inInternationalCon-ferenceonLearningRepresentations,2019.[33]IlyaLoshchilovandFrankHutter,“SGDR:StochasticGradientDescentwithWarmRestarts,”inInternationalConferenceonLearningRepresen-tations,2017.[34]PavelIzmailov,DmitriiPodoprikhin,TimurGaripov,DmitryVetrov,andAndrewGordonWilson,“Averagingweightsleadstowideroptimaandbettergeneralization,”in34thConferenceonUncertaintyinArtificialIntelligence2018,UAI2018,RicardoSilva,AmirGloberson,andAmirGloberson,Eds.2018,34thConferenceonUncertaintyinArtificialIntelligence2018,UAI2018,pp.876–885,AssociationForUncertaintyinArtificialIntelligence(AUAI).