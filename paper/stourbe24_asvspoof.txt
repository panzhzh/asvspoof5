The Automatic Speaker Verification Spoofing Countermeasures Workshop (ASVspoof 2024)
31 August 2024, Kos, Greece

72

10.21437/ASVspoof.2024-11

ExploringWavLMBack-endsforSpeechSpoofingandDeepfakeDetectionTheophileStourbe,VictorMiara,TheoLepage,RedaDehakEPITAResearchLaboratory(LRE),France{theophile.stourbe,victor.miara,theo.lepage,reda.dehak}@epita.frAbstractThispaperdescribesoursubmittedsystemstotheASVspoof5ChallengeTrack1:SpeechDeepfakeDe-tection-OpenCondition,whichconsistsofastand-alonespeechdeepfake(bonafidevsspoof)detectiontask.Re-cently,large-scaleself-supervisedmodelsbecomeastan-dardinAutomaticSpeechRecognition(ASR)andotherspeechprocessingtasks.Thus,weleverageapre-trainedWavLMasafront-endmodelandpoolitsrepresenta-tionswithdifferentback-endtechniques.Thecompleteframeworkisfine-tunedusingonlythetraineddatasetofthechallenge,similartotheclosecondition.Besides,weadoptdata-augmentationbyaddingnoiseandrever-berationusingMUSANnoiseandRIRdatasets.Wealsoexperimentwithcodecaugmentationstoincreasetheperformanceofourmethod.Ultimately,weusetheBosaristoolkitforscorecalibrationandsystemfusiontogetbetterCllrscores.Ourfusedsystemachieves0.0937minDCF,3.42%EER,0.1927Cllr,and0.1375actDCF.1.IntroductionWiththedevelopmentofDeepNeuralNetworks(DNN),speechsynthesisandVoiceConversion(VC)aremakingsignificantprogressingeneratingnaturalspeechaudio.ThisincreasestherelevanceofspoofingspeechdetectiontoprotectspeakeridentitiesandmakebiometricsystemsbasedonAutomaticSpeakerVerification(ASV)sys-temsmorerobust.Thechallengeofanti-spoofingspeechrecognitionistodetecttheartifactsproducedbythegen-erationprocessortheVCofthespoofattacks.Overthepastfewyears,manychallengeshavebeenproposedtopromotetheconsiderationofspoofingandspeechdeep-fakesdetection.ASVspoofisaseriesofchallengesthatfocusondevelopingandbenchmarkingsystemstodetectandmitigatevariousspoofingattacksinASVsystems.ASVspoof5[1,2],thefiftheditioninthisseries,com-prisestwodifferenttracks.Thefirsttrack,whichisthegoalofourwork,istodevelopacountermeasuresystemtodetectdeepfakeaudio.Thistaskcomprisestwochal-lenges;thefirstone(theclosecondition)consistsofcre-atingasystemusingonlythetrainingdatasetofthechal-lenge.Intheotherchallenge,theopencondition,pre-trainedmodels,andothertrainingdatasetsareallowed,exceptthoseusedtogeneratethetestdata.Initially,mostanti-spoofingmethodswerebasedonDNNprocessingframe-levelhandcraftedacousticfea-tures,suchasMFCC,LFCC,log-linearfilterbank,CQCCs,andCQTspectrogram[3].Otherapproachesadoptrawwaveformsastheinputtotrainanend-to-endDNNtodetectdeepfakeaudio[4].Withthesuccessoflargeself-supervisedmodelsforspeechprocessing,manysolutionswereproposedusingHuBERT[5],wav2vec2.0[6]andXLS-R[7]modelsasafeatureextractorforadownstreamsystem.WangetYa-magishi[8]exploretheuseofHuBERTandwav2vec2.0asafeatureextractorforaudiospoofingdetection.Taketal.[9]succeedinusingXLS-Rasafront-endspeechfeaturesextractorforanAASISTmodelandgetasignif-icantimprovementoverprevioustechniques.Thesetwoworksshowthatfine-tuningthefront-endmoduleisnec-essarytogetbetterperformancessincethismodelwasonlytrainedwithbonafidedata.Wecanconcludethatus-inglargeself-supervisedpre-trainedmodelsforspoofingdetectionisveryefficient.Thus,wedecidedtodevelopasubmissionforASVspoof5basedontheWavLMmodel[10]whichdemonstratesimpressiveresultsinmanyareasrelatedtospeechprocessing,emotionrecognition[11]andspeakerrecognition[12,13].OursystemisbasedontheWavLMBasemodelasafront-endfeatureextractor;pre-trainedon960hofLib-rispeech[14]whichiscompliantwiththeASVspoof5openconditiontrackrules[1].WeusedtheCNNencoderlayerandthefirst12-thTransformersencoderlayersasfeaturesforadownstreamback-endsystem.Weusetwodifferentback-endstoaggregateallrepresentationsintooneembeddingvector:WeightedAverage(WA)pool-ingandMulti-HeadFactorizedAttentive(MHFA)pool-ingproposedforspeakerrecognitionin[12].Severalworksshowthatdata-augmentationisnecessarytolearnrobustdetectionsystemsandtoavoidover-fittingandimprovegeneralization[15,16].Codecaugmentation,backgroundnoiseaugmentation,reverberationaswellasRawBoost[17]weretestedandusedtoimprovethegen-eralizationandrobustnessofourapproach.Thepaperisorganizedasfollows:Section2presentsthedatasetusedinmodeltraining,modelscoring,andsystemsfusionandcalibrationsteps.Section3detailsourapproachfordataaugmentation,andSection4describesthefront-endandtheback-endofthemodelswedevel-
73

Weighted Cross-Entropy LossLinearWavLM(pre-trained)WeightedaveragepoolingCNN EncoderTransformer Layer LTransformer Layer 1. . .Weighted Average (WA)Temporal Average Pooling(a)WeightedAverage(WA)Weighted Cross-Entropy LossLinearWavLM(pre-trained)CNN EncoderTransformer Layer LTransformer Layer 1. . .Multi-Head Factorized Attention (MHFA)LinearLinearAttentive PoolingTemporal Average Pooling(b)Multi-HeadFactorizedAttention(MHFA)Figure1:Diagramofourframeworkforfine-tuningWavLMwithWA(a)andMHFA(b)back-ends.oped.Section5describesthedifferenthyper-parametersusedforthetrainingprocess.Theresultsandperfor-mancesarepresentedinSection6.TheconclusionsaregiveninSection7.2.DatasetsTable1:Summaryofthecontentsofthetrainingdatasetanddevelopmentsubsets.SubsetUsage#utterancesBonafideSpoofTrainingModeltraining18,797163,560Development(1)Scoring8,24528,846Development(2)Fusionandcalibration23,08980,770Forsystemdevelopment,ASVspoofreleasedatrain-ingdatasetcontaining182,357utterancesandadevel-opmentdatasetcontaining140,950utterancesdistributedas16kHz,16bitspersampleFLACfiles.Weusedthetrainingdatasettofine-tunetheWavLMandtheback-endmodels.Thedevelopmentdatasetwassplitintotwoparts:thescoringsetusedtocomputeandcomparethemodels’performances;andthefusionandcalibrationsetusedforscorecalibrationandmodelfusion.Thecon-tentsofeachdatasetaresummarizedinTable1.3.Data-augmentationPreviousworksshowtheimportanceofdata-augmentationtoobtainbetterperformanceandimprovegeneralization.Duringthemodeltraining,wetestedacombinationof4differentdata-augmentationtechniquesdescribedbelow.•Utterancesareaugmentedwithbackgroundnoisesrandomlyselectedfromthe929variousnoisesoftheMUSANcorpus[18].TheSNRisuniformlysampledbetween0and15dB.•Toapplyreverberation,weconvolvetheinputau-diosegmentwithanimpulseresponserandomlysampledfromtheSimulatedRoomImpulseRe-sponseDatabase[19]•Weusetorchaudiolibrarytoapplylowandhigh-qualitymp3andoggencoder.m4acouldnotbetestedasitisnotimplementedinthetorchaudioli-brary.Wealsotestedfourtrans-codecsconfigura-tion:highmp3→highogg,lowmp3→lowogg,highmp3→lowogg,highogg→lowmp3.•WealsoexperimentwithRawBoostsimilarto[20].Thismethodrequiresnoadditionaldatasources,e.g.,noiserecordingsorimpulseresponses,anditisdata,application,andmodel-agnostic.Thisdataaugmentationalgorithmisnotusedincombinationwiththeothertechniquesmentionedabove.Allourdataaugmentationwasimplementedonline(parallel)inourtrainingframeworksothatthemodelistrainedondifferentdataateachepoch.Foreachsample,wefirstapplyornotrandomlyselectednoisefromMU-SANnoise,randomreverberationfromRIRdataset,orboth.Second,weapplyornotarandomcodecaugmen-tation.InthecaseofRawBoost,wetestedthedifferentalgorithmsproposedin[20].4.Models4.1.Baseline:ResNet-basedspoofdetectionsystemWechoseaResNet-basedbackboneasabaselinesystemandsubmittedthissystemfortheclosedcondition.Wechosethismodelbecausewegetbetterperformancecom-
74

paredtotheRawNet2[4]andtheAASIST[21]methodsproposedintheASVspoof5Toolkit.WerelyontheFastResNet-34architecture,describedin[22],processing40-dimensionallog-melspectrograminputfeatureswithaHammingwindowof25mslengthand10msframe-shift.Theencoderdimensionissetto512andweaddaReLUactivationfunctionfollowedbyalinearlayerforthespoofingdetectiontask.4.2.WavLM-basedspoofdetectionsystemWavLM[10]isaTransformer-basedmodeldesignedforAutomaticSpeechRecognition(ASR).Itispre-trainedinaself-supervisedwaythatalsocapturesnon-ASRin-formation.Duringthepre-training,themodelprocessesrawaudiothroughamulti-layerconvolutionalfeatureen-coder,transformingasequence{xt}Tt=1ofTtimewin-dowstoproduce{zt}Tt=1.TheserepresentationsarethensubjecttonoiseandoverlappingbeforemaskingandfedintotheTransformerencoder,whichoutputsaseriesofhiddenstates(cid:8)hl(cid:9)Ll=1,whereLdenotesthenumberofTransformerlayers.Additionally,themodelincorporatesgatedrelativepositionbias,enhancingitsabilitytofo-cusonrelevantspeechfeatures.WavLMistrainedonamaskedspeechdenoisingandpredictiontaskwhichim-plicitlymodelsspeakerandspeech-relatedinformationastheobjectiveistopredictthepseudo-labelsoftheoriginalspeechonmaskedregions.4.2.1.WeightedAverage(WA)back-endSeveralworksshowthattheintermediaterepresentationoftheself-supervisedmodelcontainsessentialfeaturesthatcanbeusedinvariousspeechdownstreamtasks.Generally,thetoplayers,whichareclosertotheobjec-tiveofthepre-trainingtask,tendtobethemosthelpfulforautomaticspeechrecognition(ASR).Incontrast,thespeechandspeakerfeaturesaremainlyrepresentedinthelow-andmid-levelfeatures,whichcarrymostinforma-tionaboutspeechsignals.Thus,usingonlythelastTrans-formerlayer’soutputmightbesub-optimalforspeechspoofingdetection.AsshowninFigure1-a,following[10],usingtheout-putsztand(cid:8)hlt(cid:9)Ll=1ofthel-thtransformerlayerforeachframet,welearnaweightedaverageofalltheseoutputstogenerateanewframerepresentationOtsuchthatOt=w0zt+L(cid:88)l=1wlhlt,(1)where{wk|0≤k≤L}representthelearnableweights.Next,theweightedframe-levelrepresentationOtisfedintoatemporalaveragepoolinglayerfollowedbyafullyconnectedlayertoobtainthefinalscoreforspoofdetection.4.2.2.Multi-HeadFactorizedAttention(MHFA)back-endFollowing[12],Multi-HeadFactorizedAttention(MHFA)back-end(Figure1-b)consistsofaggregatinglayer-wiseoutputsfromWavLM’stransformerlayersintoanattentivepoolingmechanismthatclustersframe-levelrepresentationsintoacousticunitsdiscoveredbythetransformermodel.Theframe-levelrepresentationsarethenaggregated(pooled)withineachclusterandcombinedtoproducethefinalframeembedding.Thismechanismallowsframeembeddingstobeconditionedonthephoneticcontentoftheinpututterances.Referto[12]formoredetails.4.2.3.ReducingoverfittingTomitigatetheeffectofoverfittingfromtheWavLMfont-end,thesetwoaggregationmethodsrelyontwocomponents:(1)L2regularizationbetweentheupdatedweightsandtheinitialweightsfromthepre-trainedWavLMmodel,whichhelpscontroloverfittingcausedbythelargenumberofparameters;(2)layer-wiselearningratedecay,following[23].Giventheprogressiveabstrac-tionofinformationacrossTransformerlayers[10],thistechniqueallowsmoreflexibleweightupdatesinhigherlayerstoadaptASRcapabilities,whileensuringlowerlayerspreservespeechsignals-relatedinformation.5.ExperimentalsetupThefront-endofallourmodelsisbasedonthepre-trainedWavLMBasemodel1,itiscomposedofaCNNencoderand12Transformerlayers.ThedimensionofeachTransformerlayer’soutputis768.ThenumberofparametersoftheWavLMis∼94M,1551forthelinearweightedaveragepoolingback-end,and∼1Mforthe32headsMHFAback-end.AllourmodelsweretrainedonthewholetrainingsetreleasedbyASVspoof5usinganNVIDIAA10080GBGPUusingthecross-entropylosswithaweightof9forthebonafideclassand1forthespoofclasstosolvetheclassimbalanceissueofthetrainingset.Thesystemsweretrainedon4sspeechutterance’slength,randomlyselectedateachepochfromeachtrainingsample.Wetrainfor100epochswithadefaultbatchsizeof120or32,andwestopthetrainingiftheEERonthescoringdatasetdoesnotimproveafter50epochs.WeuseAdamoptimizerwithalearningrateof5×10−3fortheback-endand2×10−5fortheencoder,eachreducedby5%everyepoch.Thetestscorewascomputedontheentirespeechutterance.ResultsarereportedintermsofEqualErrorRate(EER)andminimumDetectionCostFunction(minDCF)followingthesetupdescribedin[2].Wetraindifferentmodels:first,byfixingtheparam-1https://huggingface.co/microsoft/wavlm-base
75

Table2:SpoofdetectionresultsofthedifferentmodelstrainedduringtheASVspoof5challengeonourscoringandprogressdatasets.Thebestperformancesarerepresentedinboldtext.ModelTrainingData-augmentationScoringDatasetProgressDataset#Back-endFine-tuneWavLMBatchsizeNoiseandRIRRawboostCodecEER(%)minDCFEER(%)minDCFBaseline(ResNet)120✓✓15.600.346916.190.39151MHFA1206.780.15812MHFA120✓8.780.21553MHFA✓1206.410.16284MHFA✓120✓3.370.08721.420.03805MHFA✓120✓28.910.71606MHFA✓120✓✓2.180.05521.220.03207MHFA✓32✓✓1.820.04981.130.02798WA✓32✓✓1.890.05031.010.02519Fusionofmodel6,7and81.100.02720.880.0226etersoftheencodersandtrainingonlytheparametersoftheMHFAback-end.Similartopreviouswork[8,9],wedecidenexttofine-tunetheparametersoftheencoder.WeusealowerlearningratefortheWavLMmodeltoavoidoverfitting.Tomakethetrainingfaster,thesefirstexperimentswereconductedusingonlythenoiseandre-verberationdataaugmentation.Wealsotestedtheeffi-ciencyoftheRawBoostdata-augmentation.Finally,weaddedthecodecaugmentationtothebestconfigurationandretrainedthemodel.6.ResultsanddiscussionsOurstrategyconsistedofthedevelopmentofamainsys-temthatachievedthebestpossibleindividualperfor-mancebeforetrainingafusionthatcouldimprovetheperformanceofthefinalsystem.Table2summarizessomepreliminaryresultsob-tainedduringthedevelopmentofourmainsystem.Wereporttheperformancesonourscoringdatasetandtheprogressdatasetwhenthesystemwassubmittedduringtheprogressphaseofthechallenge.WedidnotsucceedinperformingwellwiththebaselinesystemsproposedintheASVspoof5toolkit.Weobtainedthebestperfor-manceforthebaselinesystembasedonaResNetwithnoise,RIR,andcodecaugmentations.Theresultsofthefirstfoursystemswereexpected:(1)theWavLMperformedbetterthanthebaseline,thishasbeendemonstratedonotherspeechprocessingtasks;(2)fine-tuningtheWavLMisnecessarytoreachbetterperformance.Data-augmentationisalsofundamentalasfine-tuningtheWavLMwiththenoiseandRIRaugmen-tationsallowedforreachingthebestresults.SincetheWavLMhasalargenumberofparameters,themodelismoresubjecttooverfittinginthiscasecomparedtothecasewheretheWavLMweightsarefrozen.DataaugmentationbecomesnecessarytoincreasetheperformanceoftheWavLMandreduceoverfitting;weexperimentwithdifferentRawBoostalgorithmsproposedin[17].WeachieveanEERof28.91%,whichisworsethantheperformanceobtainedwiththenoiseandRIRdata-augmentation.Withsystemsnumbers6and7,theperformancesob-tainedbyusingcodecaugmentationinadditiontothenoiseandRIRaugmentationarebetter.Weobtaina35%relativeimprovementoftheEERwithabatchsizeof120samplesanda47%relativeimprovementoftheEERwithabatchsizeof32.Thisresultwasexpectedbecauseithasbeenobservedinpracticethatwhenusingalargerbatch,thereisadegradationinthequalityofthemodel,asmea-suredbyitsabilitytogeneralize[24,25].Initially,weselectedalargebatchsizetomaketheexperimentsrunfasterusingdataparallelism.Toreducetheeffectofoverfitting,weimplementaWeightedAverage(WA)back-end,whichhasalimitednumberofparameterscomparedtotheMHFAback-end.Thus,thismodelislesssubjecttooverfittingcomparedtotheMHFA.Asexpected,theresultofthissystemwasalittlebitworseonourscoringdatasetthantheMHFA,butitperformedbetterontheprogressdatasetandob-tainedanEERof1.01andaminDCFof0.0251,whichisourbestperformanceontheprogressdatasetwithanin-dividualsystem.ThisresultconfirmsthatwewouldneedmoretrainingsamplesordataaugmentationalgorithmstoavoidoverfittingwhenusingtheMHFAback-end.Intheend,fusionandcalibrationwereperformedus-inglinearlogisticregressionwiththeBosaristoolkit[26].Toselectthebestfusioncombination,weimplementedagreedyfusionscheme.First,wecalibratedallthesys-temsandselectedthebest,giventhelowestminDCFcost.Thebestthreesystemswerelinearlyfusedtoobtainthesubmissionsystem.Thisfusionperformedthebestonthescoringandprogressdataset.ThisresultconfirmsthecomplementaritybetweentheMHFAandWAback-ends.Ontheevaluationdataset,thefusedsystemachieves0.0937minDCF,3.42%EER,0.1927Cllr,and0.1375actDCF.Unlikeourpreviousresultswiththeprogressdataset,thisperformanceisworsethanonourscoringdataset.Thisresultsfromnewacousticconditionswherethemodelcouldnotgeneralizebetter.
76

Table3:Detailedperformanceoffusionsystemontheevaluationdatasetaccordingtodifferentacousticconditions.pooled-codec-1codec-10codec-11codec-2codec-3codec-4codec-5codec-6codec-7codec-8codec-9pooled0.09370.02490.06440.17640.07260.04500.07280.12980.03110.05110.18240.14680.1096A170.00810.00000.00060.00760.00210.00040.00320.00530.00000.00070.01020.02560.0025A180.03280.00270.01440.07490.01180.01240.01860.03920.00690.00810.06930.05320.0388A190.16220.06240.12980.16330.06460.08300.10070.20960.07210.09870.27970.15390.1001A200.06310.01600.03860.08650.01690.03220.02420.09710.02290.03110.14320.05530.0427A210.02270.00130.01010.03400.01180.00450.02020.02610.00240.00420.03890.05550.0169A220.05060.00710.02790.11200.02250.01450.05180.07530.00790.01520.10800.09100.0489A230.03510.00330.01580.07350.01450.01070.02780.04670.00590.00980.07390.05520.0499A240.09680.00830.06040.22390.07110.02330.09800.10120.01070.05310.13880.23020.1327A250.02160.00200.00920.03640.00360.00620.01440.03030.00400.00340.06480.03720.0160A260.02610.00060.01020.05510.01300.00200.02260.03040.00020.00570.05330.05920.0297A270.06080.00740.03280.14270.01880.02750.02700.11550.01450.01970.19380.08300.0725A280.33320.08100.24030.69710.40990.16240.35270.35890.08900.20910.42930.70630.5634A290.00950.00110.00310.00950.00650.00120.00330.00540.00120.00240.00610.03080.0060A300.06410.01100.03570.14300.02030.02580.03170.12330.01740.02460.19230.08730.0612A310.11720.02830.08150.23830.04950.05110.07450.18310.04000.05980.27190.17400.1303A320.05040.00600.02710.11760.00930.02170.01640.10960.01130.01730.18430.05500.0440(a)MinimumDetectionCostFunctionpooled-codec-1codec-10codec-11codec-2codec-3codec-4codec-5codec-6codec-7codec-8codec-9pooled3.420.922.496.453.161.663.004.661.132.046.376.024.45A170.300.000.040.260.090.010.150.200.000.050.370.920.14A181.170.100.502.670.460.460.711.440.290.392.431.921.41A195.612.194.695.822.322.933.567.292.493.659.895.703.60A202.180.551.373.010.591.140.863.360.811.125.112.031.54A210.820.050.461.210.500.180.781.070.120.141.421.920.68A221.790.281.093.880.800.541.942.770.290.533.853.181.74A231.230.130.582.670.550.411.031.690.220.432.581.971.74A243.440.322.228.432.530.963.643.610.411.944.988.134.75A250.750.070.381.420.120.220.541.080.130.142.371.390.58A260.940.020.421.920.510.080.861.120.010.202.012.051.06A272.190.281.235.400.681.090.994.290.550.696.892.902.64A2812.013.038.6125.2915.136.3012.7512.853.338.0215.3124.7921.20A290.390.070.250.500.420.040.250.330.040.190.371.210.48A302.280.411.265.040.751.001.124.320.630.876.693.042.19A314.071.042.938.321.741.862.636.471.392.119.446.024.75A321.840.221.024.300.340.890.674.070.390.636.642.041.68(b)EERWereportinTable3thedetailedperformancesofoursubmittedsystemaccordingtothedifferentacousticcon-ditions.ThefirstanalysisofthesetwotablesshowsthatconditionA28,whichusesaudiospeechgeneratedusingthepre-trainedYourTTSmodel[27],isthemostchal-lengingtaskinourcase.Adetailedanalysisshowsthatusinglimitedbandwidthcodeccompressionisalsodif-ficultbecausewelosespeechinformationinhigherfre-quencies.Finally,wecannoticethatsomespecificcom-binationsareverychallengingsuchasA28-codec10andA28-codec8.7.ConclusionsInthisarticle,wehavepresentedourcountermeasuresys-temsbasedonthepre-trainedWavLMBasemodelfortheASVspoof5challengeopenconditiontask.Thesesys-temssignificantlyoutperformedthebaseline.Wehaveshownthatthismodelcanbeagoodfeatureextractorforaback-enddetectionsystem.Similartopreviousworkbasedonlargemodelssuchaswav2vec2.0,thismodelneedstobefine-tunedusingaspoofeddataset.TheMHFAback-endobtainedgoodperformanceonourdevelopmentdataset,butitwasmoresubjecttooverfit-tingthanWA,whichhasfewerparameters.ThissimpleWeightedAverage(WA)poolingobtainsthebestperfor-mancesontheprogressdataset.Wewouldneedmoretrainingsamplesandaugmentationalgorithmstoavoidthisissue.ThefusionofthesystemsbasedonMHFAandWAachievedthebestperformanceandconfirmedthecomplementaryrelationshipbetweenthetwotechniques.AsWavLMrepresentationsalsocontainvaluablespeakeridentityinformation,wecouldexplorecombiningthetwotaskswithaback-endforeachdownstreamtask.8.AcknowledgementsThisworkwasperformedusingHPCresourcesfromGENCI-IDRIS(Grant2023-AD011014623)andhasbeenpartiallyfundedbytheFrenchNationalResearchAgency(projectAPATE-ANR-22-CE39-0016-05).9.References[1]ASVspoofconsortium,“ASVspoof5evalua-tionplan,”https://www.asvspoof.org/file/ASVspoof5___Evaluation_Plan_Phase2.pdf,2024.
77

[2]X.Wangetal.,“ASVspoof5:Crowdsourceddata,deepfakesandadversarialattacksatscale,”inASVspoof2024workshop(submitted),2024.[3]ZhizhengWu,NicholasEvans,TomiKinnunen,Ju-nichiYamagishi,FedericoAlegre,andHaizhouLi,“Spoofingandcountermeasuresforspeakerverifi-cation:Asurvey,”SpeechCommunication,2015.[4]HemlataTak,JosePatino,MassimilianoTodisco,AndreasNautsch,NicholasEvans,andAn-thonyLarcher,“End-to-Endanti-spoofingwithRawNet2,”inINTERSPEECH,2021.[5]Wei-NingHsu,BenjaminBolte,Yao-HungHu-bertTsai,KushalLakhotia,RuslanSalakhutdi-nov,andAbdelrahmanMohamed,“HuBERT:Self-SupervisedSpeechRepresentationLearningbyMaskedPredictionofHiddenUnits,”IEEETASLP,2021.[6]AlexeiBaevski,YuhaoZhou,AbdelrahmanMo-hamed,andMichaelAuli,“wav2vec2.0:AFrame-workforSelf-SupervisedLearningofSpeechRep-resentations,”inNeurIPS,2020.[7]ArunBabu,ChanghanWang,AndrosTjandra,KushalLakhotia,QiantongXu,NamanGoyal,Kri-tikaSingh,PatrickvonPlaten,YatharthSaraf,JuanPino,AlexeiBaevski,AlexisConneau,andMichaelAuli,“XLS-R:Self-supervisedCross-lingualSpeechRepresentationLearningatScale,”inINTERSPEECH,2022.[8]XinWangandJunichiYamagishi,“InvestigatingSelf-SupervisedFrontEndsforSpeechSpoofingCountermeasures,”inOdyssey,2022.[9]HemlataTak,MassimilianoTodisco,XinWang,JeeweonJung,JunichiYamagishi,andNicholasEvans,“AutomaticSpeakerVerificationSpoofingandDeepfakeDetectionUsingWav2vec2.0andDataAugmentation,”inOdyssey,2022.[10]SanyuanChen,ChengyiWang,ZhengyangChen,YuWu,ShujieLiu,ZhuoChen,JinyuLi,NaoyukiKanda,TakuyaYoshioka,XiongXiao,JianWu,LongZhou,ShuoRen,YanminQian,YaoQian,JianWu,MichaelZeng,XiangzhanYu,andFuruWei,“WavLM:Large-ScaleSelf-SupervisedPre-TrainingforFullStackSpeechProcessing,”IEEEJSTSP,2022.[11]DariaDiatlova,AntonUdalov,VitaliiShutov,andEgorSpirin,“AdaptingWavLMforSpeechEmo-tionRecognition,”inOdyssey,2024.[12]JunyiPeng,OldˇrichPlchot,ThemosStafylakis,LadislavMoˇsner,Luk´aˇsBurget,andJanˇCernock´y,“AnAttention-BasedBackendAllowingEfficientFine-TuningofTransformerModelsforSpeakerVerification,”inIEEESLT,2022.[13]VictorMiara,TheoLepage,andRedaDehak,“To-wardsSupervisedPerformanceonSpeakerVerifi-cationwithSelf-SupervisedLearningbyLeverag-ingLarge-ScaleASRModels,”inINTERSPEECH,2024.[14]VassilPanayotov,GuoguoChen,DanielPovey,andSanjeevKhudanpur,“Librispeech:anASRcorpusbasedonpublicdomainaudiobooks,”inICASSP,2015.[15]ArielCohen,InbalRimon,EranAflalo,andHaimH.Permuter,“Astudyondataaugmenta-tioninvoiceanti-spoofing,”SpeechCommunica-tion,2022.[16]WanyingGe,XinWang,JunichiYamagishi,Mas-similianoTodisco,andNicholasEvans,“SpoofingAttackAugmentation:CanDifferently-TrainedAt-tackModelsImproveGeneralisation?,”inICASSP,2024.[17]HemlataTak,MadhuKamble,JosePatino,Massi-milianoTodisco,andNicholasEvans,“Rawboost:ARawDataBoostingandAugmentationMethodAppliedtoAutomaticSpeakerVerificationAnti-Spoofing,”inICASSP,2022.[18]SnyderDavid,ChenGuoguo,andPoveyDaniel,“MUSAN:AMusic,Speech,andNoiseCorpus,”arXivpreprintarXiv:1510.08484,2015.[19]TomKo,VijayadityaPeddinti,DanielPovey,MichaelL.Seltzer,andSanjeevKhudanpur,“Astudyondataaugmentationofreverberantspeechforrobustspeechrecognition,”inICASSP,2017.[20]HemlataTak,MadhuKamble,JosePatino,Massi-milianoTodisco,andNicholasEvans,“Rawboost:ARawDataBoostingandAugmentationMethodAppliedtoAutomaticSpeakerVerificationAnti-Spoofing,”inICASSP,2022.[21]Jee-WeonJung,Hee-SooHeo,HemlataTak,Hye-JinShim,JoonSonChung,Bong-JinLee,Ha-JinYu,andNicholasEvans,“AASIST:AudioAnti-SpoofingUsingIntegratedSpectro-TemporalGraphAttentionNetworks,”inICASSP,2022.[22]JoonSonChung,JaesungHuh,SeongkyuMun,MinjaeLee,Hee-SooHeo,SoyeonChoe,ChiheonHam,SunghwanJung,Bong-JinLee,andIcksangHan,“InDefenceofMetricLearningforSpeakerRecognition,”inINTERSPEECH,2020.
78

[23]SunChi,QiuXipeng,XuYige,andHuangXuan-jing,“HowtoFine-TuneBERTforTextClassifica-tion?,”inCCL,2019.[24]YannA.LeCun,L´eonBottou,GenevieveB.Orr,andKlaus-RobertM¨uller,EfficientBackProp,pp.9–48,SpringerBerlinHeidelberg,Berlin,Heidel-berg,2012.[25]NitishShirishKeskar,DheevatsaMudigere,JorgeNocedal,MikhailSmelyanskiy,andPingTakPeterTang,“Onlarge-batchtrainingfordeeplearning:Generalizationgapandsharpminima,”in5thInter-nationalConferenceonLearningRepresentations,ICLR2017,2017.[26]NikoBr¨ummerandEdwarddeVilliers,“TheBOSARIStoolkit:theory,algorithmsandcodeforsurvivingthenewDCF,”inNISTSRE11SpeakerRecognitionWorkshop,2011.[27]EdressonCasanova,JulianWeber,ChristopherDShulby,ArnaldoCandidoJunior,ErenG¨olge,andMoacirAPonti,“YourTTS:Towardszero-shotmulti-speakerTTSandzero-shotvoiceconversionforeveryone,”inICML,2022.