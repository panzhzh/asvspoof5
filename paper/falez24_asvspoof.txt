The Automatic Speaker Verification Spoofing Countermeasures Workshop (ASVspoof 2024)
31 August 2024, Kos, Greece

32

10.21437/ASVspoof.2024-5

WhispeakSpeechDeepfakeDetectionSystemsfortheASVspoof5ChallengePierreFalez,TonyMarteauWhispeak,Francepfalez@whispeak.io,tmarteau@whispeak.ioAbstractInthispaper,wepresentthesystemsubmittedbyWhis-peakfortheASVSpoof5SpeechDeepfakeDetectionandSASVChallenge.Weuseanensembleofsystems,consist-ingofLFCC-LCNN,RawGAT-ST,Wav2Vec-RawGAT-STandWav2Vec-ConformerfortheSpeechDeepfakeDetectiontracks.WeusealinearfusionofanECAPA-TDNNwiththepreviousensembleforSASVtracks.Adozendataaugmentationtech-niquesareappliedduringtraininginordertoimprovethero-bustnessofthemodelsontheASVSpoof5dataset.Wealsotestourmodelsonexternaldatasetsandshowthatmodelsarenotyetabletogeneralizewellonout-of-domaindata.ThefinalsystemgivesanEERof4.16%andaminDCFof0.1124onthetrack1evaluationsetinopencondition.1.IntroductionGenerativedeeplearningandespeciallythespeechsynthesisisanactivefieldofresearch.Today,thesyntheticspeechisrealis-ticandnaturaltothepointwhereitbecomesdifficulttodiffer-entiatethemtobonafidespeech.Therefore,speechsynthesisandeaseofaccesstothesetechnologiesisanincreasingsocietyproblem.Forexample,misinformation,orimpersonation.Toprotectsocietyandautomaticspeakerverification(ASV)sys-tems,itisessentialtoprogressinthespeechsynthesisdetec-tion.Forseveralyears,ASVSpoofChallengeisagoodsupporttoevaluatetheprogressofresearchandfosterthedevelop-mentofcountermeasures(CM).Themainpurposeofthischal-lengeistoevaluategeneralizationcapacityofdetectorsoverun-knownattacksbyprovidingmultipledatasets.FirsteditionsofASVSpoofChallenge(2015[1]and2017[2])focusedonthedevelopmentofstand-aloneCMagainstsyntheticspeechorre-playspeech.Then2019edition[3]ofthechallengecombinedtwopasteditionswithatrackonsyntheticspeechdetection(LA)andatrackonreplayspeechdetection(PA).Thisedi-tionalsointroducesanevaluationmoreASV-centric.Finally,thelasteditionin2021[4],proposedthreeindependenttrackstoevaluatethedetectionofreplayspeech(PA),thedetectionofsyntheticspeech(DF)andthedetectionofsyntheticspeechpassthroughcodecsandcommunicationchannels(LA).IntheASVSpoof5edition,unlikethepasteditions,thechallengeisbasedonnon-studio-qualitydatacollectedinau-diobookdatasets[5].Thechallengeisorganizedintwotracks:Track1toevaluateSpeechDeepfakedetectionsystemsandTrack2toevaluateSpoofing-AwareSpeakerVerification(SASV)systems.ThispaperpresentstheWhispeaksubmissionsontracks1and2withafocusonthedifferentspoofingdetectionsystemsusedinSection2.TheninSection3,wepresentthemethodofapplyingthedataaugmentation.Section4focusonthedetailsofthemodeltrainingprocedure.Finally,allspoofingdetectionsystemsarecomparedwiththedevelopmentsetprovidedbytheorganizersandotherdatasetswiththeaimofmeasuringtheirgeneralizabilityinSection52.SystemOverview2.1.SpeechDeepfakeDetectionWehavechosenfourmodelsthathaveshowngoodperformanceontheASVSpoof21dataset[4].Twoofthemdonotusetransferlearninginordertocomplywiththeclosedcondition,theothertwousespre-trainedmodelsinordertoofferbettergeneraliza-tionsintheopencondition.ThefirstmodelisLFCC-LCNN[6],whichuselinearfrequencycepstralcoefficients(LFCCs)asfea-tures.LFCCareextractedwith1024FFT,awindowlengthof20ms,astrideof10msand20filters.Deltaanddelta-deltaofthefilterareconcatenated,resultingin60channels.LCNN9[7]followedbyaBi-LSTMandalinearlayerisusedasaclassi-fier.ThesecondmodelisaRawGAT-ST[8],whichuseafrozensincconv[9]thatextract70filtersfollowedbyaGraphNeuralNetwork.ThethirdmodelisaWav2Vec-RawGAT-ST[10].Itusesapre-trainedWav2Vec2.0[11]asafrontendextractor,fol-lowedbyaclassifiersimilartotheoneusedintheRawGAT-ST.Unliketheoriginalmodel,wehaveusedtheWav2Vec2.0LargetrainedonLibriSpeech960h[12]insteadoftheXLSversiontocomplywiththechallengerules.ThelastmodelisaWav2Vec-Conformer[13],whichalsouseaWav2Vec2.0asfrontend,butuseanadaptedConformerasaclassifier.Weusetheversionimprovedin[14]whichshowsabettercapacityforgeneralisa-tion.2.2.Spoofing-AwareSpeakerVerificationForthesecondtrack,wechoosetouseindependentCMandASVmodels,inordertoreuseCMmodelsofthefirsttrack.AsASVmodels,weuseanECAPA-TDNN[15]trainedonVoxCeleb2,inasimilarwaythan[16].Inordertocomplywithclosedconditionrules,wedonotusebabbledataaug-mentation.AfirsttrainingwithAMM-Softmaxisdonewithα=0.2on2-secondsegments,andthen,afine-tuningisdonewithα=0.5on6-secondsegments.Weusemultipledataaug-mentation(NoiseadditionwiththeMUSAN[17]noisedataset,ReverberationwiththeRIRSNOISES[18]dataset,codecap-plicationandSpecAugment[19]).3.DataAugmentationInordertomakeCMmodelsmorerobust,weappliedvarioustechniquesofdataaugmentation:•SilenceRemoval:weusedanenergy-basedvoiceactiv-itydetection(VAD)toretrievespeechframes.AlltheframeswheretheVADmeasurementisbelowathresh-oldissettozero.
33

ModelpDAASVSpoof19LATestASVSpoof21LAEvalASVSpoof21DFEvalASVSpoof5DevEERminDCFEERminDCFEERminDCFEERminDCFLFCC-LCNN0.034.33%0.500037.20%0.500036.06%0.500022.34%0.35440.0126.18%0.368430.74%0.433725.69%0.358012.84%0.18350.0533.14%0.466734.34%0.481528.01%0.401412.11%0.17260.135.45%0.500036.06%0.500028.63%0.411012.95%0.1844RawGAT-ST0.033.80%0.485932.92%0.476136.63%0.386316.25%0.22780.0121.82%0.315322.41%0.322225.25%0.324816.38%0.23640.0519.17%0.276216.73%0.236527.22%0.358013.71%0.19800.125.22%0.362225.19%0.365127.43%0.385913.64%0.1971Wav2Vec-RawGAT-ST0.015.25%0.219815.68%0.225913.30%0.18681.82%0.02570.0521.56%0.309318.31%0.262814.18%0.20011.04%0.01480.119.17%0.276223.24%0.336814.82%0.21300.76%0.01090.219.35%0.277916.63%0.238114.18%0.20310.83%0.0114Wav2Vec-Conformer0.029.87%0.425940.48%0.462819.40%0.28095.44%0.07880.0517.90%0.256317.63%0.237114.33%0.19260.93%0.01290.122.80%0.323320.92%0.291815.64%0.21891.12%0.01570.219.83%0.283018.85%0.264316.03%0.22311.15%0.0164Table1:ModelsEER↓andminDCF↓ondifferentASVSpoofdatasets.•TimeStretch:changethespeedwitharatiofrom0.7to1.3withoutchangingthepitch.•PitchShift:changethepitchinarangefrom-10to10semitoneswithoutchangingthetempo.•Noise:arandomsamplefromMUSAN[17]noisedatasetisaddedwitharandomSNRfrom0dBto15dB.•Reverberation:arandomroomimpulseresponsefromRIRSNOISES[18]datasetisapplied.•Rawboost[20]:ISD,SSIandLnLareeachappliedin-dependently.•16KhzCodec:acodecisrandomlyselectedbetweenmp3,aac,opus,vorbis,mu-lawandG722witharandomcompressionlevel.•8KhzCodec:audioisresampledat8khz,thenaran-domcodecisrandomlyselectedbetweenA-law,Mu-law,AMR-NBandGSM.Finally,theaudioisresampledbackto16khz.•BitCrusher:reducethebitdepthrandomlyfrom5to14bits.•Gain:applyagaintotheaudiointherange0.25to2.0.•SpecAugment[19]:Amaskrangingfrom100samplesto1000samplesisappliedontimeand5%offrequencyaremasked.Eachofthesedataaugmentationisappliedonlineinse-ries.EachofthemhasaprobabilitypDAtobeapplied.Inthisway,differentcombinationsofdataaugmentationareusedonthedata.4.TrainingDetailsAllCMmodelsaretrainedontheASVSpoof5trainset,ex-ceptforafewmodelstrainedwiththeadditionofbonafidesamplesfromLibriSpeech960h[21](onlyinopencondition).Weusebydefaultthesamehyperparametersasintheorigi-nalworks.Wehaveadaptedsomeparameterstotakeintoac-countthespecificsofASVSpoof5.ModelsaretrainedfourtimeswithanADAMoptimizeranddifferentpDAprobabili-ties.LFCC-LCNNusesalearningrateof3−4andnoweightdecay.RawGAT-STusesalearningrateandaweightdecayof1−4.ModelswithWav2Vecfrontendusealearningrateof1−6andaweightdecayof1−4.LFCC-LCNNandRawGAT-STaretrainedfor40epochswithanexponentiallearningratescheduler(γisfixedat0.85)andabatchsizeof64.Wav2Vec-RawGAT-STandWav2Vec-Conformeraretrainedfor200,000updateswithaconstantlearningrateandabatchsizeof20.Ineachconfiguration,weuseacross-entropylosswithaweightof0.9forbonafideclassandaweightof0.1forspoofclass,inor-dertorespectthedistributionofclassesinASVSpoof5.Allthemodelsusearandomlycropped4-secondsegmentofaudio.Forshortedaudio,acircularpaddingisusedtoreachtherequireddimension.EachmodelistrainedonasingleNvidiaL40S.Foreachmodel,thefinalweightsareobtainedbyaveragingthe5checkpointsthatachievedthebestminDCFonthedevel-opmentdataset[22]duringtraining.5.ResultsPredictionsaremadeusingunifiedfeaturemapstech-niques[25].Weextractcropsof4-secondwithanoffsetof2-secondandcomputethefinalpredictionbyaveragingmodeloutputofallthecrops.Asspecifiedinthechallengeguidelines,ourmodelsareevaluatedusingtheEERandminDCFmetricsfortrack1andthetandem-EER(t-EER)andagnostic-DCF(a-DCF)metricsfortrack2.ForminDCFanda-DCF,thecostsprovidedbyorga-nizersarerespectivelyCmiss=1,Cfa=10andCmiss=1,Cfa=10,Cfa,spoof=10.Table1showsthatusingWav2Vecfrontendgreatlyim-proveresultsontheASVSpoof5developmentdataset.pDAhasanimpactonperformancesofthemodels.Forexample,addingdataaugmentationtotheWav2Vec-ConformerimproveEERfrom5.44%(pDA=0.0)to0.93%(pDA=0.05)onthedevelopmentset.Wav2Vec-RawGAT-STisthemodelwhichgivesthebestoverallperformances.Fortheclosedcon-dition,LFCC-LCNNtendstoperformbetterthanRawGAT-ST.
34

ModelpDAIn-The-Wild[23]Fake-Or-Real[24]EERminDCFEERminDCFLFCC-LCNN0.032.64%0.500025.85%0.47320.0132.31%0.438618.28%0.24960.0527.16%0.374224.95%0.32350.122.98%0.313324.06%0.3216RawGAT-ST0.026.86%0.28539.75%0.12030.0125.30%0.314618.39%0.24310.0526.33%0.305125.53%0.34640.126.39%0.30440.22570.2991Wav2Vec-RawGAT-ST0.016.30%0.218016.92%0.19660.0521.91%0.278221.56%0.27680.119.75%0.257520.41%0.25120.219.90%0.258819.16%0.2333Wav2Vec-Conformer0.014.25%0.20449.80%0.12960.0514.49%0.206124.47%0.32150.118.17%0.252434.55%0.40490.216.23%0.233627.69%0.3187Table2:ModelsEER↓andminDCF↓onotherdatasets.ModelLibriSpeechEERminDCFLFCC-LCNN✗12.11%0.1726✓14.83%0.2148Wav2Vec-RawGAT-ST✗1.04%0.0148✓1.69%0.0241Table3:ModelsEER↓andminDCF↓onASVSpoof5Track1developmentset.However,allthemodelsperformpoorlyonolderASVSpoofdatasets(ASVSpoof19LATest,ASVSpoof21LAEvalandASVSpoof21DFEval).Onepossiblereasonisthatotherdatasetsarebasedonstudio-qualitysamples(fromVCTK[26]),whichisout-of-domaincomparedtoASVSpoof5whichuseslowerqualitydata,obtainedfromaudiobooksonLibriVox.Thisshowsthatdetectionmodelsarenotyetabletogeneralizetoalltypesofdata.Table2confirmsthisobservation.AccuracyonotherdatasetsismuchworsethanonASVSpoof5.Thistime,Wav2Vec-ConformerwithoutDAseemstobetheconfigurationthatgeneralizesbest.Table3showsthataddingbonafidefromLibriSpeechduringtrainingdegradetheperformanceofthesinglemod-els.However,suchmodelsseemrelevantforfusion,sincetheLFCC-LCNNtrainedwithLibriSpeechwaschosenaspartofthebestmodelscombination.Weusebaggingmethodtodothemodelfusion.Scoremergingisdonebyusingthemedianofthepredictionofse-lectedmodels.Fortrack1inclosedcondition,allmodelsthatcomplywiththeclosedruleswereselected(LFCC-LCNNwithpDA=(0.01,0.05,0.1)andRawGAT-STwithpDA=(0.01,0.05,0.1)).Fortrack1inopencondition,modelswereselectedbyfindingthecombinationthatgivesthebestminDCFonthedevelopmentset(LFCC-LCNNtrainedonASVSpoof5andLibrispeech,Wav2Vec-ConformerwithpDA=0.1andWav2Vec-RawGAT-STwithpDA=(0.1,0.2)).Table4showsthefinalresultoftheensembleofmodelsfortheSpeechDeep-fakeDetectiontrack.ForSASVtrack,weusedthesameensembleofmodelsfortheCMpart.ASVpartusethesingleECAPA-TDNNmodel.ThefusionbetweenCMandASVscoreisrealizedinthesamewayasthescriptprovidedwiththebaseline1,byestimatinglog-1https://github.com/asvspoof-challenge/asvspoof5/tree/main/Tool-score-fusionConditionDevelopmentSetEvaluationSetEERminDCFEERminDCFClosed12.23%0.177220.13%0.5312Open0.58%0.00824.16%0.1124Table4:FusionofmodelsEER↓andminDCF↓onASVSpoof5Track1developmentandevaluationpartitions.ConditionDevelopmentSetEvaluationSett-EERaDCFt-EERaDCFClosed5.49%0.176749.34%0.4513Open1.54%0.47324.63%0.1492Table5:Fusionofmodelst-EER↓andaDCF↓onASVSpoof5Track2developmentandevaluationpartitions.likelihoodratiosofthetwosystemsondevelopmentsetandthenlinearlycombiningpredictions.Table5showfinalsubmissionforthetrack2.6.ConclusionWehavepresentedtheWhispeaksystemsforthedifferenttrackandconditionoftheASVSpoof5Challenge.Modelsthatper-formbestontheASVSpoof5datasetuseWav2Vec2.0frontend.WeshowthatusingawiderangeofdataaugmentationappliedinserieswithadefinedprobabilityimprovetherobustnessofthemodelswhentestedontheASVSpoof5dataset.However,moreworkisneededtocreatesystemscapableofgeneraliz-ingonmorediversedata.ModelstrainedonASVSpoof5haveasignificantlylowerperformancewhentestedonalternativedatasets.7.References[1]ZhizhengWu,TomiKinnunen,NicholasW.D.Evans,JunichiYamagishi,CemalHanilc¸i,Md.Sahidullah,andAleksandrSizov,“ASVspoof2015:thefirstautomaticspeakerverificationspoofingandcountermeasureschal-lenge,”inInt.SpeechConf.(INTERSPEECH),Dresden,Germany,Sept.2015,pp.2037–2041.[2]TomiKinnunen,Md.Sahidullah,H´ectorDelgado,Massi-milianoTodisco,NicholasW.D.Evans,JunichiYamag-ishi,andKong-AikLee,“TheASVspoof2017Challenge:AssessingtheLimitsofReplaySpoofingAttackDetec-tion,”inInt.SpeechConf.(INTERSPEECH),Stockholm,Sweden,Aug.2017,pp.2–6.[3]MassimilianoTodisco,XinWang,VilleVestman,Md.Sahidullah,H´ectorDelgado,AndreasNautsch,JunichiYamagishi,NicholasW.D.Evans,TomiH.Kinnunen,andKongAikLee,“ASVspoof2019:FutureHorizonsinSpoofedandFakeAudioDetection,”inInt.SpeechConf.(INTERSPEECH),Graz,Austria,Sept.2019,pp.1008–1012.[4]JunichiYamagishi,XinWang,MassimilianoTodisco,MdSahidullah,JosePatino,AndreasNautsch,XuechenLiu,KongAikLee,TomiKinnunen,NicholasEvans,andH´ectorDelgado,“ASVspoof2021:acceleratingprogressinspoofedanddeepfakespeechdetection,”inAutomaticSpeakerVerificationandSpoofingCountermeasuresChal-lenge,Online,Sept.2021,pp.47–54.
35

[5]XinWang,H´ectorDelgado,HemlataTak,Jee-weonJung,Hye-jinShim,MassimilianoTodisco,IvanKukanov,XuechenLiu,MdSahidullah,TomiKinnunen,NicholasEvans,KongAikLee,andJunichiYamagishi,“ASVspoof5:Crowdsourcedspeechdata,deepfakes,andadversarialattacksatscale,”inASVspoofWorkshop2024(accepted),2024.[6]XinWangandJunichiYamagishi,“AComparativeStudyonRecentNeuralSpoofingCountermeasuresforSyn-theticSpeechDetection,”inInt.SpeechConf.(INTER-SPEECH),Brno,Czechia,Aug.2021,pp.4259–4263.[7]XiangWu,RanHe,ZhenanSun,andTieniuTan,“AlightCNNfordeepfacerepresentationwithnoisylabels,”IEEETrans.Inf.ForensicsSecur.,vol.13,no.11,pp.2884–2896,2018.[8]Jee-weonJung,Hee-SooHeo,HemlataTak,Hye-jinShim,JoonSonChung,Bong-JinLee,Ha-JinYu,andNicholasEvans,“Aasist:Audioanti-spoofingusingin-tegratedspectro-temporalgraphattentionnetworks,”inIEEEInt.Conf.onAcoustics,SpeechandSignalPro-cessing(ICASSP),OnlineandSingapore,May2022,pp.6367–6371.[9]MircoRavanelliandYoshuaBengio,“Speakerrecogni-tionfromrawwaveformwithsincnet,”inIEEESpo-kenLanguageTechnologyWorkshop(SLT-W),Athens,Greece,Dec.2018,pp.1021–1028.[10]HemlataTak,MassimilianoTodisco,XinWang,Jee-weonJung,JunichiYamagishi,andNicholasEvans,“Automaticspeakerverificationspoofinganddeepfakedetectionus-ingwav2vec2.0anddataaugmentation,”inOdyssey:TheSpeakerandLanguageRecognitionWorkshop,Bei-jing,China,June2022,pp.112–119.[11]AlexeiBaevski,YuhaoZhou,AbdelrahmanMohamed,andMichaelAuli,“wav2vec2.0:Aframeworkforself-supervisedlearningofspeechrepresentations,”inNeuralInformationProcessingSystems(NeurIPS),Online,Dec.2020,pp.12449–12460.[12]VassilPanayotov,GuoguoChen,DanielPovey,andSan-jeevKhudanpur,“Librispeech:AnASRcorpusbasedonpublicdomainaudiobooks,”inIEEEInt.Conf.onAcous-tics,SpeechandSignalProcessing(ICASSP),SouthBris-bane,Queensland,Australia,Apr.2015,pp.5206–5210.[13]ErosRosell´oCasado,AlejandroG´omezAlan´ıs,´AngelManuelG´omezGarc´ıa,AntonioMiguelPeinadoHerreros,etal.,“Aconformer-basedclassifierforvariable-lengthutteranceprocessinginanti-spoofing,”inInt.SpeechConf.(INTERSPEECH),Dublin,Ireland,Aug.2023.[14]Duc-TuanTruong,RuijieTao,TuanNguyen,Hieu-ThiLuong,KongAikLee,andEngSiongChng,“Temporal-channelmodelinginmulti-headself-attentionforsyntheticspeechdetection,”arXivpreprintarXiv:2406.17376,2024.[15]BrechtDesplanques,JentheThienpondt,andKrisDe-muynck,“Ecapa-tdnn:Emphasizedchannelattention,propagationandaggregationintdnnbasedspeakerveri-fication,”inInt.SpeechConf.(INTERSPEECH),OnlineandShanghai,China,Oct.2020,pp.3830–3834.[16]JentheThienpondt,BrechtDesplanques,andKrisDe-muynck,“Theidlabvoxsrc-20submission:Largemar-ginfine-tuningandquality-awarescorecalibrationindnnbasedspeakerverification,”inIEEEInt.Conf.onAcous-tics,SpeechandSignalProcessing(ICASSP),Toronto,ON,Canada,June2021,pp.5814–5818.[17]DavidSnyder,GuoguoChen,andDanielPovey,“Mu-san:Amusic,speech,andnoisecorpus,”arXivpreprintarXiv:1510.08484,2015.[18]TomKo,VijayadityaPeddinti,DanielPovey,MichaelLSeltzer,andSanjeevKhudanpur,“Astudyondataaug-mentationofreverberantspeechforrobustspeechrecog-nition,”inIEEEInt.Conf.onAcoustics,SpeechandSig-nalProcessing(ICASSP),NewOrleans,LA,USA,Mar.2017,pp.5220–5224.[19]DanielSPark,WilliamChan,YuZhang,Chung-ChengChiu,BarretZoph,EkinDCubuk,andQuocVLe,“Specaugment:Asimpledataaugmentationmethodforautomaticspeechrecognition,”inInt.SpeechConf.(IN-TERSPEECH),Graz,Austria,Sept.2019,pp.2613–2617.[20]HemlataTak,MadhuKamble,JosePatino,MassimilianoTodisco,andNicholasEvans,“Rawboost:Arawdataboostingandaugmentationmethodappliedtoautomaticspeakerverificationanti-spoofing,”inIEEEInt.Conf.onAcoustics,SpeechandSignalProcessing(ICASSP),On-lineandSingapore,May2022,pp.6382–6386.[21]VassilPanayotov,GuoguoChen,DanielPovey,andSan-jeevKhudanpur,“Librispeech:Anasrcorpusbasedonpublicdomainaudiobooks,”inIEEEInt.Conf.onAcous-tics,SpeechandSignalProcessing(ICASSP),SouthBris-bane,Queensland,Apr.2015,pp.5206–5210.[22]YingboGao,ChristianHerold,ZijianYang,andHermannNey,“Revisitingcheckpointaveragingforneuralmachinetranslation,”inFindingsoftheAssociationforCompu-tationalLinguistics(AACL-IJCNLP),Online,Nov.2022,pp.188–196.[23]NicolasMM¨uller,PavelCzempin,FranziskaDieckmann,AdamFroghyar,andKonstantinB¨ottinger,“Doesau-diodeepfakedetectiongeneralize?,”inInt.SpeechConf.(INTERSPEECH),Incheon,Korea,Sept.2022,pp.2783–2787.[24]RicardoReimaoandVassiliosTzerpos,“For:Adatasetforsyntheticspeechdetection,”inInt.Conf.onSpeechTechnologyandHuman-ComputerDialogue(SpeD),Timisoara,Romania,Oct.2019,pp.1–10.[25]Cheng-ILai,NanxinChen,Jes´usVillalba,andNajimDehak,“Assert:Anti-spoofingwithsqueeze-excitationandresidualnetworks,”inInt.SpeechConf.(INTER-SPEECH),Graz,Austria,Sept.2019,pp.1013–1017.[26]ChristopheVeaux,JunichiYamagishi,KirstenMacDon-ald,etal.,“Cstrvctkcorpus:Englishmulti-speakercorpusforcstrvoicecloningtoolkit,”UniversityofEdinburgh.TheCentreforSpeechTechnologyResearch(CSTR),vol.6,pp.15,2017.